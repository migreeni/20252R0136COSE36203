{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cad6c1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['GUARDIAN_API_KEY'] = \"YOUR_API_KEY_HERE\"\n",
    "os.environ['GUARDIAN_API_KEY'] = 'cfc29433-1765-41ac-8726-14d5ce438b9d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0450cf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 2) Setup · config · small utils\n",
    "# =========================\n",
    "\n",
    "import os, re, json, time, pathlib, unicodedata\n",
    "from datetime import datetime, date\n",
    "from typing import Dict, List, Tuple, Iterable, Optional\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "API_KEY = os.getenv(\"GUARDIAN_API_KEY\", \"\").strip()\n",
    "assert API_KEY, \"Set GUARDIAN_API_KEY environment variable.\"\n",
    "\n",
    "BASE_SEARCH = \"https://content.guardianapis.com/search\"\n",
    "OUTPUT_COLS = [\n",
    "    \"id\",\"webPublicationDate\",\"headline\",\"trailText\",\"bodyText\",\n",
    "    \"webTitle\",\"webUrl\",\"apiUrl\",\"wordcount\",\"tags_titles\",\"tags_types\"\n",
    "]\n",
    "\n",
    "def slugify(s: str) -> str:\n",
    "    \"\"\"Simple, filesystem-safe slug.\"\"\"\n",
    "    s = re.sub(r'[^0-9A-Za-z]+', '_', s.lower()).strip('_')\n",
    "    return s or \"query\"\n",
    "\n",
    "def year_slices(start_date: str, end_date: str) -> List[Tuple[int, str, str]]:\n",
    "    \"\"\"Split [start_date, end_date] into per-year [start, end] strings.\"\"\"\n",
    "    sd = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
    "    ed = datetime.strptime(end_date, \"%Y-%m-%d\").date()\n",
    "    assert sd <= ed, \"start_date must be <= end_date\"\n",
    "    out = []\n",
    "    y = sd.year\n",
    "    while y <= ed.year:\n",
    "        s = max(sd, date(y,1,1))\n",
    "        e = min(ed, date(y,12,31))\n",
    "        out.append((y, s.isoformat(), e.isoformat()))\n",
    "        y += 1\n",
    "    return out\n",
    "\n",
    "def guardian_get(params: Dict, max_retries: int = 6) -> Dict:\n",
    "    \"\"\"GET /search with backoff for transient errors.\"\"\"\n",
    "    p = dict(params); p[\"api-key\"] = API_KEY\n",
    "    sleep = 1.5\n",
    "    for _ in range(max_retries):\n",
    "        r = requests.get(BASE_SEARCH, params=p, timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            return r.json()\n",
    "        if r.status_code in (429, 502, 503, 504):\n",
    "            time.sleep(sleep); sleep *= 2; continue\n",
    "        r.raise_for_status()\n",
    "    raise RuntimeError(f\"Guardian API failed: {r.status_code} {r.text[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ccd57af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 3) Fetch, normalize, and save per-year\n",
    "# =========================\n",
    "\n",
    "def iter_results(q: str, from_date: str, to_date: str,\n",
    "                 query_fields=(\"headline\",\"body\"), page_size: int = 200) -> Iterable[Dict]:\n",
    "    \"\"\"Yield /search items over all pages.\"\"\"\n",
    "    params = {\n",
    "        \"q\": q.lower(),\n",
    "        \"from-date\": from_date,\n",
    "        \"to-date\": to_date,\n",
    "        \"page-size\": page_size,   # max 200\n",
    "        \"order-by\": \"newest\",\n",
    "        \"use-date\": \"published\",\n",
    "        \"query-fields\": \",\".join(query_fields),\n",
    "        \"show-fields\": \"headline,trailText,bodyText,thumbnail,wordcount\",\n",
    "        \"show-tags\": \"all\",\n",
    "    }\n",
    "    js = guardian_get(params)\n",
    "    resp = js.get(\"response\", {})\n",
    "    pages = int(resp.get(\"pages\", 0)) or 0\n",
    "    for it in resp.get(\"results\", []):\n",
    "        yield it\n",
    "    for p in range(2, pages + 1):\n",
    "        params[\"page\"] = p\n",
    "        js = guardian_get(params)\n",
    "        for it in js.get(\"response\", {}).get(\"results\", []):\n",
    "            yield it\n",
    "\n",
    "def to_row(it: Dict) -> Dict:\n",
    "    \"\"\"Map a search item into the required output schema.\"\"\"\n",
    "    f = it.get(\"fields\") or {}\n",
    "    tags = it.get(\"tags\") or []\n",
    "    return {\n",
    "        \"id\": it.get(\"id\"),\n",
    "        \"webPublicationDate\": it.get(\"webPublicationDate\"),\n",
    "        \"headline\": f.get(\"headline\"),\n",
    "        \"trailText\": f.get(\"trailText\"),\n",
    "        \"bodyText\": f.get(\"bodyText\"),\n",
    "        \"webTitle\": it.get(\"webTitle\"),\n",
    "        \"webUrl\": it.get(\"webUrl\"),\n",
    "        \"apiUrl\": it.get(\"apiUrl\"),\n",
    "        \"wordcount\": f.get(\"wordcount\"),\n",
    "        \"tags_titles\": \"|\".join([t.get(\"webTitle\") for t in tags if t.get(\"webTitle\")]) or None,\n",
    "        \"tags_types\":  \"|\".join([t.get(\"type\") for t in tags if t.get(\"type\")]) or None,\n",
    "    }\n",
    "\n",
    "def crawl_and_save(query: str, start_date: str, end_date: str,\n",
    "                   out_dir: str = \"guardian_dump\",\n",
    "                   query_fields=(\"headline\",\"body\"),\n",
    "                   extra_aliases: Optional[List[str]] = None) -> None:\n",
    "    \"\"\"\n",
    "    Orchestrates the whole pipeline:\n",
    "      - builds boolean query\n",
    "      - loops per-year\n",
    "      - writes {slug}/{slug}_YEAR.jsonl and .csv with the fixed column order\n",
    "    \"\"\"\n",
    "    slug = slugify(query)\n",
    "    base = pathlib.Path(out_dir) / slug\n",
    "    base.mkdir(parents=True, exist_ok=True)\n",
    "    # boolean_q = build_boolean_query(query, extra_aliases)\n",
    "    boolean_q = query # Just for test\n",
    "    for y, y_start, y_end in year_slices(start_date, end_date):\n",
    "        # print(f\"[{query}] Starting {y} data crawling: {y_start} ~ {y_end}\")\n",
    "        seen, rows = set(), []\n",
    "        for it in iter_results(boolean_q, y_start, y_end, query_fields=query_fields):\n",
    "            _id = it.get(\"id\")\n",
    "            if _id in seen: \n",
    "                continue\n",
    "            seen.add(_id)\n",
    "            rows.append(to_row(it))\n",
    "\n",
    "        # write JSONL\n",
    "        jsonl_path = base / f\"{slug}_{y}.jsonl\"\n",
    "        with jsonl_path.open(\"w\", encoding=\"utf-8\") as jf:\n",
    "            for r in rows:\n",
    "                jf.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        # write CSV with exact column order\n",
    "        df = pd.DataFrame(rows).reindex(columns=OUTPUT_COLS)\n",
    "        csv_path = base / f\"{slug}_{y}.csv\"\n",
    "        df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "        print(f\"[{y}] rows={len(rows)}  JSONL={jsonl_path.name}  CSV={csv_path.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a1f1757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 3) Query augmentation with automatic accent handling (simplified)\n",
    "# =========================\n",
    "\n",
    "def remove_accents(text: str) -> str:\n",
    "    \"\"\"Remove accents from text (é->e, í->i, á->a, etc.)\"\"\"\n",
    "    nfd = unicodedata.normalize('NFD', text)\n",
    "    return ''.join(char for char in nfd if unicodedata.category(char) != 'Mn')\n",
    "\n",
    "def _tokens(name: str) -> List[str]:\n",
    "    name = re.sub(r\"[_/]\", \" \", name)\n",
    "    name = re.sub(r\"[\\\"'\"\"']\", \"\", name)\n",
    "    name = re.sub(r\"\\s+\", \" \", name).strip().lower()\n",
    "    return name.split()\n",
    "\n",
    "# def _variants(tokens: List[str]) -> List[str]:\n",
    "#     # Query augmentation disabled - only use exact names\n",
    "#     if not tokens: return []\n",
    "#     t = [x.strip(\".\") for x in tokens if x]\n",
    "#     base = {\" \".join(t), \"-\".join(t), \"_\".join(t)}\n",
    "#     if len(t) >= 3:\n",
    "#         base.add(f\"{t[0]} {t[-1]}\")\n",
    "#     # initials for all but last\n",
    "#     initials = [tok[0] for tok in t[:-1] if tok and tok[0].isalpha()]\n",
    "#     if initials and len(t) >= 2:\n",
    "#         last = t[-1]\n",
    "#         base.add(\" \".join(initials + [last]))        # j j watt\n",
    "#         base.add(\".\".join(initials) + f\". {last}\")   # j.j. watt\n",
    "#         base.add(\". \".join([i+\".\" for i in initials]) + f\" {last}\")  # j. j. watt\n",
    "#     return sorted(base)\n",
    "\n",
    "def build_boolean_query(query: str, extra_aliases: Optional[List[str]] = None) -> str:\n",
    "    \"\"\"Build query using only exact name + extra aliases + accent-free version\"\"\"\n",
    "    query_lower = query.lower().strip()\n",
    "    \n",
    "    aliases = set([query_lower])  # Start with exact query\n",
    "    \n",
    "    # Add extra aliases if provided\n",
    "    if extra_aliases: \n",
    "        aliases.update([a.lower().strip() for a in extra_aliases])\n",
    "    \n",
    "    # Auto-add accent-free version if original has accents\n",
    "    if query != remove_accents(query):\n",
    "        no_accent = remove_accents(query).lower().strip()\n",
    "        aliases.add(no_accent)\n",
    "\n",
    "    # Build OR query with all variants\n",
    "    phrases = [f'\"{p}\"' for p in sorted(aliases)]\n",
    "    return \" OR \".join(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9031f3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 4) Fetch, normalize, and save (combined years)\n",
    "# =========================\n",
    "\n",
    "def iter_results(q: str, from_date: str, to_date: str,\n",
    "                 query_fields=(\"headline\",\"body\"), page_size: int = 200) -> Iterable[Dict]:\n",
    "    \"\"\"Yield /search items over all pages.\"\"\"\n",
    "    params = {\n",
    "        \"q\": q.lower(),\n",
    "        \"from-date\": from_date,\n",
    "        \"to-date\": to_date,\n",
    "        \"page-size\": page_size,\n",
    "        \"order-by\": \"newest\",\n",
    "        \"use-date\": \"published\",\n",
    "        \"query-fields\": \",\".join(query_fields),\n",
    "        \"show-fields\": \"headline,trailText,bodyText,thumbnail,wordcount\",\n",
    "        \"show-tags\": \"all\",\n",
    "    }\n",
    "    js = guardian_get(params)\n",
    "    resp = js.get(\"response\", {})\n",
    "    pages = int(resp.get(\"pages\", 0)) or 0\n",
    "    for it in resp.get(\"results\", []):\n",
    "        yield it\n",
    "    for p in range(2, pages + 1):\n",
    "        params[\"page\"] = p\n",
    "        js = guardian_get(params)\n",
    "        for it in js.get(\"response\", {}).get(\"results\", []):\n",
    "            yield it\n",
    "\n",
    "def to_row(it: Dict) -> Dict:\n",
    "    \"\"\"Map a search item into the required output schema.\"\"\"\n",
    "    f = it.get(\"fields\") or {}\n",
    "    tags = it.get(\"tags\") or []\n",
    "    return {\n",
    "        \"id\": it.get(\"id\"),\n",
    "        \"webPublicationDate\": it.get(\"webPublicationDate\"),\n",
    "        \"headline\": f.get(\"headline\"),\n",
    "        \"trailText\": f.get(\"trailText\"),\n",
    "        \"bodyText\": f.get(\"bodyText\"),\n",
    "        \"webTitle\": it.get(\"webTitle\"),\n",
    "        \"webUrl\": it.get(\"webUrl\"),\n",
    "        \"apiUrl\": it.get(\"apiUrl\"),\n",
    "        \"wordcount\": f.get(\"wordcount\"),\n",
    "        \"tags_titles\": \"|\".join([t.get(\"webTitle\") for t in tags if t.get(\"webTitle\")]) or None,\n",
    "        \"tags_types\":  \"|\".join([t.get(\"type\") for t in tags if t.get(\"type\")]) or None,\n",
    "    }\n",
    "\n",
    "def crawl_and_save(query: str, start_date: str, end_date: str,\n",
    "                   out_dir: str = \"guardian_scrapping\",\n",
    "                   query_fields=(\"headline\",\"body\"),\n",
    "                   extra_aliases: Optional[List[str]] = None) -> None:\n",
    "    \"\"\"\n",
    "    Orchestrates the whole pipeline:\n",
    "      - builds boolean query\n",
    "      - loops through all dates\n",
    "      - writes {person_name}.jsonl in guardian_scrapping/\n",
    "      - writes {person_name}.csv in guardian_scrapping/scrapping_csv/\n",
    "    \"\"\"\n",
    "    slug = slugify(query)\n",
    "    base = pathlib.Path(out_dir)\n",
    "    base.mkdir(parents=True, exist_ok=True)\n",
    "    csv_dir = base / \"scrapping_csv\"\n",
    "    csv_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # boolean_q = build_boolean_query(query, extra_aliases)\n",
    "    boolean_q = query\n",
    "    # Collect all results across all years\n",
    "    seen, rows = set(), []\n",
    "    for y, y_start, y_end in year_slices(start_date, end_date):\n",
    "        print(f\"  [{query}] Crawling {y}: {y_start} ~ {y_end}\")\n",
    "        for it in iter_results(boolean_q, y_start, y_end, query_fields=query_fields):\n",
    "            _id = it.get(\"id\")\n",
    "            if _id in seen: continue\n",
    "            seen.add(_id)\n",
    "            rows.append(to_row(it))\n",
    "\n",
    "    # write JSONL (in guardian_scrapping/)\n",
    "    jsonl_path = base / f\"{slug}.jsonl\"\n",
    "    with jsonl_path.open(\"w\", encoding=\"utf-8\") as jf:\n",
    "        for r in rows:\n",
    "            jf.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    # write CSV (in guardian_scrapping/scrapping_csv/)\n",
    "    df = pd.DataFrame(rows).reindex(columns=OUTPUT_COLS)\n",
    "    csv_path = csv_dir / f\"{slug}.csv\"\n",
    "    df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"  [{query}] Total rows={len(rows)}  JSONL={jsonl_path.name}  CSV={csv_path.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6603c441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 5) Batch runner + summary CSV (with skip existing files + yearly counts)\n",
    "# =========================\n",
    "\n",
    "def batch_crawl_and_summary(\n",
    "    people: List[str],\n",
    "    start_date: str = \"2017-01-01\",\n",
    "    end_date: str = \"2019-12-31\",\n",
    "    out_dir: str = \"guardian_scrapping\",\n",
    "    query_fields=(\"headline\",\"body\"),\n",
    "    aliases_map: Optional[Dict[str, List[str]]] = None,\n",
    "    skip_existing: bool = True,\n",
    ") -> Dict[str, pathlib.Path]:\n",
    "    \"\"\"Crawl Guardian per person, save combined JSONL/CSV, then build summary CSV.\"\"\"\n",
    "    out_base = pathlib.Path(out_dir)\n",
    "    out_base.mkdir(parents=True, exist_ok=True)\n",
    "    csv_dir = out_base / \"scrapping_csv\"\n",
    "    csv_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if aliases_map is None:\n",
    "        aliases_map = {}\n",
    "\n",
    "    summary_data = []\n",
    "    for i, p in enumerate(people, 1):\n",
    "        slug = slugify(p)\n",
    "        jsonl_path = out_base / f\"{slug}.jsonl\"\n",
    "        csv_path = csv_dir / f\"{slug}.csv\"\n",
    "        \n",
    "        # Skip if JSONL file already exists\n",
    "        if skip_existing and jsonl_path.exists():\n",
    "            print(f\"\\n[{i}/{len(people)}] SKIP (exists): {p}\")\n",
    "        else:\n",
    "            print(f\"\\n[{i}/{len(people)}] Crawling: {p}\")\n",
    "            crawl_and_save(\n",
    "                query=p,\n",
    "                start_date=start_date,\n",
    "                end_date=end_date,\n",
    "                out_dir=out_dir,\n",
    "                query_fields=query_fields,\n",
    "                extra_aliases=aliases_map.get(p, None),\n",
    "            )\n",
    "        \n",
    "        # Count articles by year\n",
    "        count_2017, count_2018, count_2019, total_count = 0, 0, 0, 0\n",
    "        if csv_path.exists():\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path)\n",
    "                total_count = len(df)\n",
    "                # Extract year from webPublicationDate\n",
    "                df['year'] = pd.to_datetime(df['webPublicationDate']).dt.year\n",
    "                count_2017 = len(df[df['year'] == 2017])\n",
    "                count_2018 = len(df[df['year'] == 2018])\n",
    "                count_2019 = len(df[df['year'] == 2019])\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Could not read {csv_path.name}: {e}\")\n",
    "        \n",
    "        summary_data.append({\n",
    "            \"person\": p,\n",
    "            \"slug\": slug,\n",
    "            \"2017\": count_2017,\n",
    "            \"2018\": count_2018,\n",
    "            \"2019\": count_2019,\n",
    "            \"total\": total_count,\n",
    "        })\n",
    "\n",
    "    # Save summary\n",
    "    df_summary = pd.DataFrame(summary_data)\n",
    "    summary_path = \"summary_counts.csv\"\n",
    "    df_summary.to_csv(summary_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"\\n\\nSaved summary: {summary_path}\")\n",
    "    print(f\"Total people crawled: {len(people)}\")\n",
    "    print(f\"Total articles: {df_summary['total'].sum()}\")\n",
    "    print(f\"  - 2017: {df_summary['2017'].sum()}\")\n",
    "    print(f\"  - 2018: {df_summary['2018'].sum()}\")\n",
    "    print(f\"  - 2019: {df_summary['2019'].sum()}\")\n",
    "\n",
    "    return {\"summary\": summary_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b6e14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/100] SKIP (exists): Tiffany Haddish\n",
      "\n",
      "[2/100] SKIP (exists): Cameron Kasky\n",
      "\n",
      "[3/100] SKIP (exists): Jaclyn Corin\n",
      "\n",
      "[4/100] SKIP (exists): David Hogg\n",
      "\n",
      "[5/100] SKIP (exists): Emma Gonzalez\n",
      "\n",
      "[6/100] SKIP (exists): Alex Wind\n",
      "\n",
      "[7/100] SKIP (exists): Kumail Nanjiani\n",
      "\n",
      "[8/100] SKIP (exists): Cardi B\n",
      "\n",
      "[9/100] SKIP (exists): Nice Nailantei Lengete\n",
      "\n",
      "[10/100] SKIP (exists): Chloe Kim\n",
      "\n",
      "[11/100] SKIP (exists): Carl June\n",
      "\n",
      "[12/100] SKIP (exists): Jan Rader\n",
      "\n",
      "[13/100] SKIP (exists): Peggy Whitson\n",
      "\n",
      "[14/100] SKIP (exists): Issa Rae\n",
      "\n",
      "[15/100] SKIP (exists): Bhavish Aggarwal\n",
      "\n",
      "[16/100] SKIP (exists): Jesmyn Ward\n",
      "\n",
      "[17/100] SKIP (exists): Ruth Davidson\n",
      "\n",
      "[18/100] SKIP (exists): Whitney Wolfe Herd\n",
      "\n",
      "[19/100] SKIP (exists): Marica Branchesi\n",
      "\n",
      "[20/100] SKIP (exists): Ann McKee\n",
      "\n",
      "[21/100] SKIP (exists): Trevor Noah\n",
      "\n",
      "[22/100] SKIP (exists): Jian Wei Pan\n",
      "\n",
      "[23/100] SKIP (exists): Nicole Kidman\n",
      "\n",
      "[24/100] SKIP (exists): Hugh Jackman\n",
      "\n",
      "[25/100] SKIP (exists): Gal Gadot\n",
      "\n",
      "[26/100] SKIP (exists): Ryan Coogler\n",
      "\n",
      "[27/100] SKIP (exists): Sterling Brown\n",
      "\n",
      "[28/100] SKIP (exists): Millie Bobby Brown\n",
      "\n",
      "[29/100] SKIP (exists): Kehinde Wiley\n",
      "\n",
      "[30/100] SKIP (exists): Christian Siriano\n",
      "\n",
      "[31/100] SKIP (exists): Lena Waithe\n",
      "\n",
      "[32/100] SKIP (exists): Greta Gerwig\n",
      "\n",
      "[33/100] SKIP (exists): Roseanne Barr\n",
      "\n",
      "[34/100] SKIP (exists): Shawn Mendes\n",
      "\n",
      "[35/100] SKIP (exists): Guillermo del Toro\n",
      "\n",
      "[36/100] SKIP (exists): Deepika Padukone\n",
      "\n",
      "[37/100] SKIP (exists): JR artist\n",
      "\n",
      "[38/100] SKIP (exists): Jimmy Kimmel\n",
      "\n",
      "[39/100] SKIP (exists): Judy Chicago\n",
      "\n",
      "[40/100] SKIP (exists): John Krasinski\n",
      "\n",
      "[41/100] SKIP (exists): Satya Nadella\n",
      "\n",
      "[42/100] SKIP (exists): Donald Trump\n",
      "\n",
      "[43/100] SKIP (exists): Prince Harry\n",
      "\n",
      "[44/100] SKIP (exists): Meghan Markle\n",
      "\n",
      "[45/100] SKIP (exists): Carmen Yulin Cruz\n",
      "\n",
      "[46/100] SKIP (exists): Mohammed bin Salman\n",
      "\n",
      "[47/100] SKIP (exists): Sadiq Khan\n",
      "\n",
      "[48/100] SKIP (exists): Justin Trudeau\n",
      "\n",
      "[49/100] SKIP (exists): Xi Jinping\n",
      "\n",
      "[50/100] SKIP (exists): Sean Hannity\n",
      "\n",
      "[51/100] SKIP (exists): Justin James Watt\n",
      "\n",
      "[52/100] SKIP (exists): Robert Mueller\n",
      "\n",
      "[53/100] SKIP (exists): Kenneth Frazier\n",
      "\n",
      "[54/100] SKIP (exists): Nancy Pelosi\n",
      "\n",
      "[55/100] SKIP (exists): Kim Jong Un\n",
      "\n",
      "[56/100] SKIP (exists): Leo Varadkar\n",
      "\n",
      "[57/100] SKIP (exists): Emmerson Mnangagwa\n",
      "\n",
      "[58/100] Crawling: Jacinda Ardern\n",
      "  [Jacinda Ardern] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Jacinda Ardern] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Jacinda Ardern] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Jacinda Ardern] Total rows=734  JSONL=jacinda_ardern.jsonl  CSV=jacinda_ardern.csv\n",
      "\n",
      "[59/100] Crawling: Savannah Guthrie\n",
      "  [Savannah Guthrie] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Savannah Guthrie] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Savannah Guthrie] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Savannah Guthrie] Total rows=666  JSONL=savannah_guthrie.jsonl  CSV=savannah_guthrie.csv\n",
      "\n",
      "[60/100] Crawling: Hoda Kotb\n",
      "  [Hoda Kotb] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Hoda Kotb] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Hoda Kotb] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Hoda Kotb] Total rows=50  JSONL=hoda_kotb.jsonl  CSV=hoda_kotb.csv\n",
      "\n",
      "[61/100] Crawling: Shinzo Abe\n",
      "  [Shinzo Abe] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Shinzo Abe] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Shinzo Abe] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Shinzo Abe] Total rows=705  JSONL=shinzo_abe.jsonl  CSV=shinzo_abe.csv\n",
      "\n",
      "[62/100] Crawling: Sheikh Hasina\n",
      "  [Sheikh Hasina] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Sheikh Hasina] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Sheikh Hasina] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Sheikh Hasina] Total rows=841  JSONL=sheikh_hasina.jsonl  CSV=sheikh_hasina.csv\n",
      "\n",
      "[63/100] Crawling: Jeff Sessions\n",
      "  [Jeff Sessions] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Jeff Sessions] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Jeff Sessions] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Jeff Sessions] Total rows=8688  JSONL=jeff_sessions.jsonl  CSV=jeff_sessions.csv\n",
      "\n",
      "[64/100] Crawling: Moon Jae in\n",
      "  [Moon Jae in] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Moon Jae in] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Moon Jae in] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Moon Jae in] Total rows=4382  JSONL=moon_jae_in.jsonl  CSV=moon_jae_in.csv\n",
      "\n",
      "[65/100] Crawling: Emmanuel Macron\n",
      "  [Emmanuel Macron] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Emmanuel Macron] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Emmanuel Macron] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Emmanuel Macron] Total rows=3864  JSONL=emmanuel_macron.jsonl  CSV=emmanuel_macron.csv\n",
      "\n",
      "[66/100] Crawling: Mauricio Macri\n",
      "  [Mauricio Macri] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Mauricio Macri] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Mauricio Macri] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Mauricio Macri] Total rows=2261  JSONL=mauricio_macri.jsonl  CSV=mauricio_macri.csv\n",
      "\n",
      "[67/100] Crawling: Scott Pruitt\n",
      "  [Scott Pruitt] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Scott Pruitt] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Scott Pruitt] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Scott Pruitt] Total rows=903  JSONL=scott_pruitt.jsonl  CSV=scott_pruitt.csv\n",
      "\n",
      "[68/100] Crawling: Haider al Abadi\n",
      "  [Haider al Abadi] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Haider al Abadi] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Haider al Abadi] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Haider al Abadi] Total rows=423  JSONL=haider_al_abadi.jsonl  CSV=haider_al_abadi.csv\n",
      "\n",
      "[69/100] Crawling: Jennifer Lopez\n",
      "  [Jennifer Lopez] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Jennifer Lopez] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Jennifer Lopez] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Jennifer Lopez] Total rows=3790  JSONL=jennifer_lopez.jsonl  CSV=jennifer_lopez.csv\n",
      "\n",
      "[70/100] Crawling: Chadwick Boseman\n",
      "  [Chadwick Boseman] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Chadwick Boseman] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Chadwick Boseman] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Chadwick Boseman] Total rows=443  JSONL=chadwick_boseman.jsonl  CSV=chadwick_boseman.csv\n",
      "\n",
      "[71/100] Crawling: Rihanna\n",
      "  [Rihanna] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Rihanna] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Rihanna] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Rihanna] Total rows=692  JSONL=rihanna.jsonl  CSV=rihanna.csv\n",
      "\n",
      "[72/100] Crawling: Adam Rippon\n",
      "  [Adam Rippon] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Adam Rippon] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Adam Rippon] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Adam Rippon] Total rows=262  JSONL=adam_rippon.jsonl  CSV=adam_rippon.csv\n",
      "\n",
      "[73/100] Crawling: Tarana Burke\n",
      "  [Tarana Burke] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Tarana Burke] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Tarana Burke] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Tarana Burke] Total rows=1498  JSONL=tarana_burke.jsonl  CSV=tarana_burke.csv\n",
      "\n",
      "[74/100] Crawling: Cristina Jimenez\n",
      "  [Cristina Jimenez] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Cristina Jimenez] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Cristina Jimenez] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Cristina Jimenez] Total rows=453  JSONL=cristina_jimenez.jsonl  CSV=cristina_jimenez.csv\n",
      "\n",
      "[75/100] Crawling: Janet Mock\n",
      "  [Janet Mock] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Janet Mock] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Janet Mock] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Janet Mock] Total rows=3141  JSONL=janet_mock.jsonl  CSV=janet_mock.csv\n",
      "\n",
      "[76/100] Crawling: Kesha\n",
      "  [Kesha] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Kesha] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Kesha] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Kesha] Total rows=68  JSONL=kesha.jsonl  CSV=kesha.csv\n",
      "\n",
      "[77/100] Crawling: Kevin Kwan\n",
      "  [Kevin Kwan] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Kevin Kwan] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Kevin Kwan] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Kevin Kwan] Total rows=340  JSONL=kevin_kwan.jsonl  CSV=kevin_kwan.csv\n",
      "\n",
      "[78/100] Crawling: Ronan Farrow\n",
      "  [Ronan Farrow] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Ronan Farrow] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Ronan Farrow] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Ronan Farrow] Total rows=783  JSONL=ronan_farrow.jsonl  CSV=ronan_farrow.csv\n",
      "\n",
      "[79/100] Crawling: Jodi Kantor\n",
      "  [Jodi Kantor] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Jodi Kantor] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Jodi Kantor] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Jodi Kantor] Total rows=194  JSONL=jodi_kantor.jsonl  CSV=jodi_kantor.csv\n",
      "\n",
      "[80/100] Crawling: Megan Twohey\n",
      "  [Megan Twohey] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Megan Twohey] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Megan Twohey] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Megan Twohey] Total rows=1165  JSONL=megan_twohey.jsonl  CSV=megan_twohey.csv\n",
      "\n",
      "[81/100] Crawling: Maxine Waters\n",
      "  [Maxine Waters] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Maxine Waters] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Maxine Waters] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Maxine Waters] Total rows=5048  JSONL=maxine_waters.jsonl  CSV=maxine_waters.csv\n",
      "\n",
      "[82/100] Crawling: Sinta Nuriyah\n",
      "  [Sinta Nuriyah] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Sinta Nuriyah] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Sinta Nuriyah] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Sinta Nuriyah] Total rows=5  JSONL=sinta_nuriyah.jsonl  CSV=sinta_nuriyah.csv\n",
      "\n",
      "[83/100] Crawling: Rachael Denhollander\n",
      "  [Rachael Denhollander] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Rachael Denhollander] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Rachael Denhollander] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Rachael Denhollander] Total rows=583  JSONL=rachael_denhollander.jsonl  CSV=rachael_denhollander.csv\n",
      "\n",
      "[84/100] Crawling: Daniela Vega\n",
      "  [Daniela Vega] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Daniela Vega] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Daniela Vega] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Daniela Vega] Total rows=408  JSONL=daniela_vega.jsonl  CSV=daniela_vega.csv\n",
      "\n",
      "[85/100] Crawling: Virgil Abloh\n",
      "  [Virgil Abloh] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Virgil Abloh] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Virgil Abloh] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Virgil Abloh] Total rows=1134  JSONL=virgil_abloh.jsonl  CSV=virgil_abloh.csv\n",
      "\n",
      "[86/100] Crawling: Christopher Wylie\n",
      "  [Christopher Wylie] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Christopher Wylie] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Christopher Wylie] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Christopher Wylie] Total rows=6043  JSONL=christopher_wylie.jsonl  CSV=christopher_wylie.csv\n",
      "\n",
      "[87/100] Crawling: Roger Federer\n",
      "  [Roger Federer] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Roger Federer] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Roger Federer] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Roger Federer] Total rows=4373  JSONL=roger_federer.jsonl  CSV=roger_federer.csv\n",
      "\n",
      "[88/100] Crawling: Oprah Winfrey\n",
      "  [Oprah Winfrey] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Oprah Winfrey] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Oprah Winfrey] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Oprah Winfrey] Total rows=558  JSONL=oprah_winfrey.jsonl  CSV=oprah_winfrey.csv\n",
      "\n",
      "[89/100] Crawling: Jeff Bezos\n",
      "  [Jeff Bezos] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Jeff Bezos] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Jeff Bezos] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Jeff Bezos] Total rows=4935  JSONL=jeff_bezos.jsonl  CSV=jeff_bezos.csv\n",
      "\n",
      "[90/100] Crawling: Cindy Holland\n",
      "  [Cindy Holland] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Cindy Holland] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Cindy Holland] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Cindy Holland] Total rows=2431  JSONL=cindy_holland.jsonl  CSV=cindy_holland.csv\n",
      "\n",
      "[91/100] Crawling: Kevin Durant\n",
      "  [Kevin Durant] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Kevin Durant] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Kevin Durant] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Kevin Durant] Total rows=444  JSONL=kevin_durant.jsonl  CSV=kevin_durant.csv\n",
      "\n",
      "[92/100] Crawling: Elon Musk\n",
      "  [Elon Musk] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Elon Musk] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Elon Musk] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Elon Musk] Total rows=909  JSONL=elon_musk.jsonl  CSV=elon_musk.csv\n",
      "\n",
      "[93/100] Crawling: Sonia Friedman\n",
      "  [Sonia Friedman] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Sonia Friedman] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Sonia Friedman] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Sonia Friedman] Total rows=995  JSONL=sonia_friedman.jsonl  CSV=sonia_friedman.csv\n",
      "\n",
      "[94/100] Crawling: Giuliano Testa\n",
      "  [Giuliano Testa] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Giuliano Testa] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Giuliano Testa] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Giuliano Testa] Total rows=55  JSONL=giuliano_testa.jsonl  CSV=giuliano_testa.csv\n",
      "\n",
      "[95/100] Crawling: Masayoshi Son\n",
      "  [Masayoshi Son] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Masayoshi Son] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Masayoshi Son] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Masayoshi Son] Total rows=490  JSONL=masayoshi_son.jsonl  CSV=masayoshi_son.csv\n",
      "\n",
      "[96/100] Crawling: Elizabeth Diller\n",
      "  [Elizabeth Diller] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Elizabeth Diller] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Elizabeth Diller] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Elizabeth Diller] Total rows=5454  JSONL=elizabeth_diller.jsonl  CSV=elizabeth_diller.csv\n",
      "\n",
      "[97/100] Crawling: Virat Kohli\n",
      "  [Virat Kohli] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Virat Kohli] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Virat Kohli] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Virat Kohli] Total rows=593  JSONL=virat_kohli.jsonl  CSV=virat_kohli.csv\n",
      "\n",
      "[98/100] Crawling: Adam Neumann\n",
      "  [Adam Neumann] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Adam Neumann] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Adam Neumann] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Adam Neumann] Total rows=385  JSONL=adam_neumann.jsonl  CSV=adam_neumann.csv\n",
      "\n",
      "[99/100] Crawling: Pony Ma\n",
      "  [Pony Ma] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Pony Ma] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Pony Ma] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Pony Ma] Total rows=1624  JSONL=pony_ma.jsonl  CSV=pony_ma.csv\n",
      "\n",
      "[100/100] Crawling: Jose Andres\n",
      "  [Jose Andres] Crawling 2017: 2017-01-01 ~ 2017-12-31\n",
      "  [Jose Andres] Crawling 2018: 2018-01-01 ~ 2018-12-31\n",
      "  [Jose Andres] Crawling 2019: 2019-01-01 ~ 2019-12-31\n",
      "  [Jose Andres] Total rows=2788  JSONL=jose_andres.jsonl  CSV=jose_andres.csv\n",
      "\n",
      "\n",
      "Saved summary: summary_counts.csv\n",
      "Total people crawled: 100\n",
      "Total articles: 232583\n",
      "  - 2017: 80789\n",
      "  - 2018: 75618\n",
      "  - 2019: 76176\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person</th>\n",
       "      <th>slug</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tiffany Haddish</td>\n",
       "      <td>tiffany_haddish</td>\n",
       "      <td>152</td>\n",
       "      <td>153</td>\n",
       "      <td>178</td>\n",
       "      <td>483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cameron Kasky</td>\n",
       "      <td>cameron_kasky</td>\n",
       "      <td>55</td>\n",
       "      <td>59</td>\n",
       "      <td>55</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jaclyn Corin</td>\n",
       "      <td>jaclyn_corin</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>24</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>David Hogg</td>\n",
       "      <td>david_hogg</td>\n",
       "      <td>890</td>\n",
       "      <td>755</td>\n",
       "      <td>629</td>\n",
       "      <td>2274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Emma Gonzalez</td>\n",
       "      <td>emma_gonzalez</td>\n",
       "      <td>1711</td>\n",
       "      <td>1664</td>\n",
       "      <td>1857</td>\n",
       "      <td>5232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Elizabeth Diller</td>\n",
       "      <td>elizabeth_diller</td>\n",
       "      <td>1612</td>\n",
       "      <td>1524</td>\n",
       "      <td>2318</td>\n",
       "      <td>5454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Virat Kohli</td>\n",
       "      <td>virat_kohli</td>\n",
       "      <td>149</td>\n",
       "      <td>246</td>\n",
       "      <td>198</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Adam Neumann</td>\n",
       "      <td>adam_neumann</td>\n",
       "      <td>124</td>\n",
       "      <td>111</td>\n",
       "      <td>150</td>\n",
       "      <td>385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Pony Ma</td>\n",
       "      <td>pony_ma</td>\n",
       "      <td>639</td>\n",
       "      <td>508</td>\n",
       "      <td>477</td>\n",
       "      <td>1624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Jose Andres</td>\n",
       "      <td>jose_andres</td>\n",
       "      <td>1054</td>\n",
       "      <td>1136</td>\n",
       "      <td>598</td>\n",
       "      <td>2788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              person              slug  2017  2018  2019  total\n",
       "0    Tiffany Haddish   tiffany_haddish   152   153   178    483\n",
       "1      Cameron Kasky     cameron_kasky    55    59    55    169\n",
       "2       Jaclyn Corin      jaclyn_corin     9    30    24     63\n",
       "3         David Hogg        david_hogg   890   755   629   2274\n",
       "4      Emma Gonzalez     emma_gonzalez  1711  1664  1857   5232\n",
       "..               ...               ...   ...   ...   ...    ...\n",
       "95  Elizabeth Diller  elizabeth_diller  1612  1524  2318   5454\n",
       "96       Virat Kohli       virat_kohli   149   246   198    593\n",
       "97      Adam Neumann      adam_neumann   124   111   150    385\n",
       "98           Pony Ma           pony_ma   639   508   477   1624\n",
       "99       Jose Andres       jose_andres  1054  1136   598   2788\n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Run =====\n",
    "people = [\n",
    "    \"Tiffany Haddish\", \"Cameron Kasky\", \"Jaclyn Corin\", \"David Hogg\", \"Emma Gonzalez\",\n",
    "    \"Alex Wind\", \"Kumail Nanjiani\", \"Cardi B\", \"Nice Nailantei Lengete\", \"Chloe Kim\",\n",
    "    \"Carl June\", \"Jan Rader\", \"Peggy Whitson\", \"Issa Rae\", \"Bhavish Aggarwal\",\n",
    "    \"Jesmyn Ward\", \"Ruth Davidson\", \"Whitney Wolfe Herd\", \"Marica Branchesi\", \"Ann McKee\",\n",
    "    \"Trevor Noah\", \"Jian Wei Pan\", \"Nicole Kidman\", \"Hugh Jackman\", \"Gal Gadot\",\n",
    "    \"Ryan Coogler\", \"Sterling Brown\", \"Millie Bobby Brown\", \"Kehinde Wiley\", \"Christian Siriano\",\n",
    "    \"Lena Waithe\", \"Greta Gerwig\", \"Roseanne Barr\", \"Shawn Mendes\", \"Guillermo del Toro\",\n",
    "    \"Deepika Padukone\", \"JR artist\", \"Jimmy Kimmel\", \"Judy Chicago\", \"John Krasinski\",\n",
    "    \"Satya Nadella\", \"Donald Trump\", \"Prince Harry\", \"Meghan Markle\", \"Carmen Yulin Cruz\",\n",
    "    \"Mohammed bin Salman\", \"Sadiq Khan\", \"Justin Trudeau\", \"Xi Jinping\", \"Sean Hannity\",\n",
    "    \"Justin James Watt\", \"Robert Mueller\", \"Kenneth Frazier\", \"Nancy Pelosi\", \"Kim Jong Un\",\n",
    "    \"Leo Varadkar\", \"Emmerson Mnangagwa\", \"Jacinda Ardern\", \"Savannah Guthrie\", \"Hoda Kotb\",\n",
    "    \"Shinzo Abe\", \"Sheikh Hasina\", \"Jeff Sessions\", \"Moon Jae in\", \"Emmanuel Macron\",\n",
    "    \"Mauricio Macri\", \"Scott Pruitt\", \"Haider al Abadi\", \"Jennifer Lopez\", \"Chadwick Boseman\",\n",
    "    \"Rihanna\", \"Adam Rippon\", \"Tarana Burke\", \"Cristina Jimenez\", \"Janet Mock\",\n",
    "    \"Kesha\", \"Kevin Kwan\", \"Ronan Farrow\", \"Jodi Kantor\", \"Megan Twohey\",\n",
    "    \"Maxine Waters\", \"Sinta Nuriyah\", \"Rachael Denhollander\", \"Daniela Vega\", \"Virgil Abloh\",\n",
    "    \"Christopher Wylie\", \"Roger Federer\", \"Oprah Winfrey\", \"Jeff Bezos\", \"Cindy Holland\",\n",
    "    \"Kevin Durant\", \"Elon Musk\", \"Sonia Friedman\", \"Giuliano Testa\", \"Masayoshi Son\",\n",
    "    \"Elizabeth Diller\", \"Virat Kohli\", \"Adam Neumann\", \"Pony Ma\", \"Jose Andres\",\n",
    "]\n",
    "'''\n",
    "aliases_map = {\n",
    "    \"Emma Gonzalez\": [\"Emma González\"],\n",
    "    \"Nice Nailantei Lengete\": [\"Nice Nailantei Leng'ete\"],\n",
    "    \"Sterling Brown\": [\"Sterling K. Brown\"],\n",
    "    \"Carmen Yulin Cruz\": [\"Carmen Yulín Cruz\"],\n",
    "    \"Justin James Watt\": [\"J.J. Watt\"],\n",
    "    \"Kenneth Frazier\": [\"Kenneth C. Frazier\"],\n",
    "    \"Cristina Jimenez\": [\"Cristina Jiménez\"],\n",
    "    \"Jose Andres\": [\"José Andrés\"],\n",
    "}\n",
    "'''\n",
    "\n",
    "paths = batch_crawl_and_summary(\n",
    "    people=people,\n",
    "    start_date=\"2017-01-01\",\n",
    "    end_date=\"2019-12-31\",\n",
    "    out_dir=\"guardian_scrapping\",\n",
    "    aliases_map=None,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
