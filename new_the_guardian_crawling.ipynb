{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cad6c1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['GUARDIAN_API_KEY'] = \"YOUR_API_KEY_HERE\"\n",
    "# os.environ['GUARDIAN_API_KEY'] = 'cfc29433-1765-41ac-8726-14d5ce438b9d' #원준\n",
    "os.environ['GUARDIAN_API_KEY'] = '77465cf2-e74d-4260-b796-7a9d2c829333' #규주\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0450cf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 1: Setup & Config =====\n",
    "\n",
    "import os, re, json, time, pathlib\n",
    "from datetime import datetime, date\n",
    "from typing import Dict, List, Tuple, Iterable\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "API_KEY = os.getenv(\"GUARDIAN_API_KEY\", \"\").strip()\n",
    "assert API_KEY, \"Set GUARDIAN_API_KEY environment variable.\"\n",
    "\n",
    "BASE_SEARCH = \"https://content.guardianapis.com/search\"\n",
    "OUTPUT_COLS = [\n",
    "    \"id\",\"webPublicationDate\",\"headline\",\"trailText\",\"bodyText\",\n",
    "    \"webTitle\",\"webUrl\",\"apiUrl\",\"wordcount\"\n",
    "]\n",
    "\n",
    "def slugify(s: str) -> str:\n",
    "    \"\"\"파일명으로 사용 가능한 slug 생성\"\"\"\n",
    "    s = re.sub(r'[^0-9A-Za-z]+', '_', s.lower()).strip('_')\n",
    "    return s or \"query\"\n",
    "\n",
    "def year_slices(start_date: str, end_date: str) -> List[Tuple[int, str, str]]:\n",
    "    \"\"\"날짜 범위를 연도별로 분할\"\"\"\n",
    "    sd = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
    "    ed = datetime.strptime(end_date, \"%Y-%m-%d\").date()\n",
    "    assert sd <= ed, \"start_date must be <= end_date\"\n",
    "    out = []\n",
    "    y = sd.year\n",
    "    while y <= ed.year:\n",
    "        s = max(sd, date(y,1,1))\n",
    "        e = min(ed, date(y,12,31))\n",
    "        out.append((y, s.isoformat(), e.isoformat()))\n",
    "        y += 1\n",
    "    return out\n",
    "\n",
    "def guardian_get(params: Dict, max_retries: int = 6) -> Dict:\n",
    "    \"\"\"Guardian API 호출 (에러 시 자동 재시도)\"\"\"\n",
    "    p = dict(params)\n",
    "    p[\"api-key\"] = API_KEY\n",
    "    sleep = 1.5\n",
    "    for _ in range(max_retries):\n",
    "        r = requests.get(BASE_SEARCH, params=p, timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            return r.json()\n",
    "        if r.status_code in (429, 502, 503, 504):\n",
    "            time.sleep(sleep)\n",
    "            sleep *= 2\n",
    "            continue\n",
    "        r.raise_for_status()\n",
    "    raise RuntimeError(f\"Guardian API failed: {r.status_code} {r.text[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9031f3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 2: Fetch & Save Functions =====\n",
    "\n",
    "def iter_results(q: str, from_date: str, to_date: str,\n",
    "                 query_fields=(\"headline\",\"body\"), page_size: int = 200) -> Iterable[Dict]:\n",
    "    \"\"\"모든 페이지의 검색 결과를 yield\"\"\"\n",
    "    params = {\n",
    "        \"q\": q.lower(),\n",
    "        \"from-date\": from_date,\n",
    "        \"to-date\": to_date,\n",
    "        \"page-size\": page_size,\n",
    "        \"order-by\": \"newest\",\n",
    "        \"use-date\": \"published\",\n",
    "        \"query-fields\": \",\".join(query_fields),\n",
    "        \"show-fields\": \"headline,trailText,bodyText,thumbnail,wordcount\",\n",
    "        \"show-tags\": \"all\",\n",
    "    }\n",
    "    js = guardian_get(params)\n",
    "    resp = js.get(\"response\", {})\n",
    "    pages = int(resp.get(\"pages\", 0)) or 0\n",
    "    \n",
    "    for it in resp.get(\"results\", []):\n",
    "        yield it\n",
    "    \n",
    "    for p in range(2, pages + 1):\n",
    "        params[\"page\"] = p\n",
    "        js = guardian_get(params)\n",
    "        for it in js.get(\"response\", {}).get(\"results\", []):\n",
    "            yield it\n",
    "\n",
    "def to_row(it: Dict) -> Dict:\n",
    "    \"\"\"검색 결과를 출력 스키마로 변환\"\"\"\n",
    "    f = it.get(\"fields\") or {}\n",
    "    return {\n",
    "        \"id\": it.get(\"id\"),\n",
    "        \"webPublicationDate\": it.get(\"webPublicationDate\"),\n",
    "        \"headline\": f.get(\"headline\"),\n",
    "        \"trailText\": f.get(\"trailText\"),\n",
    "        \"bodyText\": f.get(\"bodyText\"),\n",
    "        \"webTitle\": it.get(\"webTitle\"),\n",
    "        \"webUrl\": it.get(\"webUrl\"),\n",
    "        \"apiUrl\": it.get(\"apiUrl\"),\n",
    "        \"wordcount\": f.get(\"wordcount\"),\n",
    "    }\n",
    "\n",
    "def crawl_and_save(query: str, start_date: str, end_date: str,\n",
    "                   out_dir: str = \"guardian_raw_scraping\",\n",
    "                   query_fields=(\"headline\",\"body\")) -> None:\n",
    "    \"\"\"크롤링 후 JSONL, CSV 저장\"\"\"\n",
    "    slug = slugify(query)\n",
    "    base = pathlib.Path(out_dir)\n",
    "    base.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 모든 연도의 결과 수집\n",
    "    seen, rows = set(), []\n",
    "    for y, y_start, y_end in year_slices(start_date, end_date):\n",
    "        print(f\"  [{query}] Crawling {y}: {y_start} ~ {y_end}\")\n",
    "        for it in iter_results(query, y_start, y_end, query_fields=query_fields):\n",
    "            _id = it.get(\"id\")\n",
    "            if _id in seen:\n",
    "                continue\n",
    "            seen.add(_id)\n",
    "            rows.append(to_row(it))\n",
    "\n",
    "    # JSONL 저장\n",
    "    jsonl_path = base / f\"{slug}.jsonl\"\n",
    "    with jsonl_path.open(\"w\", encoding=\"utf-8\") as jf:\n",
    "        for r in rows:\n",
    "            jf.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"  [{query}] Total rows={len(rows)}  JSONL={jsonl_path.name}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6603c441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3: Batch Crawl & Summary =====\n",
    "\n",
    "def batch_crawl_and_summary(\n",
    "    people: List[str],\n",
    "    start_date: str = \"2017-01-01\",\n",
    "    end_date: str = \"2019-12-31\",\n",
    "    out_dir: str = \"guardian_scraping\",\n",
    "    query_fields=(\"headline\",\"body\"),\n",
    "    skip_existing: bool = True,\n",
    ") -> Dict[str, pathlib.Path]:\n",
    "    \"\"\"여러 인물에 대해 크롤링 후 summary CSV 생성\"\"\"\n",
    "    out_base = pathlib.Path(out_dir)\n",
    "    out_base.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    summary_data = []\n",
    "    \n",
    "    for i, p in enumerate(people, 1):\n",
    "        slug = slugify(p)\n",
    "        jsonl_path = out_base / f\"{slug}.jsonl\"\n",
    "        \n",
    "        # 기존 파일 있으면 스킵\n",
    "        if skip_existing and jsonl_path.exists():\n",
    "            print(f\"\\n[{i}/{len(people)}] SKIP (exists): {p}\")\n",
    "        else:\n",
    "            print(f\"\\n[{i}/{len(people)}] Crawling: {p}\")\n",
    "            crawl_and_save(\n",
    "                query=p,\n",
    "                start_date=start_date,\n",
    "                end_date=end_date,\n",
    "                out_dir=out_dir,\n",
    "                query_fields=query_fields,\n",
    "            )\n",
    "        \n",
    "        # 연도별 기사 수 집계\n",
    "        count_2017, count_2018, count_2019, total_count = 0, 0, 0, 0\n",
    "        if jsonl_path.exists():  # csv_path → jsonl_path로 변경\n",
    "            try:\n",
    "                # JSONL 파일에서 직접 읽기\n",
    "                rows = []\n",
    "                with jsonl_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "                    for line in f:\n",
    "                        rows.append(json.loads(line))\n",
    "                \n",
    "                df = pd.DataFrame(rows)\n",
    "                total_count = len(df)\n",
    "                df['year'] = pd.to_datetime(df['webPublicationDate']).dt.year\n",
    "                count_2017 = len(df[df['year'] == 2017])\n",
    "                count_2018 = len(df[df['year'] == 2018])\n",
    "                count_2019 = len(df[df['year'] == 2019])\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Could not read {jsonl_path.name}: {e}\")\n",
    "        \n",
    "        summary_data.append({\n",
    "            \"person\": p,\n",
    "            \"slug\": slug,\n",
    "            \"2017\": count_2017,\n",
    "            \"2018\": count_2018,\n",
    "            \"2019\": count_2019,\n",
    "            \"total\": total_count,\n",
    "        })\n",
    "\n",
    "    # Summary CSV 저장\n",
    "    df_summary = pd.DataFrame(summary_data)\n",
    "    summary_path = \"guardian_raw_scraping_summary.csv\"\n",
    "    df_summary.to_csv(summary_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"\\n\\nSaved summary: {summary_path}\")\n",
    "    print(f\"Total people crawled: {len(people)}\")\n",
    "    print(f\"Total articles: {df_summary['total'].sum()}\")\n",
    "    print(f\"  - 2017: {df_summary['2017'].sum()}\")\n",
    "    print(f\"  - 2018: {df_summary['2018'].sum()}\")\n",
    "    print(f\"  - 2019: {df_summary['2019'].sum()}\")\n",
    "\n",
    "    return {\"summary\": summary_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b6e14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 299 names\n",
      "Unique: 288 people\n",
      "Duplicates: 11\n",
      "Duplicate names: ['Kim Jong Un', 'Jeff Bezos', 'Jacinda Ardern', 'LeBron James', 'Donald Trump', 'Nancy Pelosi', 'Xi Jinping', 'Robert Mueller', 'Pope Francis']\n",
      "\n",
      "Crawling list: ['Sonia Friedman', 'Jair Bolsonaro', 'Deepika Padukone', 'Jose Andres', 'Julian Assange', 'Margrethe Vestager', 'Tarana Burke', 'Tom Brady', 'Ava DuVernay', 'Carmen Yulin Cruz', 'Mahathir Mohamad', 'David Hockney', 'Desmond Meade', 'Cindy Sherman', 'Margaret Atwood', 'Chloe Kim', 'Mohammed bin Salman', 'Haider al Abadi', 'Moon Jae in', 'Donald Glover', 'Emma Gonzalez', 'Michelle Obama', 'RuPaul', 'Indya Moore', 'Samin Nosrat', 'Emmanuel Macron', 'John Legend', 'John Lewis', 'Hamdi Ulukaya', 'Leila de Lima', 'Evan Spiegel', 'Ezra Levin', 'Daniel Ek', 'Greta Thunberg', 'Lena Waithe', 'Ailbhe Smyth', 'Vladimir Putin', 'Virat Kohli', 'David Adjaye', 'Millie Bobby Brown', 'Rodrigo Duterte', 'Kehinde Wiley', 'Glenn Close', 'Judy Chicago', 'Gal Gadot', 'Alicia Keys', 'Hasan Minhaj', 'Christopher Wylie', 'Margot Robbie', 'Viola Davis', 'Jared Kushner', 'Jason Blum', 'James Monsees', 'Shep Doeleman', 'Tara Westover', 'Recep Tayyip Erdogan', 'Jaclyn Corin', 'Jennifer Hyman', 'Leana Wen', 'Demis Hassabis', 'Shinzo Abe', 'Kim Jong Un', 'Sheikh Hasina', 'Colin Kaepernick', 'Peggy Whitson', 'Mauricio Macri', 'Kevin Kwan', 'Jeff Bezos', 'Chadwick Boseman', 'Luchita Hurtado', 'Virgil Abloh', 'Prince Harry', 'Qasem Soleimani', 'Hugh Jackman', 'James Corden', 'Marlon James', 'Chance the Rapper', 'Ann McKee', 'Jodi Kantor', 'Carmen Perez', 'Jacinda Ardern', 'Marica Branchesi', 'Megan Twohey', 'Sandra Day OConnor', 'Adam Neumann', 'Jesmyn Ward', 'Trevor Noah', 'Benjamin Netanyahu', 'Christine Blasey Ford', 'Theresa May', 'Theo Epstein', 'LeBron James', 'Glenda Gray', 'Vera Jourova', 'Ryan Reynolds', 'Ryan Murphy', 'Bhavish Aggarwal', 'Linda Sarsour', 'He Jiankui', 'Donald Trump', 'Samantha Bee', 'Ed Sheeran', 'Oprah Winfrey', 'Jerome Powell', 'Cameron Kasky', 'Greta Gerwig', 'Mohamed Salah', 'Savannah Guthrie', 'Bob Bland', 'Wang Qishan', 'Caster Semenya', 'David Hogg', 'Ren Zhengfei', 'Leo Varadkar', 'Rihanna', 'Rachael Denhollander', 'Justin Trudeau', 'Masayoshi Son', 'Jeff Sessions', 'Nancy Pelosi', 'Fatou Bensouda', 'Clare Waight Keller', 'Grainne Griffin', 'Brett Kavanaugh', 'Alexandria Ocasio-Cortez', 'Cardi B', 'Satya Nadella', 'Sterling Brown', 'JJ Watt', 'Adam Rippon', 'Xi Jinping', 'Chrissy Teigen', 'Ninja Blevins', 'Conor McGregor', 'James Comey', 'Chip Gaines', 'Tiffany Haddish', 'Cristina Jimenez', 'Nicole Kidman', 'Emily Comer', 'Tamika Mallory', 'Vijay Shekhar Sharma', 'Taylor Swift', 'Pierpaolo Piccioli', 'Michael Gillon', 'Jean Liu', 'Reince Priebus', 'Sinta Nuriyah', 'Hoda Kotb', 'Kirsten Green', 'Maxine Waters', 'Barry Jenkins', 'Carl June', 'Zhang Kejian', 'Alessandro Michele', 'Stephen Bannon', 'Ariana Grande', 'Issa Rae', 'BTS', 'Alex Wind', 'Leah Greenberg', 'Roseanne Barr', 'Jeanette Vizguerra', 'Ozuna', 'Biram Dah Abeid', 'Abiy Ahmed', 'Yuriko Koike', 'Kevin Durant', 'Guus Velders', 'Janet Yellen', 'Ashley Graham', 'Jay ONeal', 'Emmerson Mnangagwa', 'Elizabeth Diller', 'JR artist', 'Emma Stone', 'Hoesung Lee', 'Imran Khan', 'George Church', 'Mahershala Ali', 'Massimo Bottura', 'Emilia Clarke', 'Robert Mueller', 'Tiger Woods', 'General James Mattis', 'Gayle King', 'Jan Rader', 'Bernard Tyson', 'Menaka Guruswamy', 'Neymar', 'Yalitza Aparicio', 'Zhang Yiming', 'Chuck Schumer', 'Loujain al Hathloul', 'Andres Manuel Lopez', 'Tom Perez', 'Whitney Wolfe Herd', 'Sandra Oh', 'Guillermo del Toro', 'Thelma Aldana', 'Ben Platt', 'Scott Pruitt', 'Christian Siriano', 'Demi Lovato', 'Adam Bowen', 'Meghan Markle', 'Ruth Davidson', 'Sean Hannity', 'Kenneth Frazier', 'Elon Musk', 'Lady Gaga', 'Elizabeth Warren', 'Sadiq Khan', 'Pony Ma', 'Riz Ahmed', 'Gavin Grimm', 'Rami Malek', 'Joanna Gaines', 'Ivanka Trump', 'Guillem Anglada Escude', 'Brie Larson', 'Jian Wei Pan', 'Radhya Almutawakel', 'Shawn Mendes', 'Janet Mock', 'Roger Federer', 'Constance Wu', 'Celina Turchi', 'Pat McGrath', 'Jeanne Gang', 'Orla OConnor', 'Jimmy Kimmel', 'Marillyn Hewson', 'Richard Madden', 'Jordan Peele', 'Giuliano Testa', 'Bob Ferguson', 'Juan Manuel Santos', 'Pope Francis', 'Mohamed bin Zayed', 'King Maha Vajiralongkorn', 'Regina King', 'Barbara Rae-Venter', 'Matteo Salvini', 'Maria Ressa', 'Narendra Modi', 'John Krasinski', 'dream hampton', 'Ronan Farrow', 'Simone Biles', 'Cindy Holland', 'Natalie Batalha', 'Gretchen Carlson', 'Jane Goodall', 'Arundhati Katju', 'Khalid', 'Spike Lee', 'Mirian G', 'Kerry James Marshall', 'Mark Zuckerberg', 'Alex Morgan', 'Juan Guaido', 'Fan Bingbing', 'Arlette Contreras', 'Aileen Lee', 'Bob Iger', 'Kesha', 'Barbara Lynch', 'James Allison', 'Jennifer Lopez', 'Dwayne Johnson', 'Raed Saleh', 'Nice Nailantei Lengete', 'Naomi Osaka', 'Colson Whitehead', 'Daniela Vega', 'Mukesh Ambani', 'Ryan Coogler', 'Lynn Nottage', 'Mitch McConnell', 'Rebekah Mercer', 'Kumail Nanjiani', 'Fred Swaniker', 'Cyril Ramaphosa', 'Raf Simons', 'William Barr', 'Leslie Jones', 'Sarah Paulson']\n",
      "\n",
      "\n",
      "[1/288] SKIP (exists): Sonia Friedman\n",
      "\n",
      "[2/288] SKIP (exists): Jair Bolsonaro\n",
      "\n",
      "[3/288] SKIP (exists): Deepika Padukone\n",
      "\n",
      "[4/288] SKIP (exists): Jose Andres\n",
      "\n",
      "[5/288] SKIP (exists): Julian Assange\n",
      "\n",
      "[6/288] SKIP (exists): Margrethe Vestager\n",
      "\n",
      "[7/288] SKIP (exists): Tarana Burke\n",
      "\n",
      "[8/288] SKIP (exists): Tom Brady\n",
      "\n",
      "[9/288] SKIP (exists): Ava DuVernay\n",
      "\n",
      "[10/288] SKIP (exists): Carmen Yulin Cruz\n",
      "\n",
      "[11/288] SKIP (exists): Mahathir Mohamad\n",
      "\n",
      "[12/288] SKIP (exists): David Hockney\n",
      "\n",
      "[13/288] SKIP (exists): Desmond Meade\n",
      "\n",
      "[14/288] SKIP (exists): Cindy Sherman\n",
      "\n",
      "[15/288] SKIP (exists): Margaret Atwood\n",
      "\n",
      "[16/288] SKIP (exists): Chloe Kim\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 4: Run =====\n",
    "\n",
    "# people_list.txt 파일 읽기\n",
    "with open('people_list.txt', 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "people_list = content.split('\\n')\n",
    "people_list = [name.strip() for name in people_list if name.strip()]\n",
    "\n",
    "# 중복 체크\n",
    "duplicates = [name for name in set(people_list) if people_list.count(name) > 1]\n",
    "people = list(set(people_list))\n",
    "\n",
    "print(f\"Total: {len(people_list)} names\")\n",
    "print(f\"Unique: {len(people)} people\")\n",
    "print(f\"Duplicates: {len(people_list) - len(people)}\")\n",
    "if duplicates:\n",
    "    print(f\"Duplicate names: {duplicates}\")\n",
    "print(f\"\\nCrawling list: {people}\")\n",
    "print()\n",
    "\n",
    "#people = [\"Samantha Bee\",\"Constance Wu\"]\n",
    "# 크롤링 실행\n",
    "paths = batch_crawl_and_summary(\n",
    "    people=people,\n",
    "    start_date=\"2017-01-01\",\n",
    "    end_date=\"2019-12-31\",\n",
    "    out_dir=\"guardian_raw_scraping\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58518828",
   "metadata": {},
   "source": [
    "# Only top100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e145a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 people by total articles: 100\n",
      "Total articles: 461270\n",
      "\n",
      "Top 10:\n",
      "          person  total\n",
      "    Donald Trump  26542\n",
      "     Theresa May  22188\n",
      "     Ryan Murphy  10453\n",
      "   Jeff Sessions   8688\n",
      "Elizabeth Warren   8476\n",
      "   Ryan Reynolds   8467\n",
      "    Ivanka Trump   8137\n",
      "    Ryan Coogler   7442\n",
      "    Prince Harry   6213\n",
      "     Kim Jong Un   6121\n",
      "\n",
      "Saved: people_top100_list.txt\n",
      "\n",
      "Copied 100 jsonl files to guardian_top100_scraping/\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 5: Extract Top 100 & Copy Files =====\n",
    "\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# guardian_raw_scraping_summary.csv 읽기\n",
    "df = pd.read_csv('guardian_raw_scraping_summary.csv')\n",
    "\n",
    "# total 기준 내림차순 정렬 후 top 100 추출\n",
    "df_sorted = df.sort_values('total', ascending=False).head(100)\n",
    "top100_people = df_sorted['person'].tolist()\n",
    "\n",
    "print(f\"Top 100 people by total articles: {len(top100_people)}\")\n",
    "print(f\"Total articles: {df_sorted['total'].sum()}\")\n",
    "print(f\"\\nTop 10:\")\n",
    "print(df_sorted[['person', 'total']].head(10).to_string(index=False))\n",
    "\n",
    "# people_top100_list.txt 저장\n",
    "with open('people_top100_list.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(top100_people))\n",
    "print(f\"\\nSaved: people_top100_list.txt\")\n",
    "\n",
    "# guardian_top100_scraping 폴더 생성\n",
    "new_dir = 'guardian_top100_scraping'\n",
    "os.makedirs(new_dir, exist_ok=True)\n",
    "\n",
    "# jsonl 파일 복사\n",
    "copied_count = 0\n",
    "for person in top100_people:\n",
    "    slug = df_sorted[df_sorted['person'] == person]['slug'].values[0]\n",
    "    src_file = f'guardian_raw_scraping/{slug}.jsonl'\n",
    "    dst_file = f'{new_dir}/{slug}.jsonl'\n",
    "    \n",
    "    if os.path.exists(src_file):\n",
    "        shutil.copy2(src_file, dst_file)\n",
    "        copied_count += 1\n",
    "\n",
    "print(f\"\\nCopied {copied_count} jsonl files to {new_dir}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
