{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxPS6NicNY8G",
        "outputId": "9723c67f-3e59-419c-b0cb-2ab924ac04ec"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: 환경 설정 및 라이브러리 임포트**"
      ],
      "metadata": {
        "id": "TbCR8fmu83tY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "j1Iv2gvS8u9s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a7d982f-30d3-41d9-c132-2851f697f377"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# 딥러닝(GRU) 라이브러리\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# 경로 설정\n",
        "DATA_DIR = Path(\"/content/drive/MyDrive/COSE362/data/feature_engineering\")\n",
        "OUTPUT_DIR = Path(\"/content/drive/MyDrive/COSE362/data/prediction_output\")\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# GPU 설정\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using Device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: 데이터 로드 및 날짜 분할 함수 정의**"
      ],
      "metadata": {
        "id": "YVvLVWFO9Fhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_split_data(file_path):\n",
        "    \"\"\"\n",
        "    Parquet 파일을 로드하고 날짜 기준으로 Train/Valid/Test로 분할\n",
        "    Train: 2017-01-01 ~ 2018-12-31\n",
        "    Valid: 2019-01-01 ~ 2019-06-30\n",
        "    Test:  2019-07-01 ~ 2019-12-31\n",
        "    \"\"\"\n",
        "    print(f\"Loading {file_path}...\")\n",
        "    df = pd.read_parquet(file_path)\n",
        "\n",
        "    # 1. 날짜순 정렬\n",
        "    if 'date_index' in df.columns:\n",
        "        df = df.sort_values('date_index').reset_index(drop=True)\n",
        "\n",
        "    # 2. 날짜 열(datetime 객체) 생성\n",
        "    # feature_engineering에서 만든 'date_str' (예: 2017_01_01)을 이용\n",
        "    if 'date_str' in df.columns:\n",
        "        df['Date'] = pd.to_datetime(df['date_str'], format='%Y_%m_%d')\n",
        "    elif 'article_date' in df.columns:\n",
        "        df['Date'] = pd.to_datetime(df['article_date'])\n",
        "    else:\n",
        "        # 만약 날짜 열이 없다면 에러 발생보다는 인덱스로 대체\n",
        "        print(\"Warning: Date column not found. Using index based splitting.\")\n",
        "        # 임시 날짜 생성\n",
        "        df['Date'] = pd.date_range(start='2017-01-01', periods=len(df), freq='D')\n",
        "\n",
        "    # 3. 날짜 기준 분할\n",
        "    train_mask = (df['Date'] >= '2017-01-01') & (df['Date'] <= '2018-12-31')\n",
        "    valid_mask = (df['Date'] >= '2019-01-01') & (df['Date'] <= '2019-06-30')\n",
        "    test_mask  = (df['Date'] >= '2019-07-01') & (df['Date'] <= '2019-12-31')\n",
        "\n",
        "    train_df = df[train_mask].copy()\n",
        "    valid_df = df[valid_mask].copy()\n",
        "    test_df  = df[test_mask].copy()\n",
        "\n",
        "    # 4. 결과 출력\n",
        "    print(f\"Train: {train_df.shape}, Valid: {valid_df.shape}, Test: {test_df.shape}\")\n",
        "\n",
        "    if len(train_df) == 0:\n",
        "        print(\"!!! Warning: Train set is empty. Check date format !!!\")\n",
        "\n",
        "    return train_df, valid_df, test_df"
      ],
      "metadata": {
        "id": "Djm4g-lo9J8t"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Feature & Target 준비**"
      ],
      "metadata": {
        "id": "gFuys3fH9Nfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_xy(df, target_col='value'):\n",
        "    \"\"\"\n",
        "    데이터프레임에서 학습에 방해되는 메타데이터를 제거하고\n",
        "    X(입력 Feature)와 y(정답 Target)를 분리\n",
        "    \"\"\"\n",
        "    # ---------------------------------------------------------\n",
        "    # 1. Target 생성 (내일의 주가를 예측 목표로 설정)\n",
        "    # ---------------------------------------------------------\n",
        "    # 오늘 행에 내일 주가(target)를 정답지로 붙임\n",
        "    df['target'] = df[target_col].shift(-1)\n",
        "\n",
        "    # 마지막 날은 내일 데이터가 없으므로(NaN) 삭제함\n",
        "    df = df.dropna(subset=['target'])\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 2. 제거할 열 정의\n",
        "    # ---------------------------------------------------------\n",
        "    # 모델은 오직 '숫자'만 계산할 수 있어서\n",
        "    # 일부 열들은 학습에서 제외해야 함\n",
        "    cols_to_drop = [\n",
        "        # [예측 목표]\n",
        "        'target',               # 정답지는 입력(X)에서 빼기\n",
        "\n",
        "        # [날짜 관련]\n",
        "        'Date',                 # load_and_split_data에서 생성된 datetime 객체\n",
        "        'date_str',             # '2017_01_01' (문자열)\n",
        "        'article_date',         # datetime 객체\n",
        "        'pub_date',             # '2017_12_31' (문자열)\n",
        "        'date_index',           # 0, 1, 2... (단순 인덱스)\n",
        "\n",
        "        # [기사/인물 메타데이터 (문자열/ID)]\n",
        "        'person',               # 'alex_morgan' (문자열)\n",
        "        'person_id',            # 1, 2... (숫자지만 One-hot 벡터와 중복됨)\n",
        "        'article_id',           # 기사 고유 ID (단순 인덱스)\n",
        "        'headline',             # 기사 제목 (문자열)\n",
        "        'idx',                  # 임베딩 파일 내부 인덱스\n",
        "\n",
        "        # [기타]\n",
        "        'embedding'             # 혹시 '_orig' 파일을 쓸 경우, 리스트 형태라 모델에 바로 못 넣음\n",
        "    ]\n",
        "\n",
        "    # 실제 데이터프레임에 존재하는 열만 골라서 제거 (에러 방지)\n",
        "    actual_drop_cols = [c for c in cols_to_drop if c in df.columns]\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 3. X, y 분리\n",
        "    # ---------------------------------------------------------\n",
        "    X = df.drop(columns=actual_drop_cols)\n",
        "    y = df['target']\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 모델에 들어가는 최종 Features\n",
        "    # 1. value (오늘 주가)\n",
        "    # 2. lag_1 ~ lag_5 (과거 주가)\n",
        "    # 3. pca_0 ~ pca_N (뉴스 임베딩 특징)\n",
        "    # 4. person_1 ~ person_100 (인물 One-hot)\n",
        "    # 5. fg_value, fg_lag_* (공포지수)\n",
        "    # ---------------------------------------------------------\n",
        "\n",
        "    return X, y, df['Date'] # 날짜 정보는 나중에 그래프 그릴 때 활용 가능해서 같이 반환"
      ],
      "metadata": {
        "id": "M7nEzNvW9Rl4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **[모델 1: Linear Regression]**"
      ],
      "metadata": {
        "id": "_ghkZ4vf9S6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "def run_regularized_regression(dataset_name, file_path, model_type=\"ridge\", alpha=1.0):\n",
        "    print(f\"\\n=== Running {model_type.upper()} (alpha={alpha}) on {dataset_name} ===\")\n",
        "\n",
        "    # 1. 데이터 로드\n",
        "    train_df, valid_df, test_df = load_and_split_data(file_path)\n",
        "\n",
        "    # 2. X, y 준비\n",
        "    X_train, y_train, _ = prepare_xy(train_df)\n",
        "    X_test, y_test, dates_test = prepare_xy(test_df)\n",
        "\n",
        "    # 메모리 확보\n",
        "    del train_df, valid_df, test_df\n",
        "    gc.collect()\n",
        "\n",
        "    # 데이터 타입 다운캐스팅\n",
        "    X_train = X_train.astype(np.float32)\n",
        "    y_train = y_train.astype(np.float32)\n",
        "    X_test = X_test.astype(np.float32)\n",
        "\n",
        "    # 3. 모델 정의\n",
        "    if model_type == \"ridge\":\n",
        "        model = make_pipeline(StandardScaler(), Ridge(alpha=alpha))\n",
        "    elif model_type == \"lasso\":\n",
        "        model = make_pipeline(StandardScaler(), Lasso(alpha=alpha))\n",
        "    else:\n",
        "        model = make_pipeline(StandardScaler(), LinearRegression())\n",
        "\n",
        "    # 4. 학습\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # 학습 데이터 삭제\n",
        "    del X_train, y_train\n",
        "    gc.collect()\n",
        "\n",
        "    # 5. 예측\n",
        "    preds_test = model.predict(X_test)\n",
        "\n",
        "    # 6. 평가\n",
        "    mse = mean_squared_error(y_test, preds_test)\n",
        "    print(f\"Test MSE: {mse:.4f}\")\n",
        "\n",
        "    # 7. 결과 저장\n",
        "    result_data = []\n",
        "    for date, true, pred in zip(dates_test, y_test, preds_test):\n",
        "        result_data.append({\n",
        "            \"date\": date.strftime('%Y-%m-%d'),\n",
        "            \"actual\": float(true),\n",
        "            \"predicted\": float(pred)\n",
        "        })\n",
        "\n",
        "    with open(OUTPUT_DIR / f\"pred_{model_type}_{dataset_name}.json\", \"w\") as f:\n",
        "        json.dump(result_data, f, indent=4)\n",
        "\n",
        "    del model, X_test, y_test, preds_test\n",
        "    gc.collect()\n",
        "\n",
        "    return mse"
      ],
      "metadata": {
        "id": "qKxqX18y9ZVL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **[모델 2: GRU(Deep Learning)]**"
      ],
      "metadata": {
        "id": "gfjUHzMa9b0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "# 1. Dataset 정의 (Lazy Loading)\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, X, y, seq_length=5):\n",
        "        # 데이터를 미리 텐서(float32)로 변환하여 저장\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X) - self.seq_length\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # 호출될 때마다 슬라이싱해서 반환\n",
        "        return self.X[i : i + self.seq_length], self.y[i + self.seq_length]\n",
        "\n",
        "# 2. GRU 모델 정의\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64, output_dim=1, num_layers=2):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.gru(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# 3. run_gru 함수\n",
        "def run_gru(dataset_name, file_path, seq_length=5, epochs=30): # Epochs 30으로 조정\n",
        "    print(f\"\\n=== Running GRU on {dataset_name} ===\")\n",
        "\n",
        "    # --- 데이터 로드 ---\n",
        "    train_df, valid_df, test_df = load_and_split_data(file_path)\n",
        "\n",
        "    # --- 전처리 (Numpy 변환) ---\n",
        "    # DataFrame 상태로 두지 않고 바로 Numpy로 변환\n",
        "    X_train_raw, y_train_raw, _ = prepare_xy(train_df)\n",
        "    X_valid_raw, y_valid_raw, _ = prepare_xy(valid_df)\n",
        "    X_test_raw, y_test_raw, dates_test = prepare_xy(test_df)\n",
        "\n",
        "    del train_df, valid_df, test_df\n",
        "    gc.collect()\n",
        "\n",
        "    # --- 스케일링 ---\n",
        "    scaler_x = MinMaxScaler()\n",
        "    scaler_y = MinMaxScaler()\n",
        "\n",
        "    X_train_scaled = scaler_x.fit_transform(X_train_raw).astype(np.float32)\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train_raw.values.reshape(-1, 1)).astype(np.float32)\n",
        "\n",
        "    X_valid_scaled = scaler_x.transform(X_valid_raw).astype(np.float32)\n",
        "    y_valid_scaled = scaler_y.transform(y_valid_raw.values.reshape(-1, 1)).astype(np.float32)\n",
        "\n",
        "    X_test_scaled = scaler_x.transform(X_test_raw).astype(np.float32)\n",
        "    y_test_raw_values = y_test_raw.values.astype(np.float32) # 정답 비교용\n",
        "\n",
        "    # --- Dataset & DataLoader (Lazy Loading 적용) ---\n",
        "    train_dataset = TimeSeriesDataset(X_train_scaled, y_train_scaled, seq_length)\n",
        "    # num_workers=0 설정\n",
        "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=0)\n",
        "\n",
        "    # Test용 Dataset (shuffle=False)\n",
        "    test_dataset = TimeSeriesDataset(X_test_scaled, y_test_raw_values, seq_length) # y값은 dummy로 넣어도 됨\n",
        "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=0)\n",
        "\n",
        "    # --- 모델 초기화 ---\n",
        "    input_dim = X_train_scaled.shape[1]\n",
        "    model = GRUModel(input_dim).to(DEVICE)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # --- 학습 루프 ---\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for bx, by in train_loader:\n",
        "            bx, by = bx.to(DEVICE), by.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(bx)\n",
        "            loss = criterion(output, by)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # 로그 출력 (10 epoch 마다)\n",
        "        if (epoch+1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.6f}\")\n",
        "\n",
        "    # --- 예측 (Batch Processing) ---\n",
        "    model.eval()\n",
        "    preds_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for bx, _ in test_loader:\n",
        "            bx = bx.to(DEVICE)\n",
        "            out = model(bx)\n",
        "            preds_list.append(out.cpu().numpy())\n",
        "\n",
        "    # 결과 합치기\n",
        "    test_preds_scaled = np.vstack(preds_list)\n",
        "    test_preds = scaler_y.inverse_transform(test_preds_scaled)\n",
        "\n",
        "    # 정답 데이터 길이 맞춤 (앞부분 seq_length 만큼 잘림)\n",
        "    y_test_true = y_test_raw_values[seq_length:]\n",
        "    test_dates_clipped = dates_test.values[seq_length:]\n",
        "\n",
        "    # --- 평가 ---\n",
        "    mse = mean_squared_error(y_test_true, test_preds)\n",
        "    print(f\"Test MSE: {mse:.4f}\")\n",
        "\n",
        "    # --- 저장 ---\n",
        "    result_data = []\n",
        "    for date, true, pred in zip(test_dates_clipped, y_test_true, test_preds):\n",
        "        date_str = pd.to_datetime(date).strftime('%Y-%m-%d')\n",
        "        result_data.append({\n",
        "            \"date\": date_str,\n",
        "            \"actual\": float(true),\n",
        "            \"predicted\": float(pred[0])\n",
        "        })\n",
        "\n",
        "    with open(OUTPUT_DIR / f\"pred_gru_{dataset_name}.json\", \"w\") as f:\n",
        "        json.dump(result_data, f, indent=4)\n",
        "\n",
        "    del model, optimizer, train_loader, test_loader, train_dataset, test_dataset\n",
        "    del X_train_scaled, X_valid_scaled, X_test_scaled\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return mse"
      ],
      "metadata": {
        "id": "DHd2Qrsn9i9T"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: 전체 실행 및 결과 집계**"
      ],
      "metadata": {
        "id": "DmrfyrLv9ms2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "# ====================================================\n",
        "# Main Execution Loop: All Datasets\n",
        "# ====================================================\n",
        "\n",
        "levels = ['B', 'C', 'D']\n",
        "methods = ['headlines', 'chunking', 'bodyText', 'paragraphs']\n",
        "types = ['pca']\n",
        "\n",
        "metrics_data = []\n",
        "\n",
        "# --- Baseline: Dataset A ---\n",
        "file_path_A = DATA_DIR / \"dataset_A.parquet\"\n",
        "if file_path_A.exists():\n",
        "    print(f\"\\n>>> Processing Baseline: Dataset A\")\n",
        "    mse_ridge = run_regularized_regression(\"Dataset_A\", file_path_A, model_type=\"ridge\")\n",
        "    metrics_data.append({\"Dataset\": \"A\", \"Method\": \"-\", \"Type\": \"-\", \"Model\": \"Ridge\", \"MSE\": mse_ridge})\n",
        "\n",
        "    mse_gru = run_gru(\"Dataset_A\", file_path_A)\n",
        "    metrics_data.append({\"Dataset\": \"A\", \"Method\": \"-\", \"Type\": \"-\", \"Model\": \"GRU\", \"MSE\": mse_gru})\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "else:\n",
        "    print(\"Warning: dataset_A.parquet not found.\")\n",
        "\n",
        "\n",
        "# --- Main Loop ---\n",
        "for level in levels:\n",
        "    for method in methods:\n",
        "        for dtype in types:\n",
        "            filename = f\"dataset_{level}_{method}_{dtype}.parquet\"\n",
        "            file_path = DATA_DIR / filename\n",
        "            dataset_name = f\"{level}_{method}_{dtype}\"\n",
        "\n",
        "            if not file_path.exists():\n",
        "                print(f\"Skipping {filename}: File not found.\")\n",
        "                continue\n",
        "\n",
        "            print(f\"\\n>>> Processing {filename} ...\")\n",
        "\n",
        "            try:\n",
        "                # 1. Ridge\n",
        "                mse_ridge = run_regularized_regression(dataset_name, file_path, model_type=\"ridge\")\n",
        "                metrics_data.append({\n",
        "                    \"Dataset\": level, \"Method\": method, \"Type\": dtype,\n",
        "                    \"Model\": \"Ridge\", \"MSE\": mse_ridge\n",
        "                })\n",
        "\n",
        "                # 2. GRU\n",
        "                mse_gru = run_gru(dataset_name, file_path, seq_length=5, epochs=30)\n",
        "                metrics_data.append({\n",
        "                    \"Dataset\": level, \"Method\": method, \"Type\": dtype,\n",
        "                    \"Model\": \"GRU\", \"MSE\": mse_gru\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "            finally:\n",
        "                gc.collect()\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "# --- 최종 결과 저장 ---\n",
        "metrics_df = pd.DataFrame(metrics_data)\n",
        "metrics_df = metrics_df.sort_values(by=[\"Dataset\", \"Method\", \"Model\"])\n",
        "metrics_df.to_csv(OUTPUT_DIR / \"final_evaluation_metrics.csv\", index=False)\n",
        "\n",
        "print(\"\\n=== Final Experiment Results (Sample) ===\")\n",
        "print(metrics_df.head(10))\n",
        "print(f\"\\nSaved to: {OUTPUT_DIR / 'final_evaluation_metrics.csv'}\")"
      ],
      "metadata": {
        "id": "7s-Hgxvp9mBn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9778c460-7ab0-413b-e84f-e32de3ce0c0c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> Processing Baseline: Dataset A\n",
            "\n",
            "=== Running RIDGE (alpha=1.0) on Dataset_A ===\n",
            "Loading /content/drive/MyDrive/COSE362/data/feature_engineering/dataset_A.parquet...\n",
            "Train: (502, 9), Valid: (124, 9), Test: (128, 9)\n",
            "Test MSE: 480.3128\n",
            "\n",
            "=== Running GRU on Dataset_A ===\n",
            "Loading /content/drive/MyDrive/COSE362/data/feature_engineering/dataset_A.parquet...\n",
            "Train: (502, 9), Valid: (124, 9), Test: (128, 9)\n",
            "Epoch 10/30, Loss: 0.006328\n",
            "Epoch 20/30, Loss: 0.003206\n",
            "Epoch 30/30, Loss: 0.002886\n",
            "Test MSE: 3823.1785\n",
            "\n",
            ">>> Processing dataset_B_headlines_pca.parquet ...\n",
            "\n",
            "=== Running RIDGE (alpha=1.0) on B_headlines_pca ===\n",
            "Loading /content/drive/MyDrive/COSE362/data/feature_engineering/dataset_B_headlines_pca.parquet...\n",
            "Train: (312535, 417), Valid: (76232, 417), Test: (72503, 417)\n",
            "Test MSE: 0.7667\n",
            "\n",
            "=== Running GRU on B_headlines_pca ===\n",
            "Loading /content/drive/MyDrive/COSE362/data/feature_engineering/dataset_B_headlines_pca.parquet...\n",
            "Train: (312535, 417), Valid: (76232, 417), Test: (72503, 417)\n",
            "Epoch 10/30, Loss: 0.000061\n",
            "Epoch 20/30, Loss: 0.000007\n",
            "Epoch 30/30, Loss: 0.000004\n",
            "Test MSE: 2336.6987\n",
            "\n",
            ">>> Processing dataset_B_chunking_pca.parquet ...\n",
            "\n",
            "=== Running RIDGE (alpha=1.0) on B_chunking_pca ===\n",
            "Loading /content/drive/MyDrive/COSE362/data/feature_engineering/dataset_B_chunking_pca.parquet...\n",
            "Train: (312185, 355), Valid: (76136, 355), Test: (72401, 355)\n",
            "Test MSE: 0.7675\n",
            "\n",
            "=== Running GRU on B_chunking_pca ===\n",
            "Loading /content/drive/MyDrive/COSE362/data/feature_engineering/dataset_B_chunking_pca.parquet...\n",
            "Train: (312185, 355), Valid: (76136, 355), Test: (72401, 355)\n",
            "Epoch 10/30, Loss: 0.000020\n",
            "Epoch 20/30, Loss: 0.000016\n",
            "Epoch 30/30, Loss: 0.000002\n",
            "Test MSE: 729.9954\n",
            "\n",
            ">>> Processing dataset_B_bodyText_pca.parquet ...\n",
            "\n",
            "=== Running RIDGE (alpha=1.0) on B_bodyText_pca ===\n",
            "Loading /content/drive/MyDrive/COSE362/data/feature_engineering/dataset_B_bodyText_pca.parquet...\n",
            "Train: (312185, 266), Valid: (76136, 266), Test: (72401, 266)\n",
            "Test MSE: 0.7673\n",
            "\n",
            "=== Running GRU on B_bodyText_pca ===\n",
            "Loading /content/drive/MyDrive/COSE362/data/feature_engineering/dataset_B_bodyText_pca.parquet...\n",
            "Train: (312185, 266), Valid: (76136, 266), Test: (72401, 266)\n",
            "Epoch 10/30, Loss: 0.000054\n",
            "Epoch 20/30, Loss: 0.000008\n",
            "Epoch 30/30, Loss: 0.000006\n",
            "Test MSE: 934.4212\n",
            "\n",
            ">>> Processing dataset_B_paragraphs_pca.parquet ...\n",
            "\n",
            "=== Running RIDGE (alpha=1.0) on B_paragraphs_pca ===\n",
            "Loading /content/drive/MyDrive/COSE362/data/feature_engineering/dataset_B_paragraphs_pca.parquet...\n",
            "Train: (312185, 291), Valid: (76136, 291), Test: (72401, 291)\n",
            "Test MSE: 0.7672\n",
            "\n",
            "=== Running GRU on B_paragraphs_pca ===\n",
            "Loading /content/drive/MyDrive/COSE362/data/feature_engineering/dataset_B_paragraphs_pca.parquet...\n",
            "Train: (312185, 291), Valid: (76136, 291), Test: (72401, 291)\n",
            "Epoch 10/30, Loss: 0.000021\n",
            "Epoch 20/30, Loss: 0.000003\n",
            "Epoch 30/30, Loss: 0.000001\n",
            "Test MSE: 1170.6049\n",
            "\n",
            ">>> Processing dataset_C_headlines_pca.parquet ...\n",
            "\n",
            "=== Running RIDGE (alpha=1.0) on C_headlines_pca ===\n",
            "Loading /content/drive/MyDrive/COSE362/data/feature_engineering/dataset_C_headlines_pca.parquet...\n",
            "Train: (312535, 517), Valid: (76232, 517), Test: (72503, 517)\n",
            "Test MSE: 0.7669\n",
            "\n",
            "=== Running GRU on C_headlines_pca ===\n",
            "Loading /content/drive/MyDrive/COSE362/data/feature_engineering/dataset_C_headlines_pca.parquet...\n",
            "Train: (312535, 517), Valid: (76232, 517), Test: (72503, 517)\n",
            "Epoch 10/30, Loss: 0.000068\n",
            "Epoch 20/30, Loss: 0.000078\n",
            "Epoch 30/30, Loss: 0.000222\n",
            "Test MSE: 2105.3518\n",
            "\n",
            ">>> Processing dataset_C_chunking_pca.parquet ...\n",
            "\n",
            "=== Running RIDGE (alpha=1.0) on C_chunking_pca ===\n",
            "Loading /content/drive/MyDrive/COSE362/data/feature_engineering/dataset_C_chunking_pca.parquet...\n",
            "Train: (312185, 455), Valid: (76136, 455), Test: (72401, 455)\n",
            "Test MSE: 0.7678\n",
            "\n",
            "=== Running GRU on C_chunking_pca ===\n",
            "Loading /content/drive/MyDrive/COSE362/data/feature_engineering/dataset_C_chunking_pca.parquet...\n",
            "Train: (312185, 455), Valid: (76136, 455), Test: (72401, 455)\n",
            "Epoch 10/30, Loss: 0.000083\n",
            "Epoch 20/30, Loss: 0.000023\n",
            "Epoch 30/30, Loss: 0.000020\n",
            "Test MSE: 2770.1033\n",
            "\n",
            ">>> Processing dataset_C_bodyText_pca.parquet ...\n",
            "\n",
            "=== Running RIDGE (alpha=1.0) on C_bodyText_pca ===\n",
            "Loading /content/drive/MyDrive/COSE362/data/feature_engineering/dataset_C_bodyText_pca.parquet...\n",
            "Train: (312185, 366), Valid: (76136, 366), Test: (72401, 366)\n",
            "Test MSE: 0.7676\n",
            "\n",
            "=== Running GRU on C_bodyText_pca ===\n",
            "Loading /content/drive/MyDrive/COSE362/data/feature_engineering/dataset_C_bodyText_pca.parquet...\n",
            "Train: (312185, 366), Valid: (76136, 366), Test: (72401, 366)\n",
            "Epoch 10/30, Loss: 0.000155\n",
            "Epoch 20/30, Loss: 0.000003\n",
            "Epoch 30/30, Loss: 0.000010\n",
            "Test MSE: 1602.8322\n",
            "\n",
            ">>> Processing dataset_C_paragraphs_pca.parquet ...\n",
            "\n",
            "=== Running RIDGE (alpha=1.0) on C_paragraphs_pca ===\n",
            "Loading /content/drive/MyDrive/COSE362/data/feature_engineering/dataset_C_paragraphs_pca.parquet...\n",
            "Train: (312185, 391), Valid: (76136, 391), Test: (72401, 391)\n",
            "Test MSE: 0.7675\n",
            "\n",
            "=== Running GRU on C_paragraphs_pca ===\n",
            "Loading /content/drive/MyDrive/COSE362/data/feature_engineering/dataset_C_paragraphs_pca.parquet...\n",
            "Train: (312185, 391), Valid: (76136, 391), Test: (72401, 391)\n",
            "Epoch 10/30, Loss: 0.000032\n",
            "Epoch 20/30, Loss: 0.000031\n",
            "Epoch 30/30, Loss: 0.000047\n",
            "Test MSE: 1670.7587\n",
            "\n",
            ">>> Processing dataset_D_headlines_pca.parquet ...\n",
            "\n",
            "=== Running RIDGE (alpha=1.0) on D_headlines_pca ===\n",
            "Loading /content/drive/MyDrive/COSE362/data/feature_engineering/dataset_D_headlines_pca.parquet...\n",
            "Train: (312535, 523), Valid: (76232, 523), Test: (72503, 523)\n",
            "Test MSE: 0.7665\n",
            "\n",
            "=== Running GRU on D_headlines_pca ===\n",
            "Loading /content/drive/MyDrive/COSE362/data/feature_engineering/dataset_D_headlines_pca.parquet...\n",
            "Train: (312535, 523), Valid: (76232, 523), Test: (72503, 523)\n",
            "Epoch 10/30, Loss: 0.000032\n",
            "Epoch 20/30, Loss: 0.000062\n",
            "Epoch 30/30, Loss: 0.000015\n",
            "Test MSE: 741.9156\n",
            "\n",
            ">>> Processing dataset_D_chunking_pca.parquet ...\n",
            "\n",
            "=== Running RIDGE (alpha=1.0) on D_chunking_pca ===\n",
            "Loading /content/drive/MyDrive/COSE362/data/feature_engineering/dataset_D_chunking_pca.parquet...\n",
            "Train: (312185, 461), Valid: (76136, 461), Test: (72401, 461)\n",
            "Test MSE: 0.7675\n",
            "\n",
            "=== Running GRU on D_chunking_pca ===\n",
            "Loading /content/drive/MyDrive/COSE362/data/feature_engineering/dataset_D_chunking_pca.parquet...\n",
            "Train: (312185, 461), Valid: (76136, 461), Test: (72401, 461)\n",
            "Epoch 10/30, Loss: 0.000042\n",
            "Epoch 20/30, Loss: 0.000059\n",
            "Epoch 30/30, Loss: 0.000004\n",
            "Test MSE: 2096.7725\n",
            "\n",
            ">>> Processing dataset_D_bodyText_pca.parquet ...\n",
            "\n",
            "=== Running RIDGE (alpha=1.0) on D_bodyText_pca ===\n",
            "Loading /content/drive/MyDrive/COSE362/data/feature_engineering/dataset_D_bodyText_pca.parquet...\n",
            "Train: (312185, 372), Valid: (76136, 372), Test: (72401, 372)\n",
            "Test MSE: 0.7673\n",
            "\n",
            "=== Running GRU on D_bodyText_pca ===\n",
            "Loading /content/drive/MyDrive/COSE362/data/feature_engineering/dataset_D_bodyText_pca.parquet...\n",
            "Train: (312185, 372), Valid: (76136, 372), Test: (72401, 372)\n",
            "Epoch 10/30, Loss: 0.000022\n",
            "Epoch 20/30, Loss: 0.000119\n",
            "Epoch 30/30, Loss: 0.000011\n",
            "Test MSE: 448.3315\n",
            "\n",
            ">>> Processing dataset_D_paragraphs_pca.parquet ...\n",
            "\n",
            "=== Running RIDGE (alpha=1.0) on D_paragraphs_pca ===\n",
            "Loading /content/drive/MyDrive/COSE362/data/feature_engineering/dataset_D_paragraphs_pca.parquet...\n",
            "Train: (312185, 397), Valid: (76136, 397), Test: (72401, 397)\n",
            "Test MSE: 0.7671\n",
            "\n",
            "=== Running GRU on D_paragraphs_pca ===\n",
            "Loading /content/drive/MyDrive/COSE362/data/feature_engineering/dataset_D_paragraphs_pca.parquet...\n",
            "Train: (312185, 397), Valid: (76136, 397), Test: (72401, 397)\n",
            "Epoch 10/30, Loss: 0.000016\n",
            "Epoch 20/30, Loss: 0.000074\n",
            "Epoch 30/30, Loss: 0.000004\n",
            "Test MSE: 1252.6591\n",
            "\n",
            "=== Final Experiment Results (Sample) ===\n",
            "  Dataset      Method Type  Model          MSE\n",
            "1       A           -    -    GRU  3823.178467\n",
            "0       A           -    -  Ridge   480.312796\n",
            "7       B    bodyText  pca    GRU   934.421204\n",
            "6       B    bodyText  pca  Ridge     0.767345\n",
            "5       B    chunking  pca    GRU   729.995361\n",
            "4       B    chunking  pca  Ridge     0.767483\n",
            "3       B   headlines  pca    GRU  2336.698730\n",
            "2       B   headlines  pca  Ridge     0.766737\n",
            "9       B  paragraphs  pca    GRU  1170.604858\n",
            "8       B  paragraphs  pca  Ridge     0.767214\n",
            "\n",
            "Saved to: /content/drive/MyDrive/COSE362/data/prediction_output/final_evaluation_metrics.csv\n"
          ]
        }
      ]
    }
  ]
}