{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad6c1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GUARDIAN_API_KEY'] = \"YOUR_API_KEY_HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f29377",
   "metadata": {},
   "source": [
    "## 1. Setup & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0450cf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json, time, pathlib\n",
    "from datetime import datetime, date\n",
    "from typing import Dict, List, Tuple, Iterable\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "API_KEY = os.getenv(\"GUARDIAN_API_KEY\", \"\").strip()\n",
    "assert API_KEY, \"Set GUARDIAN_API_KEY environment variable.\"\n",
    "\n",
    "BASE_SEARCH = \"https://content.guardianapis.com/search\"\n",
    "OUTPUT_COLS = [\n",
    "    \"id\",\"webPublicationDate\",\"headline\",\"trailText\",\"bodyText\",\n",
    "    \"webTitle\",\"webUrl\",\"apiUrl\",\"wordcount\"\n",
    "]\n",
    "\n",
    "def slugify(s: str) -> str:\n",
    "    \"\"\"파일명으로 사용 가능한 slug 생성\"\"\"\n",
    "    s = re.sub(r'[^0-9A-Za-z]+', '_', s.lower()).strip('_')\n",
    "    return s or \"query\"\n",
    "\n",
    "def year_slices(start_date: str, end_date: str) -> List[Tuple[int, str, str]]:\n",
    "    \"\"\"날짜 범위를 연도별로 분할\"\"\"\n",
    "    sd = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
    "    ed = datetime.strptime(end_date, \"%Y-%m-%d\").date()\n",
    "    assert sd <= ed, \"start_date must be <= end_date\"\n",
    "    out = []\n",
    "    y = sd.year\n",
    "    while y <= ed.year:\n",
    "        s = max(sd, date(y,1,1))\n",
    "        e = min(ed, date(y,12,31))\n",
    "        out.append((y, s.isoformat(), e.isoformat()))\n",
    "        y += 1\n",
    "    return out\n",
    "\n",
    "def guardian_get(params: Dict, max_retries: int = 6) -> Dict:\n",
    "    \"\"\"Guardian API 호출 (에러 시 자동 재시도)\"\"\"\n",
    "    p = dict(params)\n",
    "    p[\"api-key\"] = API_KEY\n",
    "    sleep = 1.5\n",
    "    for _ in range(max_retries):\n",
    "        r = requests.get(BASE_SEARCH, params=p, timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            return r.json()\n",
    "        if r.status_code in (429, 502, 503, 504):\n",
    "            time.sleep(sleep)\n",
    "            sleep *= 2\n",
    "            continue\n",
    "        r.raise_for_status()\n",
    "    raise RuntimeError(f\"Guardian API failed: {r.status_code} {r.text[:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87731940",
   "metadata": {},
   "source": [
    "## 2. Fetch & Save Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9031f3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_results(q: str, from_date: str, to_date: str,\n",
    "                 query_fields=(\"headline\",\"body\"), page_size: int = 200) -> Iterable[Dict]:\n",
    "    \"\"\"모든 페이지의 검색 결과를 yield\"\"\"\n",
    "    params = {\n",
    "        \"q\": q.lower(),\n",
    "        \"from-date\": from_date,\n",
    "        \"to-date\": to_date,\n",
    "        \"page-size\": page_size,\n",
    "        \"order-by\": \"newest\",\n",
    "        \"use-date\": \"published\",\n",
    "        \"query-fields\": \",\".join(query_fields),\n",
    "        \"show-fields\": \"headline,trailText,bodyText,thumbnail,wordcount\",\n",
    "        \"show-tags\": \"all\",\n",
    "    }\n",
    "    js = guardian_get(params)\n",
    "    resp = js.get(\"response\", {})\n",
    "    pages = int(resp.get(\"pages\", 0)) or 0\n",
    "    \n",
    "    for it in resp.get(\"results\", []):\n",
    "        yield it\n",
    "    \n",
    "    for p in range(2, pages + 1):\n",
    "        params[\"page\"] = p\n",
    "        js = guardian_get(params)\n",
    "        for it in js.get(\"response\", {}).get(\"results\", []):\n",
    "            yield it\n",
    "\n",
    "def to_row(it: Dict) -> Dict:\n",
    "    \"\"\"검색 결과를 출력 스키마로 변환\"\"\"\n",
    "    f = it.get(\"fields\") or {}\n",
    "    return {\n",
    "        \"id\": it.get(\"id\"),\n",
    "        \"webPublicationDate\": it.get(\"webPublicationDate\"),\n",
    "        \"headline\": f.get(\"headline\"),\n",
    "        \"trailText\": f.get(\"trailText\"),\n",
    "        \"bodyText\": f.get(\"bodyText\"),\n",
    "        \"webTitle\": it.get(\"webTitle\"),\n",
    "        \"webUrl\": it.get(\"webUrl\"),\n",
    "        \"apiUrl\": it.get(\"apiUrl\"),\n",
    "        \"wordcount\": f.get(\"wordcount\"),\n",
    "    }\n",
    "\n",
    "def crawl_and_save(query: str, start_date: str, end_date: str,\n",
    "                   out_dir: str = \"guardian_raw_scraping\",\n",
    "                   query_fields=(\"headline\",\"body\")) -> None:\n",
    "    \"\"\"크롤링 후 JSONL, CSV 저장\"\"\"\n",
    "    slug = slugify(query)\n",
    "    base = pathlib.Path(out_dir)\n",
    "    base.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 모든 연도의 결과 수집\n",
    "    seen, rows = set(), []\n",
    "    for y, y_start, y_end in year_slices(start_date, end_date):\n",
    "        print(f\"  [{query}] Crawling {y}: {y_start} ~ {y_end}\")\n",
    "        for it in iter_results(query, y_start, y_end, query_fields=query_fields):\n",
    "            _id = it.get(\"id\")\n",
    "            if _id in seen:\n",
    "                continue\n",
    "            seen.add(_id)\n",
    "            rows.append(to_row(it))\n",
    "\n",
    "    # JSONL 저장\n",
    "    jsonl_path = base / f\"{slug}.jsonl\"\n",
    "    with jsonl_path.open(\"w\", encoding=\"utf-8\") as jf:\n",
    "        for r in rows:\n",
    "            jf.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"  [{query}] Total rows={len(rows)}  JSONL={jsonl_path.name}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3346aa47",
   "metadata": {},
   "source": [
    "## 3. Batch Crawl & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6603c441",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batch_crawl_and_summary(\n",
    "    people: List[str],\n",
    "    start_date: str = \"2017-01-01\",\n",
    "    end_date: str = \"2019-12-31\",\n",
    "    out_dir: str = \"guardian_scraping\",\n",
    "    query_fields=(\"headline\",\"body\"),\n",
    "    skip_existing: bool = True,\n",
    ") -> Dict[str, pathlib.Path]:\n",
    "    \"\"\"여러 인물에 대해 크롤링 후 summary CSV 생성\"\"\"\n",
    "    out_base = pathlib.Path(out_dir)\n",
    "    out_base.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    summary_data = []\n",
    "    \n",
    "    for i, p in enumerate(people, 1):\n",
    "        slug = slugify(p)\n",
    "        jsonl_path = out_base / f\"{slug}.jsonl\"\n",
    "        \n",
    "        # 기존 파일 있으면 스킵\n",
    "        if skip_existing and jsonl_path.exists():\n",
    "            print(f\"\\n[{i}/{len(people)}] SKIP (exists): {p}\")\n",
    "        else:\n",
    "            print(f\"\\n[{i}/{len(people)}] Crawling: {p}\")\n",
    "            crawl_and_save(\n",
    "                query=p,\n",
    "                start_date=start_date,\n",
    "                end_date=end_date,\n",
    "                out_dir=out_dir,\n",
    "                query_fields=query_fields,\n",
    "            )\n",
    "        \n",
    "        # 연도별 기사 수 집계\n",
    "        count_2017, count_2018, count_2019, total_count = 0, 0, 0, 0\n",
    "        if jsonl_path.exists():  # csv_path → jsonl_path로 변경\n",
    "            try:\n",
    "                # JSONL 파일에서 직접 읽기\n",
    "                rows = []\n",
    "                with jsonl_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "                    for line in f:\n",
    "                        rows.append(json.loads(line))\n",
    "                \n",
    "                df = pd.DataFrame(rows)\n",
    "                total_count = len(df)\n",
    "                df['year'] = pd.to_datetime(df['webPublicationDate']).dt.year\n",
    "                count_2017 = len(df[df['year'] == 2017])\n",
    "                count_2018 = len(df[df['year'] == 2018])\n",
    "                count_2019 = len(df[df['year'] == 2019])\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Could not read {jsonl_path.name}: {e}\")\n",
    "        \n",
    "        summary_data.append({\n",
    "            \"person\": p,\n",
    "            \"slug\": slug,\n",
    "            \"2017\": count_2017,\n",
    "            \"2018\": count_2018,\n",
    "            \"2019\": count_2019,\n",
    "            \"total\": total_count,\n",
    "        })\n",
    "\n",
    "    # Summary CSV 저장\n",
    "    df_summary = pd.DataFrame(summary_data)\n",
    "    summary_path = \"guardian_raw_scraping_summary.csv\"\n",
    "    df_summary.to_csv(summary_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"\\n\\nSaved summary: {summary_path}\")\n",
    "    print(f\"Total people crawled: {len(people)}\")\n",
    "    print(f\"Total articles: {df_summary['total'].sum()}\")\n",
    "    print(f\"  - 2017: {df_summary['2017'].sum()}\")\n",
    "    print(f\"  - 2018: {df_summary['2018'].sum()}\")\n",
    "    print(f\"  - 2019: {df_summary['2019'].sum()}\")\n",
    "\n",
    "    return {\"summary\": summary_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec79904",
   "metadata": {},
   "source": [
    "## Cell 4: Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b6e14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# people_list.txt 파일 읽기\n",
    "with open('people_list.txt', 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "people_list = content.split('\\n')\n",
    "people_list = [name.strip() for name in people_list if name.strip()]\n",
    "\n",
    "# 중복 체크\n",
    "duplicates = [name for name in set(people_list) if people_list.count(name) > 1]\n",
    "people = list(set(people_list))\n",
    "\n",
    "print(f\"Total: {len(people_list)} names\")\n",
    "print(f\"Unique: {len(people)} people\")\n",
    "print(f\"Duplicates: {len(people_list) - len(people)}\")\n",
    "if duplicates:\n",
    "    print(f\"Duplicate names: {duplicates}\")\n",
    "print(f\"\\nCrawling list: {people}\")\n",
    "print()\n",
    "\n",
    "#people = [\"Samantha Bee\",\"Constance Wu\"]\n",
    "# 크롤링 실행\n",
    "paths = batch_crawl_and_summary(\n",
    "    people=people,\n",
    "    start_date=\"2017-01-01\",\n",
    "    end_date=\"2019-12-31\",\n",
    "    out_dir=\"../guardian_raw_scraping\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58518828",
   "metadata": {},
   "source": [
    "## 5: Extract Top 100 & Copy Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e145a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# guardian_raw_scraping_summary.csv 읽기\n",
    "df = pd.read_csv('guardian_raw_scraping_summary.csv')\n",
    "\n",
    "# total 기준 내림차순 정렬 후 top 100 추출\n",
    "df_sorted = df.sort_values('total', ascending=False).head(100)\n",
    "top100_people = df_sorted['person'].tolist()\n",
    "\n",
    "print(f\"Top 100 people by total articles: {len(top100_people)}\")\n",
    "print(f\"Total articles: {df_sorted['total'].sum()}\")\n",
    "print(f\"\\nTop 10:\")\n",
    "print(df_sorted[['person', 'total']].head(10).to_string(index=False))\n",
    "\n",
    "# people_top100_list.txt 저장\n",
    "with open('people_top100_list.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(top100_people))\n",
    "print(f\"\\nSaved: people_top100_list.txt\")\n",
    "\n",
    "# ../guardian_top100_scraping 폴더 생성\n",
    "new_dir = '../guardian_top100_scraping'\n",
    "os.makedirs(new_dir, exist_ok=True)\n",
    "\n",
    "# jsonl 파일 복사\n",
    "copied_count = 0\n",
    "for person in top100_people:\n",
    "    slug = df_sorted[df_sorted['person'] == person]['slug'].values[0]\n",
    "    src_file = f'../guardian_raw_scraping/{slug}.jsonl'\n",
    "    dst_file = f'{new_dir}/{slug}.jsonl'\n",
    "    \n",
    "    if os.path.exists(src_file):\n",
    "        shutil.copy2(src_file, dst_file)\n",
    "        copied_count += 1\n",
    "\n",
    "print(f\"\\nCopied {copied_count} jsonl files to {new_dir}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
