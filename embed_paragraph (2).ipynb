{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AU9bDiifoTjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**all-MiniLM-L6-v2**"
      ],
      "metadata": {
        "id": "raBXLqJg1nwf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "US9cCy6Aknjg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "\n",
        "# 로깅 설정\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# 0. GPU 설정\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logging.info(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# === 모델 및 설정 변경 ===\n",
        "# Output Dimension: 384\n",
        "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "# 데이터 경로\n",
        "DATA_DIR = Path(\"/content/drive/MyDrive/COSE362/data/guardian_top100_scraping\")\n",
        "\n",
        "# 출력 경로\n",
        "OUTPUT_DIR = Path(\"/content/drive/MyDrive/COSE362/data/vector_paragraph_1\")\n",
        "BATCH_SIZE = 32\n",
        "CHECKPOINT_FILE = OUTPUT_DIR / \"checkpoint.json\"\n",
        "\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# 3. Model과 Tokenizer 로드\n",
        "logging.info(f\"Loading model: {MODEL_NAME} ...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModel.from_pretrained(MODEL_NAME)\n",
        "model = model.to(DEVICE)\n",
        "model.eval()\n",
        "logging.info(\"Model loaded successfully!\")\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def extract_person_name(filename):\n",
        "    \"\"\"파일명에서 person 이름 추출 (예: alex_morgan.jsonl -> alex_morgan)\"\"\"\n",
        "    return filename.stem\n",
        "\n",
        "def parse_pub_date(web_pub_date):\n",
        "    \"\"\"webPublicationDate를 YYYY_MM_DD 형식으로 변환\"\"\"\n",
        "    try:\n",
        "        dt = datetime.fromisoformat(web_pub_date.replace('Z', '+00:00'))\n",
        "        return dt.strftime(\"%Y_%m_%d\")\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def preprocess_text_first_last(text):\n",
        "    \"\"\"\n",
        "    기사의 첫 문단과 마지막 문단만 추출하여 결합\n",
        "    \"\"\"\n",
        "    if not text or text.strip() == '':\n",
        "        return None\n",
        "\n",
        "    # 줄바꿈을 기준으로 문단 분리 (공백 라인 제거)\n",
        "    paragraphs = [line.strip() for line in text.split('\\n') if line.strip()]\n",
        "\n",
        "    if not paragraphs:\n",
        "        return None\n",
        "\n",
        "    if len(paragraphs) == 1:\n",
        "        # 문단이 하나뿐이면 그것만 사용\n",
        "        return paragraphs[0]\n",
        "    else:\n",
        "        # 첫 문단 + 공백 + 마지막 문단\n",
        "        return f\"{paragraphs[0]} {paragraphs[-1]}\"\n",
        "\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    \"\"\"\n",
        "    sentence-transformers 모델을 위한 Mean Pooling 함수\n",
        "    (모든 토큰의 임베딩을 평균내어 문장 임베딩 생성)\n",
        "    \"\"\"\n",
        "    token_embeddings = model_output.last_hidden_state\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_embeddings(texts, model, tokenizer, batch_size=32):\n",
        "    \"\"\"Batch 단위로 embedding 생성 (Mean Pooling 적용)\"\"\"\n",
        "    all_embeddings = []\n",
        "\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "\n",
        "        # Tokenize\n",
        "        encoded = tokenizer(batch, padding=True, truncation=True,\n",
        "                            max_length=512, return_tensors='pt')\n",
        "        encoded = {k: v.to(DEVICE) for k, v in encoded.items()}\n",
        "\n",
        "        # Model Inference\n",
        "        outputs = model(**encoded)\n",
        "\n",
        "        # CLS 토큰 대신 Mean Pooling 사용\n",
        "        batch_embeddings = mean_pooling(outputs, encoded['attention_mask'])\n",
        "\n",
        "        # Normalize\n",
        "        batch_embeddings = torch.nn.functional.normalize(batch_embeddings, p=2, dim=1)\n",
        "\n",
        "        all_embeddings.append(batch_embeddings.cpu().numpy())\n",
        "\n",
        "    if not all_embeddings:\n",
        "        return np.array([])\n",
        "\n",
        "    return np.vstack(all_embeddings)\n",
        "\n",
        "# --- Main Execution Logic (Checkpoint & Loop) ---\n",
        "\n",
        "# Checkpoint 확인\n",
        "processed_files = set()\n",
        "if CHECKPOINT_FILE.exists():\n",
        "    try:\n",
        "        with open(CHECKPOINT_FILE, 'r') as f:\n",
        "            checkpoint = json.load(f)\n",
        "            processed_files = set(checkpoint.get('processed_files', []))\n",
        "            logging.info(f\"Checkpoint found: {len(processed_files)} files already processed\")\n",
        "    except json.JSONDecodeError:\n",
        "        logging.warning(\"Checkpoint file is corrupted. Starting fresh.\")\n",
        "        processed_files = set()\n",
        "\n",
        "# 기존 저장된 데이터 로드 (Append 모드 지원)\n",
        "embeddings_file = OUTPUT_DIR / \"embeddings.npy\"\n",
        "metadata_file = OUTPUT_DIR / \"metadata.jsonl\"\n",
        "\n",
        "existing_embeddings = None\n",
        "existing_metadata = []\n",
        "\n",
        "if embeddings_file.exists() and metadata_file.exists():\n",
        "    logging.info(\"Loading existing data...\")\n",
        "    try:\n",
        "        existing_embeddings = np.load(embeddings_file)\n",
        "        with open(metadata_file, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                existing_metadata.append(json.loads(line))\n",
        "\n",
        "        if len(existing_embeddings) == len(existing_metadata):\n",
        "            logging.info(f\"Loaded {len(existing_metadata)} existing entries\")\n",
        "        else:\n",
        "            logging.error(\"Data mismatch! Starting fresh.\")\n",
        "            existing_embeddings = None\n",
        "            existing_metadata = []\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading existing data: {e}. Starting fresh.\")\n",
        "        existing_embeddings = None\n",
        "        existing_metadata = []\n",
        "else:\n",
        "    logging.info(\"Starting fresh (no existing data found)\")\n",
        "\n",
        "# 처리할 파일 목록 생성\n",
        "jsonl_files = sorted([f for f in DATA_DIR.glob(\"*.jsonl\") if f.name not in processed_files])\n",
        "logging.info(f\"Total files to process: {len(jsonl_files)}\")\n",
        "\n",
        "logging.info(\"전체 데이터 임베딩 작업을 시작합니다.\")\n",
        "\n",
        "# 메인 루프\n",
        "for idx, file_path in enumerate(tqdm(jsonl_files, desc=\"Overall Progress\")):\n",
        "    person = extract_person_name(file_path)\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            articles = [json.loads(line) for line in f]\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to read {file_path}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # 배치를 위한 데이터 수집\n",
        "    valid_texts = []\n",
        "    valid_metadata = []\n",
        "\n",
        "    for article in articles:\n",
        "        body_text = article.get('bodyText', '')\n",
        "        article_id = article.get('id')\n",
        "        pub_date_raw = article.get('webPublicationDate')\n",
        "\n",
        "        if not all([body_text, article_id, pub_date_raw]):\n",
        "            continue\n",
        "\n",
        "        # 첫 문단 + 마지막 문단 추출\n",
        "        processed_text = preprocess_text_first_last(body_text)\n",
        "\n",
        "        if processed_text:\n",
        "            valid_texts.append(processed_text)\n",
        "            valid_metadata.append({\n",
        "                'person': person,\n",
        "                'article_id': article_id,\n",
        "                'pub_date': parse_pub_date(pub_date_raw)\n",
        "            })\n",
        "\n",
        "    # 임베딩 생성 및 저장\n",
        "    if valid_texts:\n",
        "        # 한 파일 내의 모든 기사를 배치 처리\n",
        "        new_embeddings = generate_embeddings(valid_texts, model, tokenizer, batch_size=BATCH_SIZE)\n",
        "\n",
        "        if new_embeddings.size > 0:\n",
        "            # 기존 데이터와 병합\n",
        "            if existing_embeddings is not None:\n",
        "                combined_embeddings = np.vstack([existing_embeddings, new_embeddings])\n",
        "            else:\n",
        "                combined_embeddings = new_embeddings\n",
        "\n",
        "            combined_metadata = existing_metadata + valid_metadata\n",
        "\n",
        "            # 즉시 저장 (덮어쓰기 방식)\n",
        "            try:\n",
        "                np.save(embeddings_file, combined_embeddings)\n",
        "                with open(metadata_file, 'w', encoding='utf-8') as f:\n",
        "                    for meta in combined_metadata:\n",
        "                        f.write(json.dumps(meta, ensure_ascii=False) + '\\n')\n",
        "\n",
        "                # 메모리 갱신\n",
        "                existing_embeddings = combined_embeddings\n",
        "                existing_metadata = combined_metadata\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Failed to save data for {person}: {e}\")\n",
        "                continue\n",
        "\n",
        "    # Checkpoint 업데이트\n",
        "    processed_files.add(file_path.name)\n",
        "    with open(CHECKPOINT_FILE, 'w') as f:\n",
        "        json.dump({'processed_files': list(processed_files)}, f)\n",
        "\n",
        "logging.info(f\"\\n All processing complete!\")\n",
        "if existing_embeddings is not None:\n",
        "    logging.info(f\"Final embeddings shape: {existing_embeddings.shape}\")\n",
        "    logging.info(f\"Total articles: {len(existing_metadata)}\")\n",
        "\n",
        "if CHECKPOINT_FILE.exists():\n",
        "    CHECKPOINT_FILE.unlink()\n",
        "    logging.info(\"Checkpoint file removed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**jina-embeddings-v3**"
      ],
      "metadata": {
        "id": "RtTEUjKD1Zig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from transformers import AutoModel\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "\n",
        "# 로깅 설정\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# 0. GPU 설정\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logging.info(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# === 모델 및 설정 변경 ===\n",
        "# Jina Embeddings v3 (8192 context length, 1024 dimension)\n",
        "MODEL_NAME = \"jinaai/jina-embeddings-v3\"\n",
        "\n",
        "# 데이터 경로\n",
        "DATA_DIR = Path(\"/content/drive/MyDrive/COSE362/data/guardian_top100_scraping\")\n",
        "\n",
        "# 출력 경로\n",
        "OUTPUT_DIR = Path(\"/content/drive/MyDrive/COSE362/data/vector_paragraph_2\")\n",
        "CHECKPOINT_FILE = OUTPUT_DIR / \"checkpoint.json\"\n",
        "\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# 3. Model 로드\n",
        "# Jina v3는 trust_remote_code=True가 필수\n",
        "logging.info(f\"Loading model: {MODEL_NAME} ...\")\n",
        "# Tokenizer는 model.encode 내부에서 처리되므로 별도로 불러오지 않아도 됨\n",
        "model = AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "model = model.to(DEVICE)\n",
        "model.eval()\n",
        "logging.info(\"Model loaded successfully!\")\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def extract_person_name(filename):\n",
        "    \"\"\"파일명에서 person 이름 추출\"\"\"\n",
        "    return filename.stem\n",
        "\n",
        "def parse_pub_date(web_pub_date):\n",
        "    \"\"\"webPublicationDate를 YYYY_MM_DD 형식으로 변환\"\"\"\n",
        "    try:\n",
        "        dt = datetime.fromisoformat(web_pub_date.replace('Z', '+00:00'))\n",
        "        return dt.strftime(\"%Y_%m_%d\")\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def preprocess_text_first_last(text):\n",
        "    \"\"\"\n",
        "    기사의 첫 문단과 마지막 문단만 추출하여 결합\n",
        "    \"\"\"\n",
        "    if not text or text.strip() == '':\n",
        "        return None\n",
        "\n",
        "    # 줄바꿈을 기준으로 문단 분리 (공백 라인 제거)\n",
        "    paragraphs = [line.strip() for line in text.split('\\n') if line.strip()]\n",
        "\n",
        "    if not paragraphs:\n",
        "        return None\n",
        "\n",
        "    if len(paragraphs) == 1:\n",
        "        # 문단이 하나뿐이면 그것만 사용\n",
        "        return paragraphs[0]\n",
        "    else:\n",
        "        # 첫 문단 + 공백 + 마지막 문단\n",
        "        return f\"{paragraphs[0]} {paragraphs[-1]}\"\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_embeddings_jina(texts, model, batch_size=32):\n",
        "    \"\"\"\n",
        "    Jina v3 전용 임베딩 생성 함수\n",
        "    - task=\"retrieval.passage\": 문서(Passage) 임베딩 모드 사용\n",
        "    - model.encode()가 배치 처리 및 토크나이징을 자동으로 수행함\n",
        "    \"\"\"\n",
        "    # Jina 모델의 내장 encode 함수 사용 (NumPy array 반환)\n",
        "    # max_length는 모델의 최대 길이(8192)를 따름\n",
        "    embeddings = model.encode(\n",
        "        texts,\n",
        "        task=\"retrieval.passage\",\n",
        "        batch_size=batch_size,\n",
        "        show_progress_bar=False\n",
        "    )\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "# --- Main Execution Logic (Checkpoint & Loop) ---\n",
        "\n",
        "# Checkpoint 확인\n",
        "processed_files = set()\n",
        "if CHECKPOINT_FILE.exists():\n",
        "    try:\n",
        "        with open(CHECKPOINT_FILE, 'r') as f:\n",
        "            checkpoint = json.load(f)\n",
        "            processed_files = set(checkpoint.get('processed_files', []))\n",
        "            logging.info(f\"Checkpoint found: {len(processed_files)} files already processed\")\n",
        "    except json.JSONDecodeError:\n",
        "        logging.warning(\"Checkpoint file is corrupted. Starting fresh.\")\n",
        "        processed_files = set()\n",
        "\n",
        "# 기존 저장된 데이터 로드 (Append 모드 지원)\n",
        "embeddings_file = OUTPUT_DIR / \"embeddings.npy\"\n",
        "metadata_file = OUTPUT_DIR / \"metadata.jsonl\"\n",
        "\n",
        "existing_embeddings = None\n",
        "existing_metadata = []\n",
        "\n",
        "if embeddings_file.exists() and metadata_file.exists():\n",
        "    logging.info(\"Loading existing data...\")\n",
        "    try:\n",
        "        existing_embeddings = np.load(embeddings_file)\n",
        "        with open(metadata_file, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                existing_metadata.append(json.loads(line))\n",
        "\n",
        "        if len(existing_embeddings) == len(existing_metadata):\n",
        "            logging.info(f\"Loaded {len(existing_metadata)} existing entries\")\n",
        "        else:\n",
        "            logging.error(\"Data mismatch! Starting fresh.\")\n",
        "            existing_embeddings = None\n",
        "            existing_metadata = []\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading existing data: {e}. Starting fresh.\")\n",
        "        existing_embeddings = None\n",
        "        existing_metadata = []\n",
        "else:\n",
        "    logging.info(\"Starting fresh (no existing data found)\")\n",
        "\n",
        "# 처리할 파일 목록 생성\n",
        "jsonl_files = sorted([f for f in DATA_DIR.glob(\"*.jsonl\") if f.name not in processed_files])\n",
        "logging.info(f\"Total files to process: {len(jsonl_files)}\")\n",
        "\n",
        "logging.info(\"전체 데이터 임베딩 작업을 시작합니다 (Jina v3).\")\n",
        "\n",
        "# 메인 루프\n",
        "# Jina v3는 8192 토큰까지 처리 가능하므로 배치 사이즈를 너무 크게 잡으면 OOM 발생 가능 (조절 필요 시 batch_size 축소)\n",
        "BATCH_SIZE_JINA = 16\n",
        "\n",
        "for idx, file_path in enumerate(tqdm(jsonl_files, desc=\"Overall Progress\")):\n",
        "    person = extract_person_name(file_path)\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            articles = [json.loads(line) for line in f]\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to read {file_path}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # 배치를 위한 데이터 수집\n",
        "    valid_texts = []\n",
        "    valid_metadata = []\n",
        "\n",
        "    for article in articles:\n",
        "        body_text = article.get('bodyText', '')\n",
        "        article_id = article.get('id')\n",
        "        pub_date_raw = article.get('webPublicationDate')\n",
        "\n",
        "        if not all([body_text, article_id, pub_date_raw]):\n",
        "            continue\n",
        "\n",
        "        # 첫 문단 + 마지막 문단 추출\n",
        "        processed_text = preprocess_text_first_last(body_text)\n",
        "\n",
        "        if processed_text:\n",
        "            valid_texts.append(processed_text)\n",
        "            valid_metadata.append({\n",
        "                'person': person,\n",
        "                'article_id': article_id,\n",
        "                'pub_date': parse_pub_date(pub_date_raw)\n",
        "            })\n",
        "\n",
        "    # 임베딩 생성 및 저장\n",
        "    if valid_texts:\n",
        "        try:\n",
        "            # Jina 모델로 임베딩 생성\n",
        "            new_embeddings = generate_embeddings_jina(valid_texts, model, batch_size=BATCH_SIZE_JINA)\n",
        "\n",
        "            if new_embeddings.size > 0:\n",
        "                # 기존 데이터와 병합\n",
        "                if existing_embeddings is not None:\n",
        "                    combined_embeddings = np.vstack([existing_embeddings, new_embeddings])\n",
        "                else:\n",
        "                    combined_embeddings = new_embeddings\n",
        "\n",
        "                combined_metadata = existing_metadata + valid_metadata\n",
        "\n",
        "                # 즉시 저장 (덮어쓰기 방식)\n",
        "                np.save(embeddings_file, combined_embeddings)\n",
        "                with open(metadata_file, 'w', encoding='utf-8') as f:\n",
        "                    for meta in combined_metadata:\n",
        "                        f.write(json.dumps(meta, ensure_ascii=False) + '\\n')\n",
        "\n",
        "                # 메모리 갱신\n",
        "                existing_embeddings = combined_embeddings\n",
        "                existing_metadata = combined_metadata\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to save data for {person}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Checkpoint 업데이트\n",
        "    processed_files.add(file_path.name)\n",
        "    with open(CHECKPOINT_FILE, 'w') as f:\n",
        "        json.dump({'processed_files': list(processed_files)}, f)\n",
        "\n",
        "logging.info(f\"\\n All processing complete!\")\n",
        "if existing_embeddings is not None:\n",
        "    logging.info(f\"Final embeddings shape: {existing_embeddings.shape}\")\n",
        "    logging.info(f\"Total articles: {len(existing_metadata)}\")\n",
        "\n",
        "if CHECKPOINT_FILE.exists():\n",
        "    CHECKPOINT_FILE.unlink()\n",
        "    logging.info(\"Checkpoint file removed.\")"
      ],
      "metadata": {
        "id": "Qzkm0hW2kvY7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}