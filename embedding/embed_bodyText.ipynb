{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 import\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Processing people 0 to 49\n"
     ]
    }
   ],
   "source": [
    "# ===== GPU 분할 설정 =====\n",
    "GPU_ID = 0\n",
    "START_IDX = 0\n",
    "END_IDX = 100\n",
    "# ======================================================\n",
    "\n",
    "# 설정\n",
    "MODEL_NAME = \"jinaai/jina-embeddings-v3\"\n",
    "DATA_DIR = Path(\"../guardian_top100_scraping\")\n",
    "OUTPUT_DIR = Path(f\"vector_bodyText_gpu{GPU_ID}\")\n",
    "BATCH_SIZE = 16\n",
    "CHECKPOINT_FILE = OUTPUT_DIR / \"checkpoint.json\"\n",
    "\n",
    "# Jina v3 토큰 설정\n",
    "MAX_TOKENS = 8192\n",
    "CHUNK_TOKENS = 8000\n",
    "OVERLAP_TOKENS = 100\n",
    "\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "print(f\"GPU {GPU_ID}: Processing people {START_IDX} to {END_IDX-1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Model과 Tokenizer load\n",
    "print(\"Loading model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_person_name(filename):\n",
    "    \"\"\"파일명에서 person 이름 추출\"\"\"\n",
    "    return filename.stem\n",
    "\n",
    "def parse_pub_date(web_pub_date):\n",
    "    \"\"\"webPublicationDate를 YYYY_MM_DD 형식으로 변환\"\"\"\n",
    "    dt = datetime.fromisoformat(web_pub_date.replace('Z', '+00:00'))\n",
    "    return dt.strftime(\"%Y_%m_%d\")\n",
    "\n",
    "def chunk_text_by_tokens(text, tokenizer, max_tokens=8000, overlap_tokens=100):\n",
    "    \"\"\"텍스트를 토큰 기준으로 청킹 (overlap 포함)\"\"\"\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    \n",
    "    if len(tokens) <= max_tokens:\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    step = max_tokens - overlap_tokens\n",
    "    \n",
    "    for i in range(0, len(tokens), step):\n",
    "        chunk_tokens = tokens[i:i + max_tokens]\n",
    "        chunk_text = tokenizer.decode(chunk_tokens, skip_special_tokens=True)\n",
    "        chunks.append(chunk_text)\n",
    "        \n",
    "        if i + max_tokens >= len(tokens):\n",
    "            break\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_embeddings(texts, batch_size=16):\n",
    "    \"\"\"Batch 단위로 embedding 생성 (Jina v3 사용)\"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        \n",
    "        # 각 텍스트를 개별적으로 처리 (truncation 보장)\n",
    "        batch_embeddings = []\n",
    "        for text in batch:\n",
    "            # Tokenize with strict truncation\n",
    "            tokens = tokenizer.encode(\n",
    "                text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=MAX_TOKENS,\n",
    "                truncation=True\n",
    "            )\n",
    "            \n",
    "            # Decode back to ensure it's within limit\n",
    "            truncated_text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "            \n",
    "            # Get embedding using model's encode method\n",
    "            emb = model.encode(\n",
    "                [truncated_text],\n",
    "                task='text-matching',\n",
    "                show_progress_bar=False,\n",
    "                convert_to_numpy=True,\n",
    "                normalize_embeddings=True\n",
    "            )\n",
    "            batch_embeddings.append(emb[0])\n",
    "        \n",
    "        all_embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    return np.array(all_embeddings)\n",
    "\n",
    "def mean_pool_embeddings(embeddings):\n",
    "    \"\"\"여러 chunk embeddings를 mean pooling\"\"\"\n",
    "    return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint found: 9 files already processed\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint 확인\n",
    "processed_files = set()\n",
    "if CHECKPOINT_FILE.exists():\n",
    "    with open(CHECKPOINT_FILE, 'r') as f:\n",
    "        checkpoint = json.load(f)\n",
    "        processed_files = set(checkpoint.get('processed_files', []))\n",
    "        print(f\"Checkpoint found: {len(processed_files)} files already processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing data...\n",
      "Loaded 34288 existing entries\n"
     ]
    }
   ],
   "source": [
    "# 기존 저장된 데이터 로드\n",
    "embeddings_file = OUTPUT_DIR / \"embeddings.npy\"\n",
    "metadata_file = OUTPUT_DIR / \"metadata.jsonl\"\n",
    "\n",
    "if embeddings_file.exists() and metadata_file.exists():\n",
    "    print(\"Loading existing data...\")\n",
    "    existing_embeddings = np.load(embeddings_file)\n",
    "    existing_metadata = []\n",
    "    with open(metadata_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            existing_metadata.append(json.loads(line))\n",
    "    print(f\"Loaded {len(existing_metadata)} existing entries\")\n",
    "else:\n",
    "    existing_embeddings = None\n",
    "    existing_metadata = []\n",
    "    print(\"Starting fresh (no existing data found)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files in this GPU: 41\n"
     ]
    }
   ],
   "source": [
    "# 모든 .jsonl 파일 수집 및 GPU별 분할\n",
    "all_files = sorted([f for f in DATA_DIR.glob(\"*.jsonl\")])\n",
    "jsonl_files = all_files[START_IDX:END_IDX]\n",
    "jsonl_files = [f for f in jsonl_files if f.name not in processed_files]\n",
    "\n",
    "print(f\"Total files in this GPU: {len(jsonl_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 수집 및 embedding 생성 (인물별로 처리)\n",
    "for idx, file_path in enumerate(jsonl_files):\n",
    "    person = extract_person_name(file_path)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"\\n[{START_IDX + idx + 1}/{len(jsonl_files)}] Processing: {person}\")\n",
    "    \n",
    "    # 파일에서 기사 읽기\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        articles = [json.loads(line) for line in f]\n",
    "    \n",
    "    print(f\"  Total articles: {len(articles)}\")\n",
    "    \n",
    "    # 현재 인물의 기사별 처리\n",
    "    person_embeddings = []\n",
    "    person_metadata = []\n",
    "    \n",
    "    # Progress bar로 기사 처리\n",
    "    for article in tqdm(articles, desc=f\"  Processing articles\", leave=False):\n",
    "        body_text = article.get('bodyText', '')\n",
    "        if not body_text:\n",
    "            continue\n",
    "        \n",
    "        # 토큰 기준으로 청킹\n",
    "        chunks = chunk_text_by_tokens(\n",
    "            body_text, \n",
    "            tokenizer, \n",
    "            max_tokens=CHUNK_TOKENS, \n",
    "            overlap_tokens=OVERLAP_TOKENS\n",
    "        )\n",
    "        \n",
    "        if not chunks:\n",
    "            continue\n",
    "        \n",
    "        # 각 chunk의 embedding 생성\n",
    "        chunk_embeddings = generate_embeddings(chunks, batch_size=BATCH_SIZE)\n",
    "        \n",
    "        # Mean pooling\n",
    "        article_embedding = mean_pool_embeddings(chunk_embeddings)\n",
    "        \n",
    "        person_embeddings.append(article_embedding)\n",
    "        person_metadata.append({\n",
    "            'person': person,\n",
    "            'article_id': article.get('id', ''),\n",
    "            'pub_date': parse_pub_date(article.get('webPublicationDate', ''))\n",
    "        })\n",
    "    \n",
    "    # 현재 인물의 결과 저장\n",
    "    if person_embeddings:\n",
    "        person_embeddings_array = np.array(person_embeddings)\n",
    "        \n",
    "        # 기존 데이터와 합치기\n",
    "        if existing_embeddings is not None:\n",
    "            combined_embeddings = np.vstack([existing_embeddings, person_embeddings_array])\n",
    "        else:\n",
    "            combined_embeddings = person_embeddings_array\n",
    "        \n",
    "        combined_metadata = existing_metadata + person_metadata\n",
    "        \n",
    "        # 즉시 저장\n",
    "        np.save(embeddings_file, combined_embeddings)\n",
    "        with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "            for meta in combined_metadata:\n",
    "                f.write(json.dumps(meta, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        # 다음 반복을 위해 업데이트\n",
    "        existing_embeddings = combined_embeddings\n",
    "        existing_metadata = combined_metadata\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"  ✓ Done! Processed {len(person_metadata)} articles (Total: {len(combined_metadata)} articles)\")\n",
    "        print(f\"  ⏱ Time taken: {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    # Checkpoint 업데이트\n",
    "    processed_files.add(file_path.name)\n",
    "    with open(CHECKPOINT_FILE, 'w') as f:\n",
    "        json.dump({'processed_files': list(processed_files)}, f)\n",
    "\n",
    "print(f\"\\nAll processing complete for GPU {GPU_ID}!\")\n",
    "print(f\"Final embeddings shape: {existing_embeddings.shape}\")\n",
    "print(f\"Total articles: {len(existing_metadata)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모든 GPU 결과 합치기\n",
    "**4개 GPU 모두 완료된 후에 실행**\n",
    "- 일단은 1개 GPU만 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging results from all GPUs...\n",
      "\n",
      "✓ GPU 0: Loaded 225145 embeddings\n",
      "✓ GPU 1: Loaded 235577 embeddings\n",
      "\n",
      "==================================================\n",
      "✅ Merge complete!\n",
      "Final embeddings shape: (460722, 1024)\n",
      "Total articles: 460722\n",
      "Saved to: vector_bodyText/\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 모든 GPU 결과 합치기\n",
    "print(\"Merging results from all GPUs...\\n\")\n",
    "\n",
    "all_embeddings = []\n",
    "all_metadata = []\n",
    "\n",
    "for gpu_id in range(1):\n",
    "    gpu_dir = Path(f\"vector_bodyText_gpu{gpu_id}\")\n",
    "    emb_file = gpu_dir / \"embeddings.npy\"\n",
    "    meta_file = gpu_dir / \"metadata.jsonl\"\n",
    "    \n",
    "    if not emb_file.exists() or not meta_file.exists():\n",
    "        print(f\"⚠️  GPU {gpu_id}: Files not found, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Load embeddings\n",
    "    emb = np.load(emb_file)\n",
    "    all_embeddings.append(emb)\n",
    "    print(f\"✓ GPU {gpu_id}: Loaded {emb.shape[0]} embeddings\")\n",
    "    \n",
    "    # Load metadata\n",
    "    with open(meta_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            all_metadata.append(json.loads(line))\n",
    "\n",
    "# 합치기\n",
    "if all_embeddings:\n",
    "    final_embeddings = np.vstack(all_embeddings)\n",
    "    \n",
    "    # 최종 결과 저장\n",
    "    final_dir = Path(\"vector_bodyText\")\n",
    "    final_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    np.save(final_dir / \"embeddings.npy\", final_embeddings)\n",
    "    with open(final_dir / \"metadata.jsonl\", 'w', encoding='utf-8') as f:\n",
    "        for meta in all_metadata:\n",
    "            f.write(json.dumps(meta, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"✅ Merge complete!\")\n",
    "    print(f\"Final embeddings shape: {final_embeddings.shape}\")\n",
    "    print(f\"Total articles: {len(all_metadata)}\")\n",
    "    print(f\"Saved to: {final_dir}/\")\n",
    "    print(f\"{'='*50}\")\n",
    "else:\n",
    "    print(\"\\n❌ No data found to merge!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
