{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wonjun/.conda/envs/mlproject/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 import\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설정\n",
    "MODEL_NAME = \"BAAI/bge-large-en-v1.5\"\n",
    "DATA_DIR = Path(\"../guardian_top100_scraping\")\n",
    "OUTPUT_DIR = Path(\"vector_chunking\")\n",
    "BATCH_SIZE = 64\n",
    "CHECKPOINT_FILE = OUTPUT_DIR / \"checkpoint.json\"\n",
    "\n",
    "# Chunking 설정\n",
    "CHUNK_SIZE = 10  # 열 문장씩\n",
    "OVERLAP = 3      # 세 문장씩 overlap\n",
    "\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Model과 Tokenizer load\n",
    "print(\"Loading model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_person_name(filename):\n",
    "    \"\"\"파일명에서 person 이름 추출\"\"\"\n",
    "    return filename.stem\n",
    "\n",
    "def parse_pub_date(web_pub_date):\n",
    "    \"\"\"webPublicationDate를 YYYY_MM_DD 형식으로 변환\"\"\"\n",
    "    dt = datetime.fromisoformat(web_pub_date.replace('Z', '+00:00'))\n",
    "    return dt.strftime(\"%Y_%m_%d\")\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    \"\"\"텍스트를 문장 단위로 분리 (. 기준)\"\"\"\n",
    "    sentences = [s.strip() for s in text.split('. ') if s.strip()]\n",
    "    return sentences\n",
    "\n",
    "def create_chunks(sentences, chunk_size=2, overlap=1):\n",
    "    \"\"\"문장들을 chunk_size만큼 묶고, overlap만큼 겹치게 생성\"\"\"\n",
    "    chunks = []\n",
    "    step = chunk_size - overlap\n",
    "    \n",
    "    for i in range(0, len(sentences), step):\n",
    "        chunk_sentences = sentences[i:i+chunk_size]\n",
    "        if chunk_sentences:\n",
    "            chunk_text = '. '.join(chunk_sentences) + '.'\n",
    "            chunks.append(chunk_text)\n",
    "        \n",
    "        if i + chunk_size >= len(sentences):\n",
    "            break\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_embeddings(texts, batch_size=32):\n",
    "    \"\"\"Batch 단위로 embedding 생성\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoded = tokenizer(batch, padding=True, truncation=True, \n",
    "                          max_length=512, return_tensors='pt')\n",
    "        encoded = {k: v.cuda() for k, v in encoded.items()}\n",
    "        \n",
    "        # Generate embeddings\n",
    "        outputs = model(**encoded)\n",
    "        # CLS token embedding 사용\n",
    "        batch_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Normalize\n",
    "        batch_embeddings = torch.nn.functional.normalize(batch_embeddings, p=2, dim=1)\n",
    "        \n",
    "        embeddings.append(batch_embeddings.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "def mean_pool_embeddings(embeddings):\n",
    "    \"\"\"여러 chunk embeddings를 mean pooling\"\"\"\n",
    "    return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint found: 84 files already processed\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint 확인\n",
    "processed_files = set()\n",
    "if CHECKPOINT_FILE.exists():\n",
    "    with open(CHECKPOINT_FILE, 'r') as f:\n",
    "        checkpoint = json.load(f)\n",
    "        processed_files = set(checkpoint.get('processed_files', []))\n",
    "        print(f\"Checkpoint found: {len(processed_files)} files already processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files to process: 16\n"
     ]
    }
   ],
   "source": [
    "# 모든 .jsonl 파일 수집\n",
    "jsonl_files = sorted([f for f in DATA_DIR.glob(\"*.jsonl\") if f.name not in processed_files])\n",
    "print(f\"Total files to process: {len(jsonl_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing data...\n",
      "Loaded 363740 existing entries\n"
     ]
    }
   ],
   "source": [
    "# 기존 저장된 데이터 로드 (있다면)\n",
    "embeddings_file = OUTPUT_DIR / \"embeddings.npy\"\n",
    "metadata_file = OUTPUT_DIR / \"metadata.jsonl\"\n",
    "\n",
    "if embeddings_file.exists() and metadata_file.exists():\n",
    "    print(\"Loading existing data...\")\n",
    "    existing_embeddings = np.load(embeddings_file)\n",
    "    existing_metadata = []\n",
    "    with open(metadata_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            existing_metadata.append(json.loads(line))\n",
    "    print(f\"Loaded {len(existing_metadata)} existing entries\")\n",
    "else:\n",
    "    existing_embeddings = None\n",
    "    existing_metadata = []\n",
    "    print(\"Starting fresh (no existing data found)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/16] Processing: ryan_coogler\n",
      "  ✓ Done! Processed 7441 articles (Total: 371181 articles)\n",
      "\n",
      "[2/16] Processing: ryan_murphy\n",
      "  ✓ Done! Processed 10452 articles (Total: 381633 articles)\n",
      "\n",
      "[3/16] Processing: ryan_reynolds\n",
      "  ✓ Done! Processed 8466 articles (Total: 390099 articles)\n",
      "\n",
      "[4/16] Processing: sadiq_khan\n",
      "  ✓ Done! Processed 4420 articles (Total: 394519 articles)\n",
      "\n",
      "[5/16] Processing: samantha_bee\n",
      "  ✓ Done! Processed 2264 articles (Total: 396783 articles)\n",
      "\n",
      "[6/16] Processing: sandra_day_oconnor\n",
      "  ✓ Done! Processed 3663 articles (Total: 400446 articles)\n",
      "\n",
      "[7/16] Processing: sean_hannity\n",
      "  ✓ Done! Processed 5331 articles (Total: 405777 articles)\n",
      "\n",
      "[8/16] Processing: spike_lee\n",
      "  ✓ Done! Processed 2341 articles (Total: 408118 articles)\n",
      "\n",
      "[9/16] Processing: sterling_brown\n",
      "  ✓ Done! Processed 4010 articles (Total: 412128 articles)\n",
      "\n",
      "[10/16] Processing: taylor_swift\n",
      "  ✓ Done! Processed 3762 articles (Total: 415890 articles)\n",
      "\n",
      "[11/16] Processing: theresa_may\n",
      "  ✓ Done! Processed 22164 articles (Total: 438054 articles)\n",
      "\n",
      "[12/16] Processing: tiger_woods\n",
      "  ✓ Done! Processed 4411 articles (Total: 442465 articles)\n",
      "\n",
      "[13/16] Processing: trevor_noah\n",
      "  ✓ Done! Processed 3190 articles (Total: 445655 articles)\n",
      "\n",
      "[14/16] Processing: viola_davis\n",
      "  ✓ Done! Processed 6072 articles (Total: 451727 articles)\n",
      "\n",
      "[15/16] Processing: vladimir_putin\n",
      "  ✓ Done! Processed 4956 articles (Total: 456683 articles)\n",
      "\n",
      "[16/16] Processing: xi_jinping\n",
      "  ✓ Done! Processed 4039 articles (Total: 460722 articles)\n",
      "\n",
      "All processing complete!\n",
      "Final embeddings shape: (460722, 1024)\n",
      "Total articles: 460722\n"
     ]
    }
   ],
   "source": [
    "# 데이터 수집 및 embedding 생성 (인물별로 처리)\n",
    "for idx, file_path in enumerate(jsonl_files):\n",
    "    person = extract_person_name(file_path)\n",
    "    print(f\"\\n[{idx+1}/{len(jsonl_files)}] Processing: {person}\")\n",
    "    \n",
    "    # 파일에서 기사 읽기\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        articles = [json.loads(line) for line in f]\n",
    "    \n",
    "    # 현재 인물의 기사별 처리\n",
    "    person_embeddings = []\n",
    "    person_metadata = []\n",
    "    \n",
    "    for article in articles:\n",
    "        body_text = article.get('bodyText', '')\n",
    "        if not body_text:\n",
    "            continue\n",
    "        \n",
    "        # 문장 분리\n",
    "        sentences = split_into_sentences(body_text)\n",
    "        if not sentences:\n",
    "            continue\n",
    "        \n",
    "        # Chunks 생성\n",
    "        chunks = create_chunks(sentences, chunk_size=CHUNK_SIZE, overlap=OVERLAP)\n",
    "        if not chunks:\n",
    "            continue\n",
    "        \n",
    "        # 각 chunk의 embedding 생성\n",
    "        chunk_embeddings = generate_embeddings(chunks, batch_size=BATCH_SIZE)\n",
    "        \n",
    "        # Mean pooling: 한 기사의 모든 chunk embeddings를 평균\n",
    "        article_embedding = mean_pool_embeddings(chunk_embeddings)\n",
    "        \n",
    "        person_embeddings.append(article_embedding)\n",
    "        person_metadata.append({\n",
    "            'person': person,\n",
    "            'article_id': article.get('id', ''),\n",
    "            'pub_date': parse_pub_date(article.get('webPublicationDate', ''))\n",
    "        })\n",
    "    \n",
    "    # 현재 인물의 결과 저장\n",
    "    if person_embeddings:\n",
    "        person_embeddings_array = np.array(person_embeddings)\n",
    "        \n",
    "        # 기존 데이터와 합치기\n",
    "        if existing_embeddings is not None:\n",
    "            combined_embeddings = np.vstack([existing_embeddings, person_embeddings_array])\n",
    "        else:\n",
    "            combined_embeddings = person_embeddings_array\n",
    "        \n",
    "        combined_metadata = existing_metadata + person_metadata\n",
    "        \n",
    "        # 즉시 저장\n",
    "        np.save(embeddings_file, combined_embeddings)\n",
    "        with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "            for meta in combined_metadata:\n",
    "                f.write(json.dumps(meta, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        # 다음 반복을 위해 업데이트\n",
    "        existing_embeddings = combined_embeddings\n",
    "        existing_metadata = combined_metadata\n",
    "        \n",
    "        print(f\"  ✓ Done! Processed {len(person_metadata)} articles (Total: {len(combined_metadata)} articles)\")\n",
    "    \n",
    "    # Checkpoint 업데이트\n",
    "    processed_files.add(file_path.name)\n",
    "    with open(CHECKPOINT_FILE, 'w') as f:\n",
    "        json.dump({'processed_files': list(processed_files)}, f)\n",
    "\n",
    "print(f\"\\nAll processing complete!\")\n",
    "print(f\"Final embeddings shape: {existing_embeddings.shape}\")\n",
    "print(f\"Total articles: {len(existing_metadata)}\")\n",
    "\n",
    "# alex morgan -> 9m 30s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint file removed.\n",
      "\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint 삭제\n",
    "if CHECKPOINT_FILE.exists():\n",
    "    CHECKPOINT_FILE.unlink()\n",
    "    print(\"Checkpoint file removed.\")\n",
    "\n",
    "print(\"\\nAll done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelnel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "mlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
