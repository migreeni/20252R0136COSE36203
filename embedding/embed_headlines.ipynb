{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 import\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설정\n",
    "MODEL_NAME = \"BAAI/bge-large-en-v1.5\"\n",
    "DATA_DIR = Path(\"../guardian_top100_scraping\")\n",
    "OUTPUT_DIR = Path(\"vector_headlines\")\n",
    "BATCH_SIZE = 32\n",
    "CHECKPOINT_FILE = OUTPUT_DIR / \"checkpoint.json\"\n",
    "\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model과 Tokenizer load\n",
    "print(\"Loading model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_person_name(filename):\n",
    "    \"\"\"파일명에서 person 이름 추출 (예: alex_morgan.jsonl -> alex_morgan)\"\"\"\n",
    "    return filename.stem\n",
    "\n",
    "def parse_pub_date(web_pub_date):\n",
    "    \"\"\"webPublicationDate를 YYYY_MM_DD 형식으로 변환\"\"\"\n",
    "    dt = datetime.fromisoformat(web_pub_date.replace('Z', '+00:00'))\n",
    "    return dt.strftime(\"%Y_%m_%d\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_embeddings(texts, batch_size=32):\n",
    "    \"\"\"Batch 단위로 embedding 생성\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoded = tokenizer(batch, padding=True, truncation=True, \n",
    "                          max_length=512, return_tensors='pt')\n",
    "        encoded = {k: v.cuda() for k, v in encoded.items()}\n",
    "        \n",
    "        # Generate embeddings\n",
    "        outputs = model(**encoded)\n",
    "        # CLS token embedding 사용\n",
    "        batch_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Normalize\n",
    "        batch_embeddings = torch.nn.functional.normalize(batch_embeddings, p=2, dim=1)\n",
    "        \n",
    "        embeddings.append(batch_embeddings.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint 확인\n",
    "processed_files = set()\n",
    "if CHECKPOINT_FILE.exists():\n",
    "    with open(CHECKPOINT_FILE, 'r') as f:\n",
    "        checkpoint = json.load(f)\n",
    "        processed_files = set(checkpoint.get('processed_files', []))\n",
    "        print(f\"Checkpoint found: {len(processed_files)} files already processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 .jsonl 파일 수집\n",
    "jsonl_files = sorted([f for f in DATA_DIR.glob(\"*.jsonl\") if f.name not in processed_files])\n",
    "print(f\"Total files to process: {len(jsonl_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 저장된 데이터 로드 (있다면)\n",
    "embeddings_file = OUTPUT_DIR / \"embeddings.npy\"\n",
    "metadata_file = OUTPUT_DIR / \"metadata.jsonl\"\n",
    "\n",
    "if embeddings_file.exists() and metadata_file.exists():\n",
    "    print(\"Loading existing data...\")\n",
    "    existing_embeddings = np.load(embeddings_file)\n",
    "    existing_metadata = []\n",
    "    with open(metadata_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            existing_metadata.append(json.loads(line))\n",
    "    print(f\"Loaded {len(existing_metadata)} existing entries\")\n",
    "else:\n",
    "    existing_embeddings = None\n",
    "    existing_metadata = []\n",
    "    print(\"Starting fresh (no existing data found)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 수집 및 embedding 생성 (인물별로 처리)\n",
    "for idx, file_path in enumerate(jsonl_files):\n",
    "    person = extract_person_name(file_path)\n",
    "    print(f\"\\n[{idx+1}/{len(jsonl_files)}] Processing: {person}\")\n",
    "    \n",
    "    # 파일에서 기사 읽기\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        articles = [json.loads(line) for line in f]\n",
    "    \n",
    "    # 현재 인물의 headline과 metadata 추출\n",
    "    person_headlines = []\n",
    "    person_metadata = []\n",
    "    \n",
    "    for article in articles:\n",
    "        headline = article.get('headline', '')\n",
    "        if headline:  # headline이 있는 경우만 처리\n",
    "            person_headlines.append(headline)\n",
    "            person_metadata.append({\n",
    "                'person': person,\n",
    "                'article_id': article.get('id', ''),\n",
    "                'pub_date': parse_pub_date(article.get('webPublicationDate', '')),\n",
    "                'headline': headline\n",
    "            })\n",
    "    \n",
    "    # 현재 인물의 embedding 생성\n",
    "    if person_headlines:\n",
    "        print(f\"  → Generating embeddings for {len(person_headlines)} headlines...\")\n",
    "        person_embeddings = generate_embeddings(person_headlines, batch_size=BATCH_SIZE)\n",
    "        \n",
    "        # 기존 데이터와 합치기\n",
    "        if existing_embeddings is not None:\n",
    "            combined_embeddings = np.vstack([existing_embeddings, person_embeddings])\n",
    "        else:\n",
    "            combined_embeddings = person_embeddings\n",
    "        \n",
    "        combined_metadata = existing_metadata + person_metadata\n",
    "        \n",
    "        # 즉시 저장\n",
    "        np.save(embeddings_file, combined_embeddings)\n",
    "        with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "            for meta in combined_metadata:\n",
    "                f.write(json.dumps(meta, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        # 다음 반복을 위해 업데이트\n",
    "        existing_embeddings = combined_embeddings\n",
    "        existing_metadata = combined_metadata\n",
    "        \n",
    "        print(f\"  ✓ Done! (Total: {len(combined_metadata)} headlines processed)\")\n",
    "    \n",
    "    # Checkpoint 업데이트\n",
    "    processed_files.add(file_path.name)\n",
    "    with open(CHECKPOINT_FILE, 'w') as f:\n",
    "        json.dump({'processed_files': list(processed_files)}, f)\n",
    "\n",
    "print(f\"\\nAll processing complete!\")\n",
    "print(f\"Final embeddings shape: {existing_embeddings.shape}\")\n",
    "print(f\"Total headlines: {len(existing_metadata)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint 삭제\n",
    "if CHECKPOINT_FILE.exists():\n",
    "    CHECKPOINT_FILE.unlink()\n",
    "    print(\"Checkpoint file removed.\")\n",
    "\n",
    "print(\"\\nAll done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
