{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULwQ56tnfAcr"
      },
      "source": [
        "**Cell 0**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylDIBMdReoLP"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbZEYxOwfIpK"
      },
      "source": [
        "**Cell 1: 환경 설정 및 라이브러리 임포트**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRBflq8bfMrt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from transformers import AutoModel\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from tqdm.auto import tqdm\n",
        "import logging\n",
        "\n",
        "# 로깅 설정\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# GPU 설정\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnTDzbHTfRmF"
      },
      "source": [
        "**Cell 2: 경로 및 모델 설정**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCUb1GenfVEk"
      },
      "outputs": [],
      "source": [
        "# === 설정 ===\n",
        "MODEL_NAME = \"jinaai/jina-embeddings-v3\"\n",
        "DATA_DIR = Path(\"/content/drive/MyDrive/COSE362/data/guardian_top100_scraping\")\n",
        "OUTPUT_DIR = Path(\"/content/drive/MyDrive/COSE362/data/vector_paragraphs\")\n",
        "CHECKPOINT_FILE = OUTPUT_DIR / \"checkpoint.json\"\n",
        "\n",
        "# Jina v3 (1024차원) 배치 사이즈 설정\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"Output Directory: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tCRsP7gfe3Y"
      },
      "source": [
        "**Cell 3: 모델 로드**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UTgMtuBfhzg"
      },
      "outputs": [],
      "source": [
        "# Model Load\n",
        "print(f\"Loading model: {MODEL_NAME} ...\")\n",
        "\n",
        "# trust_remote_code=True 필수\n",
        "model = AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "model = model.to(DEVICE)\n",
        "model.eval()\n",
        "print(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTHJNR5tfkX2"
      },
      "source": [
        "**Cell 4: 헬퍼 함수 정의**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZu3G2UzfndG"
      },
      "outputs": [],
      "source": [
        "def extract_person_name(filename):\n",
        "    \"\"\"파일명에서 person 이름 추출\"\"\"\n",
        "    return filename.stem\n",
        "\n",
        "def parse_pub_date(web_pub_date):\n",
        "    \"\"\"webPublicationDate를 YYYY_MM_DD 형식으로 변환\"\"\"\n",
        "    try:\n",
        "        dt = datetime.fromisoformat(web_pub_date.replace('Z', '+00:00'))\n",
        "        return dt.strftime(\"%Y_%m_%d\")\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def preprocess_text_first_last(text):\n",
        "    \"\"\"\n",
        "    기사의 첫 문단과 마지막 문단만 추출하여 결합\n",
        "    \"\"\"\n",
        "    if not text or text.strip() == '':\n",
        "        return None\n",
        "\n",
        "    # 줄바꿈 기준 문단 분리\n",
        "    paragraphs = [line.strip() for line in text.split('\\n') if line.strip()]\n",
        "\n",
        "    if not paragraphs:\n",
        "        return None\n",
        "\n",
        "    if len(paragraphs) == 1:\n",
        "        # 문단이 하나뿐이면 그것만 사용\n",
        "        return paragraphs[0]\n",
        "    else:\n",
        "        # 첫 문단 + 공백 + 마지막 문단\n",
        "        return f\"{paragraphs[0]} {paragraphs[-1]}\"\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_embeddings_jina(texts, batch_size=16):\n",
        "    \"\"\"Jina v3 전용 임베딩 생성 (task='retrieval.passage')\"\"\"\n",
        "    embeddings = model.encode(\n",
        "        texts,\n",
        "        task=\"retrieval.passage\",\n",
        "        batch_size=batch_size,\n",
        "        show_progress_bar=False\n",
        "    )\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7XgcJaffs-U"
      },
      "source": [
        "**Cell 5:  체크포인트 및 데이터 정합성 검사**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHIJXaJkfwhz"
      },
      "outputs": [],
      "source": [
        "# 1. Checkpoint 확인\n",
        "processed_files = set()\n",
        "if CHECKPOINT_FILE.exists():\n",
        "    try:\n",
        "        with open(CHECKPOINT_FILE, 'r') as f:\n",
        "            checkpoint = json.load(f)\n",
        "            processed_files = set(checkpoint.get('processed_files', []))\n",
        "            print(f\"Checkpoint found: {len(processed_files)} files already processed\")\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"Checkpoint corrupted. Starting fresh.\")\n",
        "        processed_files = set()\n",
        "\n",
        "# 2. 기존 데이터 정합성 확인\n",
        "embeddings_file = OUTPUT_DIR / \"embeddings.npy\"\n",
        "metadata_file = OUTPUT_DIR / \"metadata.jsonl\"\n",
        "\n",
        "if embeddings_file.exists() and metadata_file.exists():\n",
        "    print(\"Checking existing data consistency...\")\n",
        "    try:\n",
        "        # 메타데이터 라인 수 계산\n",
        "        with open(metadata_file, 'r', encoding='utf-8') as f:\n",
        "            meta_count = sum(1 for _ in f)\n",
        "\n",
        "        # 임베딩 로드하여 개수 확인\n",
        "        temp_emb = np.load(embeddings_file)\n",
        "        emb_count = len(temp_emb)\n",
        "        del temp_emb # 메모리 해제\n",
        "\n",
        "        if emb_count == meta_count:\n",
        "            print(f\"Data is consistent. {meta_count} rows loaded. Resuming...\")\n",
        "        else:\n",
        "            print(f\"Mismatch detected! Emb: {emb_count}, Meta: {meta_count}. Resetting data.\")\n",
        "            if embeddings_file.exists(): os.remove(embeddings_file)\n",
        "            if metadata_file.exists(): os.remove(metadata_file)\n",
        "            processed_files = set()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error checking data: {e}. Resetting.\")\n",
        "        if embeddings_file.exists(): os.remove(embeddings_file)\n",
        "        if metadata_file.exists(): os.remove(metadata_file)\n",
        "        processed_files = set()\n",
        "\n",
        "elif embeddings_file.exists() or metadata_file.exists():\n",
        "    # 둘 중 하나만 있으면 꼬인 것이므로 삭제\n",
        "    print(\"File pair incomplete. Deleting and starting fresh.\")\n",
        "    if embeddings_file.exists(): os.remove(embeddings_file)\n",
        "    if metadata_file.exists(): os.remove(metadata_file)\n",
        "    processed_files = set()\n",
        "\n",
        "# 처리할 파일 목록 생성\n",
        "jsonl_files = sorted([f for f in DATA_DIR.glob(\"*.jsonl\") if f.name not in processed_files])\n",
        "print(f\"Total files to process: {len(jsonl_files)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j83FJLgxfzMI"
      },
      "source": [
        "**Cell 6: 메인 루프 (실행 및 저장)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CORbEJsPf3Wp"
      },
      "outputs": [],
      "source": [
        "for idx, file_path in enumerate(tqdm(jsonl_files, desc=\"Processing Files\")):\n",
        "    person = extract_person_name(file_path)\n",
        "\n",
        "    # 1. 파일 읽기\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            articles = [json.loads(line) for line in f]\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file_path}: {e}\")\n",
        "        continue\n",
        "\n",
        "    valid_texts = []\n",
        "    valid_metadata = []\n",
        "\n",
        "    # 2. 전처리 (첫문단 + 끝문단)\n",
        "    for article in articles:\n",
        "        body_text = article.get('bodyText', '')\n",
        "        article_id = article.get('id')\n",
        "        pub_date_raw = article.get('webPublicationDate')\n",
        "\n",
        "        if not all([body_text, article_id, pub_date_raw]):\n",
        "            continue\n",
        "\n",
        "        processed_text = preprocess_text_first_last(body_text)\n",
        "\n",
        "        if processed_text:\n",
        "            valid_texts.append(processed_text)\n",
        "            valid_metadata.append({\n",
        "                'person': person,\n",
        "                'article_id': article_id,\n",
        "                'pub_date': parse_pub_date(pub_date_raw)\n",
        "            })\n",
        "\n",
        "    # 3. 임베딩 생성 및 저장\n",
        "    if valid_texts:\n",
        "        try:\n",
        "            # 임베딩 생성\n",
        "            new_embeddings = generate_embeddings_jina(valid_texts, batch_size=BATCH_SIZE)\n",
        "\n",
        "            if new_embeddings.size > 0:\n",
        "                # 기존 파일 로드 -> 합치기(vstack) -> 저장\n",
        "                if embeddings_file.exists():\n",
        "                    current_emb = np.load(embeddings_file)\n",
        "                    updated_emb = np.vstack([current_emb, new_embeddings])\n",
        "                    np.save(embeddings_file, updated_emb)\n",
        "                    del current_emb, updated_emb # 메모리 정리\n",
        "                else:\n",
        "                    np.save(embeddings_file, new_embeddings)\n",
        "\n",
        "                # 메타데이터 이어쓰기 ('a' mode)\n",
        "                with open(metadata_file, 'a', encoding='utf-8') as f:\n",
        "                    for meta in valid_metadata:\n",
        "                        f.write(json.dumps(meta, ensure_ascii=False) + '\\n')\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to save data for {person}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # 4. 체크포인트 업데이트\n",
        "    processed_files.add(file_path.name)\n",
        "    with open(CHECKPOINT_FILE, 'w') as f:\n",
        "        json.dump({'processed_files': list(processed_files)}, f)\n",
        "\n",
        "print(\"\\nProcessing complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOg4Bf1sf9t3"
      },
      "source": [
        "**Cell 7: 결과 확인 및 정리**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5dHvnbbgAio"
      },
      "outputs": [],
      "source": [
        "# 최종 결과 확인\n",
        "if embeddings_file.exists():\n",
        "    final_emb = np.load(embeddings_file)\n",
        "    print(f\"Final embeddings shape: {final_emb.shape}\")\n",
        "\n",
        "    with open(metadata_file, 'r', encoding='utf-8') as f:\n",
        "        final_meta_count = sum(1 for _ in f)\n",
        "    print(f\"Total articles in metadata: {final_meta_count}\")\n",
        "\n",
        "# 체크포인트 파일 삭제 (완료되었으므로)\n",
        "if CHECKPOINT_FILE.exists():\n",
        "    CHECKPOINT_FILE.unlink()\n",
        "    print(\"Checkpoint file removed.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
