{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wonjun/.conda/envs/mlproject/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 import\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Processing people 0 to 99\n"
     ]
    }
   ],
   "source": [
    "# ===== GPU 분할 설정 =====\n",
    "GPU_ID = 0\n",
    "START_IDX = 0\n",
    "END_IDX = 100  \n",
    "# ======================================================\n",
    "\n",
    "# 설정\n",
    "MODEL_NAME = \"jinaai/jina-embeddings-v3\"\n",
    "DATA_DIR = Path(\"../guardian_top100_scraping\")\n",
    "OUTPUT_DIR = Path(f\"vector_paragraphs\")\n",
    "BATCH_SIZE = 4\n",
    "CHECKPOINT_FILE = OUTPUT_DIR / \"checkpoint.json\"\n",
    "\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "print(f\"GPU {GPU_ID}: Processing people {START_IDX} to {END_IDX-1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_first_last(text):\n",
    "    \"\"\"\n",
    "    기사의 첫 문단과 마지막 문단만 추출하여 결합\n",
    "    \"\"\"\n",
    "    if not text or text.strip() == '':\n",
    "        return None\n",
    "    \n",
    "    # 줄바꿈을 기준으로 문단 분리 (공백 라인 제거)\n",
    "    paragraphs = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "    \n",
    "    if not paragraphs:\n",
    "        return None\n",
    "    \n",
    "    if len(paragraphs) == 1:\n",
    "        # 문단이 하나뿐이면 그것만 사용\n",
    "        return paragraphs[0]\n",
    "    else:\n",
    "        # 첫 문단 + 공백 + 마지막 문단\n",
    "        return f\"{paragraphs[0]} {paragraphs[-1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Model과 Tokenizer load\n",
    "print(\"Loading model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_person_name(filename):\n",
    "    \"\"\"파일명에서 person 이름 추출 (예: alex_morgan.jsonl -> alex_morgan)\"\"\"\n",
    "    return filename.stem\n",
    "\n",
    "def parse_pub_date(web_pub_date):\n",
    "    \"\"\"webPublicationDate를 YYYY_MM_DD 형식으로 변환\"\"\"\n",
    "    dt = datetime.fromisoformat(web_pub_date.replace('Z', '+00:00'))\n",
    "    return dt.strftime(\"%Y_%m_%d\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_embeddings(texts, batch_size=32):\n",
    "    \"\"\"Batch 단위로 embedding 생성 (truncation 자동 처리)\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    # Progress bar 추가\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"    Embedding batches\", leave=False):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize (truncation=True로 자동 처리)\n",
    "        encoded = tokenizer(\n",
    "            batch, \n",
    "            padding=True, \n",
    "            truncation=True,  # 8192 토큰 초과 시 자동 truncate\n",
    "            max_length=8192, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        encoded = {k: v.cuda() for k, v in encoded.items()}\n",
    "        \n",
    "        # Generate embeddings\n",
    "        outputs = model(**encoded)\n",
    "        # CLS token embedding 사용\n",
    "        batch_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Normalize\n",
    "        batch_embeddings = torch.nn.functional.normalize(batch_embeddings, p=2, dim=1)\n",
    "        \n",
    "        # BFloat16 -> Float32 변환 후 numpy로 변환\n",
    "        batch_embeddings = batch_embeddings.to(torch.float32)\n",
    "        \n",
    "        embeddings.append(batch_embeddings.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "def verify_lengths(person, embeddings_array, metadata_list):\n",
    "    \"\"\"Embeddings와 metadata 길이 확인\"\"\"\n",
    "    emb_len = len(embeddings_array)\n",
    "    meta_len = len(metadata_list)\n",
    "    \n",
    "    if emb_len == meta_len:\n",
    "        print(f\"  ✓ Length verification passed: {emb_len} entries\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"  ✗ Length mismatch! Embeddings: {emb_len}, Metadata: {meta_len}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint found: 98 files already processed\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint 확인\n",
    "processed_files = set()\n",
    "if CHECKPOINT_FILE.exists():\n",
    "    with open(CHECKPOINT_FILE, 'r') as f:\n",
    "        checkpoint = json.load(f)\n",
    "        processed_files = set(checkpoint.get('processed_files', []))\n",
    "        print(f\"Checkpoint found: {len(processed_files)} files already processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files to process in this GPU: 2\n"
     ]
    }
   ],
   "source": [
    "# 모든 .jsonl 파일 수집 및 GPU별 분할\n",
    "all_files = sorted([f for f in DATA_DIR.glob(\"*.jsonl\")])\n",
    "jsonl_files = all_files[START_IDX:END_IDX]\n",
    "jsonl_files = [f for f in jsonl_files if f.name not in processed_files]\n",
    "\n",
    "print(f\"Total files to process in this GPU: {len(jsonl_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/2] Processing: vladimir_putin (Total articles: 4958)\n",
      "  → Generating embeddings for 4956 valid articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Length verification passed: 4956 entries\n",
      "  ✓ Saved: vladimir_putin_embeddings.npy, vladimir_putin_metadata.jsonl\n",
      "\n",
      "[2/2] Processing: xi_jinping (Total articles: 4048)\n",
      "  → Generating embeddings for 4039 valid articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Length verification passed: 4039 entries\n",
      "  ✓ Saved: xi_jinping_embeddings.npy, xi_jinping_metadata.jsonl\n",
      "\n",
      "All processing complete!\n"
     ]
    }
   ],
   "source": [
    "# 데이터 수집 및 embedding 생성 (인물별로 처리)\n",
    "for idx, file_path in enumerate(jsonl_files):\n",
    "    person = extract_person_name(file_path)\n",
    "    \n",
    "    # 파일에서 기사 읽기\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        articles = [json.loads(line) for line in f]\n",
    "    \n",
    "    total_articles = len(articles)\n",
    "    print(f\"\\n[{idx+1}/{len(jsonl_files)}] Processing: {person} (Total articles: {total_articles})\")\n",
    "    \n",
    "    # 현재 인물의 body text와 metadata 추출\n",
    "    person_texts = []\n",
    "    person_metadata = []\n",
    "    \n",
    "    # Article 수집\n",
    "    for article in articles:\n",
    "        body_text = article.get('bodyText', '')\n",
    "        article_id = article.get('id')\n",
    "        pub_date_raw = article.get('webPublicationDate')\n",
    "        \n",
    "        if not all([body_text, article_id, pub_date_raw]):\n",
    "            continue\n",
    "        \n",
    "        # 첫 문단 + 마지막 문단 추출\n",
    "        processed_text = preprocess_text_first_last(body_text)\n",
    "        \n",
    "        if processed_text:\n",
    "            person_texts.append(processed_text)\n",
    "            person_metadata.append({\n",
    "                'person': person,\n",
    "                'article_id': article_id,\n",
    "                'pub_date': parse_pub_date(pub_date_raw)\n",
    "            })\n",
    "    \n",
    "    # 현재 인물의 embedding 생성\n",
    "    if person_texts:\n",
    "        print(f\"  → Generating embeddings for {len(person_texts)} valid articles...\")\n",
    "        person_embeddings = generate_embeddings(person_texts, batch_size=BATCH_SIZE)\n",
    "        \n",
    "        # 길이 검증\n",
    "        if not verify_lengths(person, person_embeddings, person_metadata):\n",
    "            print(f\"  ⚠️  Warning: Skipping save due to length mismatch\")\n",
    "            continue\n",
    "        \n",
    "        # 개별 파일로 저장\n",
    "        person_emb_file = OUTPUT_DIR / f\"{person}_embeddings.npy\"\n",
    "        person_meta_file = OUTPUT_DIR / f\"{person}_metadata.jsonl\"\n",
    "        \n",
    "        np.save(person_emb_file, person_embeddings)\n",
    "        with open(person_meta_file, 'w', encoding='utf-8') as f:\n",
    "            for meta in person_metadata:\n",
    "                f.write(json.dumps(meta, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        print(f\"  ✓ Saved: {person_emb_file.name}, {person_meta_file.name}\")\n",
    "    \n",
    "    # Checkpoint 업데이트\n",
    "    processed_files.add(file_path.name)\n",
    "    with open(CHECKPOINT_FILE, 'w') as f:\n",
    "        json.dump({'processed_files': list(processed_files)}, f)\n",
    "\n",
    "print(f\"\\nAll processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint file removed.\n",
      "\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint 삭제\n",
    "if CHECKPOINT_FILE.exists():\n",
    "    CHECKPOINT_FILE.unlink()\n",
    "    print(\"Checkpoint file removed.\")\n",
    "\n",
    "print(\"\\nAll done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 매칭된 파일 쌍 개수: 100\n",
      "저장 완료: vector_paragraphs/embeddings.npy\n",
      "저장 완료: vector_paragraphs/metadata.jsonl\n"
     ]
    }
   ],
   "source": [
    "# 모든 *_embeddings.npy / *_metadata.jsonl 병합해서 -> embeddings.npy, metadata.jsonl 생성\n",
    "\n",
    "root = Path(\"vector_paragraphs\")  # 폴더 경로\n",
    "\n",
    "# 파일 리스트 수집\n",
    "embed_files = sorted(root.glob(\"*_embeddings.npy\"))\n",
    "meta_files = sorted(root.glob(\"*_metadata.jsonl\"))\n",
    "\n",
    "# base name (사람 이름 등) 기준으로 매칭\n",
    "def get_key(path, tail):\n",
    "    name = path.name\n",
    "    if not name.endswith(tail):\n",
    "        return None\n",
    "    return name[: -len(tail)]\n",
    "\n",
    "embed_map = {get_key(p, \"_embeddings.npy\"): p for p in embed_files}\n",
    "meta_map  = {get_key(p, \"_metadata.jsonl\"): p for p in meta_files}\n",
    "\n",
    "common_keys = sorted(set(embed_map.keys()) & set(meta_map.keys()))\n",
    "\n",
    "print(f\"총 매칭된 파일 쌍 개수: {len(common_keys)}\")\n",
    "\n",
    "emb_list = []\n",
    "meta_out_path = root / \"metadata.jsonl\"\n",
    "\n",
    "with meta_out_path.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "    for key in common_keys:\n",
    "        e_path = embed_map[key]\n",
    "        m_path = meta_map[key]\n",
    "\n",
    "        # embeddings 병합\n",
    "        emb = np.load(e_path)\n",
    "        emb_list.append(emb)\n",
    "\n",
    "        # metadata 병합(jsonl -> 그대로 줄 단위 복사)\n",
    "        with m_path.open(\"r\", encoding=\"utf-8\") as fin:\n",
    "            for line in fin:\n",
    "                if line.strip():\n",
    "                    fout.write(line if line.endswith(\"\\n\") else line + \"\\n\")\n",
    "\n",
    "# 하나의 embeddings 배열로 합치기\n",
    "combined_emb = np.concatenate(emb_list, axis=0)\n",
    "emb_out_path = root / \"embeddings.npy\"\n",
    "np.save(emb_out_path, combined_emb)\n",
    "\n",
    "print(f\"저장 완료: {emb_out_path}\")\n",
    "print(f\"저장 완료: {meta_out_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
