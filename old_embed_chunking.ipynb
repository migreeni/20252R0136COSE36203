{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 import\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설정\n",
    "MODEL_NAME = \"BAAI/bge-large-en-v1.5\"\n",
    "DATA_DIR = Path(\"guardian_top100_scraping\")\n",
    "OUTPUT_DIR = Path(\"vector_chunking\")\n",
    "BATCH_SIZE = 32\n",
    "CHECKPOINT_FILE = OUTPUT_DIR / \"checkpoint.json\"\n",
    "\n",
    "# Chunking 설정\n",
    "CHUNK_SIZE = 2  # 두 문장씩\n",
    "OVERLAP = 1     # 한 문장씩 overlap\n",
    "\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Model과 Tokenizer load\n",
    "print(\"Loading model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_person_name(filename):\n",
    "    \"\"\"파일명에서 person 이름 추출\"\"\"\n",
    "    return filename.stem\n",
    "\n",
    "def parse_pub_date(web_pub_date):\n",
    "    \"\"\"webPublicationDate를 YYYY_MM_DD 형식으로 변환\"\"\"\n",
    "    dt = datetime.fromisoformat(web_pub_date.replace('Z', '+00:00'))\n",
    "    return dt.strftime(\"%Y_%m_%d\")\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    \"\"\"텍스트를 문장 단위로 분리 (. 기준)\"\"\"\n",
    "    sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "    return sentences\n",
    "\n",
    "def create_chunks(sentences, chunk_size=2, overlap=1):\n",
    "    \"\"\"문장들을 chunk_size만큼 묶고, overlap만큼 겹치게 생성\"\"\"\n",
    "    chunks = []\n",
    "    step = chunk_size - overlap\n",
    "    \n",
    "    for i in range(0, len(sentences), step):\n",
    "        chunk_sentences = sentences[i:i+chunk_size]\n",
    "        if chunk_sentences:\n",
    "            chunk_text = '. '.join(chunk_sentences) + '.'\n",
    "            chunks.append(chunk_text)\n",
    "        \n",
    "        if i + chunk_size >= len(sentences):\n",
    "            break\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_embeddings(texts, batch_size=32):\n",
    "    \"\"\"Batch 단위로 embedding 생성\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoded = tokenizer(batch, padding=True, truncation=True, \n",
    "                          max_length=512, return_tensors='pt')\n",
    "        encoded = {k: v.cuda() for k, v in encoded.items()}\n",
    "        \n",
    "        # Generate embeddings\n",
    "        outputs = model(**encoded)\n",
    "        # CLS token embedding 사용\n",
    "        batch_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Normalize\n",
    "        batch_embeddings = torch.nn.functional.normalize(batch_embeddings, p=2, dim=1)\n",
    "        \n",
    "        embeddings.append(batch_embeddings.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "def mean_pool_embeddings(embeddings):\n",
    "    \"\"\"여러 chunk embeddings를 mean pooling\"\"\"\n",
    "    return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint found: 1 files already processed\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint 확인\n",
    "processed_files = set()\n",
    "if CHECKPOINT_FILE.exists():\n",
    "    with open(CHECKPOINT_FILE, 'r') as f:\n",
    "        checkpoint = json.load(f)\n",
    "        processed_files = set(checkpoint.get('processed_files', []))\n",
    "        print(f\"Checkpoint found: {len(processed_files)} files already processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files to process: 99\n"
     ]
    }
   ],
   "source": [
    "# 모든 .jsonl 파일 수집\n",
    "jsonl_files = sorted([f for f in DATA_DIR.glob(\"*.jsonl\") if f.name not in processed_files])\n",
    "print(f\"Total files to process: {len(jsonl_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/99] Processing: alicia_keys\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# 각 chunk의 embedding 생성\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m chunk_embeddings = \u001b[43mgenerate_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Mean pooling: 한 기사의 모든 chunk embeddings를 평균\u001b[39;00m\n\u001b[32m     38\u001b[39m article_embedding = mean_pool_embeddings(chunk_embeddings)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mlproject/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mgenerate_embeddings\u001b[39m\u001b[34m(texts, batch_size)\u001b[39m\n\u001b[32m     49\u001b[39m     \u001b[38;5;66;03m# Normalize\u001b[39;00m\n\u001b[32m     50\u001b[39m     batch_embeddings = torch.nn.functional.normalize(batch_embeddings, p=\u001b[32m2\u001b[39m, dim=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     embeddings.append(\u001b[43mbatch_embeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.numpy())\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.vstack(embeddings)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 데이터 수집 및 embedding 생성 (인물별로 처리)\n",
    "all_embeddings = []\n",
    "all_metadata = []\n",
    "total_articles = 0\n",
    "\n",
    "for idx, file_path in enumerate(jsonl_files):\n",
    "    person = extract_person_name(file_path)\n",
    "    print(f\"\\n[{idx+1}/{len(jsonl_files)}] Processing: {person}\")\n",
    "    \n",
    "    # 파일에서 기사 읽기\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        articles = [json.loads(line) for line in f]\n",
    "    \n",
    "    # 현재 인물의 기사별 처리\n",
    "    person_embeddings = []\n",
    "    person_metadata = []\n",
    "    article_count = 0\n",
    "    \n",
    "    for article in articles:\n",
    "        body_text = article.get('bodyText', '')\n",
    "        if not body_text:\n",
    "            continue\n",
    "        \n",
    "        # 문장 분리\n",
    "        sentences = split_into_sentences(body_text)\n",
    "        if not sentences:\n",
    "            continue\n",
    "        \n",
    "        # Chunks 생성\n",
    "        chunks = create_chunks(sentences, chunk_size=CHUNK_SIZE, overlap=OVERLAP)\n",
    "        if not chunks:\n",
    "            continue\n",
    "        \n",
    "        # 각 chunk의 embedding 생성\n",
    "        chunk_embeddings = generate_embeddings(chunks, batch_size=BATCH_SIZE)\n",
    "        \n",
    "        # Mean pooling: 한 기사의 모든 chunk embeddings를 평균\n",
    "        article_embedding = mean_pool_embeddings(chunk_embeddings)\n",
    "        \n",
    "        person_embeddings.append(article_embedding)\n",
    "        person_metadata.append({\n",
    "            'person': person,\n",
    "            'article_id': article.get('id', ''),\n",
    "            'pub_date': parse_pub_date(article.get('webPublicationDate', ''))\n",
    "        })\n",
    "        article_count += 1\n",
    "    \n",
    "    # 현재 인물의 결과 저장\n",
    "    if person_embeddings:\n",
    "        person_embeddings_array = np.array(person_embeddings)\n",
    "        all_embeddings.append(person_embeddings_array)\n",
    "        all_metadata.extend(person_metadata)\n",
    "        total_articles += article_count\n",
    "        print(f\"  ✓ Done! Processed {article_count} articles (Total: {total_articles} articles)\")\n",
    "    \n",
    "    # Checkpoint 업데이트\n",
    "    processed_files.add(file_path.name)\n",
    "    with open(CHECKPOINT_FILE, 'w') as f:\n",
    "        json.dump({'processed_files': list(processed_files)}, f)\n",
    "\n",
    "# 모든 embeddings 합치기\n",
    "print(f\"\\nCombining all embeddings...\")\n",
    "embeddings = np.vstack(all_embeddings)\n",
    "print(f\"Final embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Total articles: {len(all_metadata)}\")\n",
    "\n",
    "# 5829 articles -> 18분 정도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 저장\n",
    "print(\"\\nSaving results...\")\n",
    "\n",
    "# embeddings.npy 저장\n",
    "np.save(OUTPUT_DIR / \"embeddings.npy\", embeddings)\n",
    "print(f\"Saved: {OUTPUT_DIR / 'embeddings.npy'}\")\n",
    "\n",
    "# metadata.jsonl 저장\n",
    "with open(OUTPUT_DIR / \"metadata.jsonl\", 'w', encoding='utf-8') as f:\n",
    "    for meta in all_metadata:\n",
    "        f.write(json.dumps(meta, ensure_ascii=False) + '\\n')\n",
    "print(f\"Saved: {OUTPUT_DIR / 'metadata.jsonl'}\")\n",
    "\n",
    "# Checkpoint 삭제 (완료되었으므로)\n",
    "if CHECKPOINT_FILE.exists():\n",
    "    CHECKPOINT_FILE.unlink()\n",
    "\n",
    "print(\"\\nAll done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelnel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "mlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
