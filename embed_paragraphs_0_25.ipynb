{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wonjun/.conda/envs/mlproject/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 import\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Processing people 0 to 99\n"
     ]
    }
   ],
   "source": [
    "# ===== GPU 분할 설정 (이 부분만 각 파일마다 다르게!) =====\n",
    "GPU_ID = 0\n",
    "START_IDX = 0\n",
    "END_IDX = 100  # 25명씩 분할\n",
    "# ======================================================\n",
    "\n",
    "# 설정\n",
    "MODEL_NAME = \"jinaai/jina-embeddings-v3\"\n",
    "DATA_DIR = Path(\"guardian_top100_scraping\")\n",
    "OUTPUT_DIR = Path(f\"vector_paragraphs_gpu{GPU_ID}\")\n",
    "BATCH_SIZE = 8\n",
    "CHECKPOINT_FILE = OUTPUT_DIR / \"checkpoint.json\"\n",
    "\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "print(f\"GPU {GPU_ID}: Processing people {START_IDX} to {END_IDX-1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_first_last(text):\n",
    "    \"\"\"\n",
    "    기사의 첫 문단과 마지막 문단만 추출하여 결합\n",
    "    \"\"\"\n",
    "    if not text or text.strip() == '':\n",
    "        return None\n",
    "    \n",
    "    # 줄바꿈을 기준으로 문단 분리 (공백 라인 제거)\n",
    "    paragraphs = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "    \n",
    "    if not paragraphs:\n",
    "        return None\n",
    "    \n",
    "    if len(paragraphs) == 1:\n",
    "        # 문단이 하나뿐이면 그것만 사용\n",
    "        return paragraphs[0]\n",
    "    else:\n",
    "        # 첫 문단 + 공백 + 마지막 문단\n",
    "        return f\"{paragraphs[0]} {paragraphs[-1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Model과 Tokenizer load\n",
    "print(\"Loading model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_person_name(filename):\n",
    "    \"\"\"파일명에서 person 이름 추출 (예: alex_morgan.jsonl -> alex_morgan)\"\"\"\n",
    "    return filename.stem\n",
    "\n",
    "def parse_pub_date(web_pub_date):\n",
    "    \"\"\"webPublicationDate를 YYYY_MM_DD 형식으로 변환\"\"\"\n",
    "    dt = datetime.fromisoformat(web_pub_date.replace('Z', '+00:00'))\n",
    "    return dt.strftime(\"%Y_%m_%d\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_embeddings(texts, batch_size=32):\n",
    "    \"\"\"Batch 단위로 embedding 생성 (truncation 자동 처리)\"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    # Progress bar 추가\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"    Embedding batches\", leave=False):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize (truncation=True로 자동 처리)\n",
    "        encoded = tokenizer(\n",
    "            batch, \n",
    "            padding=True, \n",
    "            truncation=True,  # 8192 토큰 초과 시 자동 truncate\n",
    "            max_length=8192, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        encoded = {k: v.cuda() for k, v in encoded.items()}\n",
    "        \n",
    "        # Generate embeddings\n",
    "        outputs = model(**encoded)\n",
    "        # CLS token embedding 사용\n",
    "        batch_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Normalize\n",
    "        batch_embeddings = torch.nn.functional.normalize(batch_embeddings, p=2, dim=1)\n",
    "        \n",
    "        # BFloat16 -> Float32 변환 후 numpy로 변환\n",
    "        batch_embeddings = batch_embeddings.to(torch.float32)\n",
    "        \n",
    "        embeddings.append(batch_embeddings.cpu().numpy())\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "def verify_lengths(person, embeddings_array, metadata_list):\n",
    "    \"\"\"Embeddings와 metadata 길이 확인\"\"\"\n",
    "    emb_len = len(embeddings_array)\n",
    "    meta_len = len(metadata_list)\n",
    "    \n",
    "    if emb_len == meta_len:\n",
    "        print(f\"  ✓ Length verification passed: {emb_len} entries\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"  ✗ Length mismatch! Embeddings: {emb_len}, Metadata: {meta_len}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint found: 87 files already processed\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint 확인\n",
    "processed_files = set()\n",
    "if CHECKPOINT_FILE.exists():\n",
    "    with open(CHECKPOINT_FILE, 'r') as f:\n",
    "        checkpoint = json.load(f)\n",
    "        processed_files = set(checkpoint.get('processed_files', []))\n",
    "        print(f\"Checkpoint found: {len(processed_files)} files already processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files to process in this GPU: 13\n"
     ]
    }
   ],
   "source": [
    "# 모든 .jsonl 파일 수집 및 GPU별 분할\n",
    "all_files = sorted([f for f in DATA_DIR.glob(\"*.jsonl\")])\n",
    "jsonl_files = all_files[START_IDX:END_IDX]\n",
    "jsonl_files = [f for f in jsonl_files if f.name not in processed_files]\n",
    "\n",
    "print(f\"Total files to process in this GPU: {len(jsonl_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/13] Processing: sadiq_khan (Total articles: 4420)\n",
      "  → Generating embeddings for 4420 valid articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 23.53 GiB of which 4.24 GiB is free. Including non-PyTorch memory, this process has 19.28 GiB memory in use. Of the allocated memory 17.65 GiB is allocated by PyTorch, and 1.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m person_texts:\n\u001b[32m     38\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  → Generating embeddings for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(person_texts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m valid articles...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     person_embeddings = \u001b[43mgenerate_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperson_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# 길이 검증\u001b[39;00m\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m verify_lengths(person, person_embeddings, person_metadata):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mlproject/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mgenerate_embeddings\u001b[39m\u001b[34m(texts, batch_size)\u001b[39m\n\u001b[32m     27\u001b[39m encoded = {k: v.cuda() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m encoded.items()}\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Generate embeddings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mencoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# CLS token embedding 사용\u001b[39;00m\n\u001b[32m     32\u001b[39m batch_embeddings = outputs.last_hidden_state[:, \u001b[32m0\u001b[39m, :]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mlproject/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mlproject/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/jinaai/xlm_hyphen_roberta_hyphen_flash_hyphen_implementation/2b6bc3f30750b3a9648fe9b63448c09920efe9be/modeling_lora.py:372\u001b[39m, in \u001b[36mXLMRobertaLoRA.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    371\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mlproject/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mlproject/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/jinaai/xlm_hyphen_roberta_hyphen_flash_hyphen_implementation/2b6bc3f30750b3a9648fe9b63448c09920efe9be/modeling_xlm_roberta.py:734\u001b[39m, in \u001b[36mXLMRobertaModel.forward\u001b[39m\u001b[34m(self, input_ids, position_ids, token_type_ids, attention_mask, masked_tokens_mask, return_dict, output_hidden_states, **kwargs)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    732\u001b[39m     subset_mask = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m sequence_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubset_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubset_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m    \u001b[49m\u001b[43madapter_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43madapter_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    740\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    743\u001b[39m     all_hidden_states = sequence_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mlproject/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mlproject/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/jinaai/xlm_hyphen_roberta_hyphen_flash_hyphen_implementation/2b6bc3f30750b3a9648fe9b63448c09920efe9be/modeling_xlm_roberta.py:230\u001b[39m, in \u001b[36mXLMRobertaEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, key_padding_mask, subset_mask, adapter_mask, output_hidden_states)\u001b[39m\n\u001b[32m    223\u001b[39m         hidden_states = torch.utils.checkpoint.checkpoint(\n\u001b[32m    224\u001b[39m             layer,\n\u001b[32m    225\u001b[39m             hidden_states,\n\u001b[32m    226\u001b[39m             use_reentrant=\u001b[38;5;28mself\u001b[39m.use_reentrant,\n\u001b[32m    227\u001b[39m             mixer_kwargs=mixer_kwargs,\n\u001b[32m    228\u001b[39m         )\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m         hidden_states = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmixer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmixer_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    232\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mlproject/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mlproject/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/jinaai/xlm_hyphen_roberta_hyphen_flash_hyphen_implementation/2b6bc3f30750b3a9648fe9b63448c09920efe9be/block.py:201\u001b[39m, in \u001b[36mBlock.forward\u001b[39m\u001b[34m(self, hidden_states, residual, mixer_subset, mixer_kwargs)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m residual \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m     mixer_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmixer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmixer_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmixer_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_residual:  \u001b[38;5;66;03m# mixer out is actually a pair here\u001b[39;00m\n\u001b[32m    205\u001b[39m         mixer_out, hidden_states = mixer_out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mlproject/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mlproject/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/jinaai/xlm_hyphen_roberta_hyphen_flash_hyphen_implementation/2b6bc3f30750b3a9648fe9b63448c09920efe9be/mha.py:740\u001b[39m, in \u001b[36mMHA.forward\u001b[39m\u001b[34m(self, x, x_kv, key_padding_mask, cu_seqlens, max_seqlen, mixer_subset, inference_params, adapter_mask, **kwargs)\u001b[39m\n\u001b[32m    738\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inference_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    739\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.checkpointing:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m         context = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minner_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqkv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    741\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    742\u001b[39m         context = torch.utils.checkpoint.checkpoint(\n\u001b[32m    743\u001b[39m             \u001b[38;5;28mself\u001b[39m.inner_attn, qkv, **kwargs\n\u001b[32m    744\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mlproject/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mlproject/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/jinaai/xlm_hyphen_roberta_hyphen_flash_hyphen_implementation/2b6bc3f30750b3a9648fe9b63448c09920efe9be/mha.py:271\u001b[39m, in \u001b[36mSelfAttention.forward\u001b[39m\u001b[34m(self, qkv, causal, key_padding_mask)\u001b[39m\n\u001b[32m    269\u001b[39m     padding_mask.masked_fill_(key_padding_mask, \u001b[32m0.0\u001b[39m)\n\u001b[32m    270\u001b[39m     \u001b[38;5;66;03m# TD [2022-09-30]: Adding is faster than masked_fill_ (idk why, just better kernel I guess)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m     scores = \u001b[43mscores\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mrearrange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mb s -> b 1 1 s\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m causal:\n\u001b[32m    273\u001b[39m     \u001b[38;5;66;03m# \"triu_tril_cuda_template\" not implemented for 'BFloat16'\u001b[39;00m\n\u001b[32m    274\u001b[39m     \u001b[38;5;66;03m# So we have to construct the mask in float\u001b[39;00m\n\u001b[32m    275\u001b[39m     causal_mask = torch.triu(\n\u001b[32m    276\u001b[39m         torch.full((seqlen, seqlen), -\u001b[32m10000.0\u001b[39m, device=scores.device), \u001b[32m1\u001b[39m\n\u001b[32m    277\u001b[39m     )\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 23.53 GiB of which 4.24 GiB is free. Including non-PyTorch memory, this process has 19.28 GiB memory in use. Of the allocated memory 17.65 GiB is allocated by PyTorch, and 1.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# 데이터 수집 및 embedding 생성 (인물별로 처리)\n",
    "for idx, file_path in enumerate(jsonl_files):\n",
    "    person = extract_person_name(file_path)\n",
    "    \n",
    "    # 파일에서 기사 읽기\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        articles = [json.loads(line) for line in f]\n",
    "    \n",
    "    total_articles = len(articles)\n",
    "    print(f\"\\n[{idx+1}/{len(jsonl_files)}] Processing: {person} (Total articles: {total_articles})\")\n",
    "    \n",
    "    # 현재 인물의 body text와 metadata 추출\n",
    "    person_texts = []\n",
    "    person_metadata = []\n",
    "    \n",
    "    # Article 수집\n",
    "    for article in articles:\n",
    "        body_text = article.get('bodyText', '')\n",
    "        article_id = article.get('id')\n",
    "        pub_date_raw = article.get('webPublicationDate')\n",
    "        \n",
    "        if not all([body_text, article_id, pub_date_raw]):\n",
    "            continue\n",
    "        \n",
    "        # 첫 문단 + 마지막 문단 추출\n",
    "        processed_text = preprocess_text_first_last(body_text)\n",
    "        \n",
    "        if processed_text:\n",
    "            person_texts.append(processed_text)\n",
    "            person_metadata.append({\n",
    "                'person': person,\n",
    "                'article_id': article_id,\n",
    "                'pub_date': parse_pub_date(pub_date_raw)\n",
    "            })\n",
    "    \n",
    "    # 현재 인물의 embedding 생성\n",
    "    if person_texts:\n",
    "        print(f\"  → Generating embeddings for {len(person_texts)} valid articles...\")\n",
    "        person_embeddings = generate_embeddings(person_texts, batch_size=BATCH_SIZE)\n",
    "        \n",
    "        # 길이 검증\n",
    "        if not verify_lengths(person, person_embeddings, person_metadata):\n",
    "            print(f\"  ⚠️  Warning: Skipping save due to length mismatch\")\n",
    "            continue\n",
    "        \n",
    "        # 개별 파일로 저장\n",
    "        person_emb_file = OUTPUT_DIR / f\"{person}_embeddings.npy\"\n",
    "        person_meta_file = OUTPUT_DIR / f\"{person}_metadata.jsonl\"\n",
    "        \n",
    "        np.save(person_emb_file, person_embeddings)\n",
    "        with open(person_meta_file, 'w', encoding='utf-8') as f:\n",
    "            for meta in person_metadata:\n",
    "                f.write(json.dumps(meta, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        print(f\"  ✓ Saved: {person_emb_file.name}, {person_meta_file.name}\")\n",
    "    \n",
    "    # Checkpoint 업데이트\n",
    "    processed_files.add(file_path.name)\n",
    "    with open(CHECKPOINT_FILE, 'w') as f:\n",
    "        json.dump({'processed_files': list(processed_files)}, f)\n",
    "\n",
    "print(f\"\\nAll processing complete for GPU {GPU_ID}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint 삭제\n",
    "if CHECKPOINT_FILE.exists():\n",
    "    CHECKPOINT_FILE.unlink()\n",
    "    print(\"Checkpoint file removed.\")\n",
    "\n",
    "print(\"\\nAll done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ========== 모든 GPU 결과 합치기 (주석 처리) ==========\n",
    "# print(\"Merging results from all GPUs...\\n\")\n",
    "\n",
    "# all_embeddings = []\n",
    "# all_metadata = []\n",
    "\n",
    "# # GPU 개수에 맞게 수정\n",
    "# for gpu_id in range(4):  # 예: 4개 GPU 사용\n",
    "#     gpu_dir = Path(f\"vector_paragraphs_gpu{gpu_id}\")\n",
    "#     \n",
    "#     # 해당 GPU 디렉토리의 모든 person 파일 수집\n",
    "#     person_emb_files = sorted(gpu_dir.glob(\"*_embeddings.npy\"))\n",
    "#     \n",
    "#     if not person_emb_files:\n",
    "#         print(f\"⚠️  GPU {gpu_id}: No files found, skipping...\")\n",
    "#         continue\n",
    "#     \n",
    "#     print(f\"GPU {gpu_id}: Found {len(person_emb_files)} people\")\n",
    "#     \n",
    "#     for emb_file in person_emb_files:\n",
    "#         person = emb_file.stem.replace('_embeddings', '')\n",
    "#         meta_file = gpu_dir / f\"{person}_metadata.jsonl\"\n",
    "#         \n",
    "#         if not meta_file.exists():\n",
    "#             print(f\"  ⚠️  {person}: metadata file not found, skipping...\")\n",
    "#             continue\n",
    "#         \n",
    "#         # Load embeddings\n",
    "#         emb = np.load(emb_file)\n",
    "#         all_embeddings.append(emb)\n",
    "#         \n",
    "#         # Load metadata\n",
    "#         with open(meta_file, 'r', encoding='utf-8') as f:\n",
    "#             for line in f:\n",
    "#                 all_metadata.append(json.loads(line))\n",
    "#     \n",
    "#     print(f\"  ✓ GPU {gpu_id}: Loaded {sum(len(e) for e in all_embeddings[-len(person_emb_files):])} total entries\")\n",
    "\n",
    "# # 합치기\n",
    "# if all_embeddings:\n",
    "#     final_embeddings = np.vstack(all_embeddings)\n",
    "#     \n",
    "#     # 길이 검증\n",
    "#     print(f\"\\nFinal verification:\")\n",
    "#     print(f\"  Embeddings shape: {final_embeddings.shape}\")\n",
    "#     print(f\"  Metadata entries: {len(all_metadata)}\")\n",
    "#     \n",
    "#     if final_embeddings.shape[0] != len(all_metadata):\n",
    "#         print(f\"  ✗ Length mismatch! Not saving.\")\n",
    "#     else:\n",
    "#         # 최종 결과 저장\n",
    "#         final_dir = Path(\"vector_paragraphs\")\n",
    "#         final_dir.mkdir(exist_ok=True)\n",
    "#         \n",
    "#         np.save(final_dir / \"embeddings.npy\", final_embeddings)\n",
    "#         with open(final_dir / \"metadata.jsonl\", 'w', encoding='utf-8') as f:\n",
    "#             for meta in all_metadata:\n",
    "#                 f.write(json.dumps(meta, ensure_ascii=False) + '\\n')\n",
    "#         \n",
    "#         print(f\"\\n{'='*50}\")\n",
    "#         print(f\"✅ Merge complete!\")\n",
    "#         print(f\"Final embeddings shape: {final_embeddings.shape}\")\n",
    "#         print(f\"Total articles: {len(all_metadata)}\")\n",
    "#         print(f\"Saved to: {final_dir}/\")\n",
    "#         print(f\"{'='*50}\")\n",
    "# else:\n",
    "#     print(\"\\n❌ No data found to merge!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
