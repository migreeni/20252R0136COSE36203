{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uz028ATXMbXM",
        "outputId": "34df8a0b-7953-45d8-8436-34cac45f66a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhzPdzxXMf3O",
        "outputId": "87f8f2ae-0f3b-45b3-c749-c5f64e2bfbb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "45ztn7h9Mi8Z"
      },
      "outputs": [],
      "source": [
        "!unzip -q -o \"/content/drive/My Drive/ML_team_project_final/guardian_top100_scraping.zip\" -d \"/content/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMF12ZaULeMn"
      },
      "source": [
        "Cell 1: 라이브러리 임포트 및 기본 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1LWkdSsK65k",
        "outputId": "69f48077-e024-4ad2-b8d5-75ab8029e382"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "\n",
        "# 로깅 설정\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# 0. GPU 설정 (사용 가능한 GPU 번호로 변경하세요)\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logging.info(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# 1. 설정\n",
        "MODEL_NAME = \"BAAI/bge-large-en-v1.5\" # Headine/Chunking과 동일\n",
        "DATA_DIR = Path(\"/content/guardian_top100_scraping\")\n",
        "OUTPUT_DIR = Path(\"/content/drive/My Drive/ML_team_project_final/vector_bodyText\")\n",
        "BATCH_SIZE = 32  # GPU 메모리에 따라 조절\n",
        "CHECKPOINT_FILE = OUTPUT_DIR / \"checkpoint.json\"\n",
        "\n",
        "# 2. BodyText Chunking 설정\n",
        "# (512 - 2 [CLS, SEP])\n",
        "CHUNK_LENGTH = 510\n",
        "# 토큰 기준 50개 겹치기\n",
        "OVERLAP = 50\n",
        "\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# 3. Model과 Tokenizer 로드\n",
        "logging.info(\"Loading model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModel.from_pretrained(MODEL_NAME)\n",
        "model = model.to(DEVICE)\n",
        "model.eval()\n",
        "logging.info(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw6BnysiLMPK"
      },
      "source": [
        "Cell 2: 헬퍼 함수 정의 (핵심 로직)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7eWHTwflLJ0N"
      },
      "outputs": [],
      "source": [
        "def extract_person_name(filename):\n",
        "    \"\"\"파일명에서 person 이름 추출 (예: alex_morgan.jsonl -> alex_morgan)\"\"\"\n",
        "    return filename.stem\n",
        "\n",
        "def parse_pub_date(web_pub_date):\n",
        "    \"\"\"webPublicationDate를 YYYY_MM_DD 형식으로 변환\"\"\"\n",
        "    try:\n",
        "        dt = datetime.fromisoformat(web_pub_date.replace('Z', '+00:00'))\n",
        "        return dt.strftime(\"%Y_%m_%d\")\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def create_token_chunks(text, tokenizer, chunk_length=510, overlap=50):\n",
        "    \"\"\"\n",
        "    긴 텍스트를 토큰 기준으로 'chunk_length'만큼 자르고 'overlap'만큼 겹치게 합니다.\n",
        "    Sentence-Transformer의 방식(토큰화 -> 디코딩)을 차용합니다.\n",
        "    \"\"\"\n",
        "    if not text or text.strip() == '':\n",
        "        return []\n",
        "\n",
        "    # 1. 전체 텍스트를 한 번에 토큰화 (특수 토큰 제외)\n",
        "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    if not tokens:\n",
        "        return []\n",
        "\n",
        "    # 2. 토큰 리스트를 기준으로 겹치는 윈도우 생성\n",
        "    step = chunk_length - overlap\n",
        "    token_chunks = []\n",
        "\n",
        "    for i in range(0, len(tokens), step):\n",
        "        chunk = tokens[i:i + chunk_length]\n",
        "        token_chunks.append(chunk)\n",
        "\n",
        "        # 마지막 chunk가 전체 길이를 넘어서면 중단\n",
        "        if i + chunk_length >= len(tokens):\n",
        "            break\n",
        "\n",
        "    # 3. 토큰 chunk들을 다시 텍스트(string)로 디코딩\n",
        "    # BGE 모델은 [CLS] 토큰이 필요하므로, 디코딩 후 다시 encode 함수에 넣습니다.\n",
        "    text_chunks = [tokenizer.decode(chunk) for chunk in token_chunks if chunk]\n",
        "\n",
        "    return text_chunks\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_embeddings(texts, model, tokenizer, batch_size=32):\n",
        "    \"\"\"Batch 단위로 embedding 생성\"\"\"\n",
        "    embeddings = []\n",
        "\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "\n",
        "        # Tokenize (BGE 모델은 max_length=512)\n",
        "        encoded = tokenizer(batch, padding=True, truncation=True,\n",
        "                            max_length=512, return_tensors='pt')\n",
        "        encoded = {k: v.to(DEVICE) for k, v in encoded.items()}\n",
        "\n",
        "        # Generate embeddings\n",
        "        outputs = model(**encoded)\n",
        "\n",
        "        # CLS token embedding 사용 및 Normalize\n",
        "        batch_embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "        batch_embeddings = torch.nn.functional.normalize(batch_embeddings, p=2, dim=1)\n",
        "\n",
        "        embeddings.append(batch_embeddings.cpu().numpy())\n",
        "\n",
        "    if not embeddings:\n",
        "        return np.array([])\n",
        "\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "def mean_pool_embeddings(embeddings):\n",
        "    \"\"\"여러 chunk embeddings를 mean pooling\"\"\"\n",
        "    if embeddings.size == 0:\n",
        "        return None\n",
        "    return np.mean(embeddings, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qjoeqbALRRS"
      },
      "source": [
        "Cell 3: 이어달리기(Checkpoint) 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OSATpHmGLL9-"
      },
      "outputs": [],
      "source": [
        "# Checkpoint 확인\n",
        "processed_files = set()\n",
        "if CHECKPOINT_FILE.exists():\n",
        "    try:\n",
        "        with open(CHECKPOINT_FILE, 'r') as f:\n",
        "            checkpoint = json.load(f)\n",
        "            processed_files = set(checkpoint.get('processed_files', []))\n",
        "            logging.info(f\"Checkpoint found: {len(processed_files)} files already processed\")\n",
        "    except json.JSONDecodeError:\n",
        "        logging.warning(\"Checkpoint file is corrupted. Starting fresh.\")\n",
        "        processed_files = set()\n",
        "\n",
        "# 기존 저장된 데이터 로드\n",
        "embeddings_file = OUTPUT_DIR / \"embeddings.npy\"\n",
        "metadata_file = OUTPUT_DIR / \"metadata.jsonl\"\n",
        "\n",
        "if embeddings_file.exists() and metadata_file.exists():\n",
        "    logging.info(\"Loading existing data...\")\n",
        "    try:\n",
        "        existing_embeddings = np.load(embeddings_file)\n",
        "        existing_metadata = []\n",
        "        with open(metadata_file, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                existing_metadata.append(json.loads(line))\n",
        "\n",
        "        if len(existing_embeddings) == len(existing_metadata):\n",
        "            logging.info(f\"Loaded {len(existing_metadata)} existing entries\")\n",
        "        else:\n",
        "            logging.error(\"Data mismatch! embeddings and metadata have different lengths. Starting fresh.\")\n",
        "            existing_embeddings = None\n",
        "            existing_metadata = []\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading existing data: {e}. Starting fresh.\")\n",
        "        existing_embeddings = None\n",
        "        existing_metadata = []\n",
        "else:\n",
        "    logging.info(\"Starting fresh (no existing data found)\")\n",
        "    existing_embeddings = None\n",
        "    existing_metadata = []\n",
        "\n",
        "# 모든 .jsonl 파일 수집 (처리되지 않은 파일만)\n",
        "jsonl_files = sorted([f for f in DATA_DIR.glob(\"*.jsonl\") if f.name not in processed_files])\n",
        "logging.info(f\"Total files to process: {len(jsonl_files)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8OYQbNELWS6"
      },
      "source": [
        "Cell 4: 메인 루프 (데이터 처리 및 저장)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iRfMBBpRf-9",
        "outputId": "f2dfb112-441e-43b0-bb93-325f9fe01a88"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (514 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " 파일 열기: alex_morgan.jsonl\n",
            "\n",
            "[1/5] 기사 처리 중: uk-news/2017/dec/31/new-years-eve-celebrations-to-go-ahead-despite-storm-dylan\n",
            "   - 본문 미리보기: New year celebrations are being prepared as planned across the UK, despite Storm Dylan bearing down on the country bringing 80mph winds and the Met Office issuing four weather warnings. Organisers sai...\n",
            "   - 생성된 Chunk 개수: 2\n",
            "   - 임베딩 성공! 벡터 모양(Shape): (1024,)\n",
            "   - 벡터 값 예시: [-0.03895908  0.01207704  0.00474441 -0.01309364 -0.02077837] ...\n",
            "\n",
            "[2/5] 기사 처리 중: sport/2017/dec/31/alastair-cook-david-warner-test-maestros-ashes-jason-gillespie\n",
            "   - 본문 미리보기: Cricket is a sport of wonderful contrasts and during the Melbourne Test we saw the beauty of this in Alastair Cook and David Warner. Here are two champions who are performing the same job for their te...\n",
            "   - 생성된 Chunk 개수: 3\n",
            "   - 임베딩 성공! 벡터 모양(Shape): (1024,)\n",
            "   - 벡터 값 예시: [ 0.03043569 -0.0053199  -0.01531123  0.00610584 -0.01735093] ...\n",
            "\n",
            "[3/5] 기사 처리 중: world/2017/dec/31/eight-big-ideas-for-2018-politics-culture-space-science-food\n",
            "   - 본문 미리보기: Art: Stefan Kalmar AN AGE OF CRISIS: WHAT A GREAT OPPORTUNITY… 2018 is all about reclaiming reality, opposing governmental and corporate paradoxes, and dissecting lies, before they become a new truth,...\n",
            "   - 생성된 Chunk 개수: 8\n",
            "   - 임베딩 성공! 벡터 모양(Shape): (1024,)\n",
            "   - 벡터 값 예시: [ 0.01615938 -0.016415   -0.0035248   0.01425068  0.00447398] ...\n",
            "\n",
            "[4/5] 기사 처리 중: world/2017/dec/31/look-to-the-future-what-does-2018-have-in-store\n",
            "   - 본문 미리보기: International affairs Iran nuclear deal, 15 January We should know by mid-January how serious Donald Trump is about seeking a confrontation with Iran over its nuclear programme. The US president refus...\n",
            "   - 생성된 Chunk 개수: 10\n",
            "   - 임베딩 성공! 벡터 모양(Shape): (1024,)\n",
            "   - 벡터 값 예시: [ 0.01615087 -0.02048843 -0.01182835  0.02578254 -0.02698717] ...\n",
            "\n",
            "[5/5] 기사 처리 중: sport/2017/dec/30/london-irish-newcastle-premiership-match-report\n",
            "   - 본문 미리보기: When you are down and in need of luck, fate usually spits in your face. London Irish, without a win here in the Premiership since March 2016, were 6-3 ahead after 22 minutes and looking reasonably com...\n",
            "   - 생성된 Chunk 개수: 3\n",
            "   - 임베딩 성공! 벡터 모양(Shape): (1024,)\n",
            "   - 벡터 값 예시: [ 0.02673532  0.01635421  0.01157774  0.04568122 -0.04786739] ...\n",
            "\n",
            "==================================================\n",
            "테스트 완료! 5개의 기사가 정상적으로 임베딩 되었습니다.\n",
            "이제 이 코드는 지우고, 원래의 [Cell 4]를 실행해서 전체를 돌리셔도 됩니다.\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# === 테스트용 Main Loop (기사 5개만 처리하고 멈춤) ===\n",
        "\n",
        "logging.info(\"테스트 모드를 시작합니다. (기사 5개만 처리)\")\n",
        "\n",
        "test_count = 0\n",
        "target_count = 5  # 테스트할 기사 개수\n",
        "\n",
        "# 전체 파일 중 '첫 번째 파일'만 가져옵니다.\n",
        "test_files = jsonl_files[:1]\n",
        "\n",
        "for file_path in test_files:\n",
        "    person = extract_person_name(file_path)\n",
        "    print(f\"\\n 파일 열기: {file_path.name}\")\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            # 파일 전체를 읽지 않고 한 줄씩 읽으며 테스트\n",
        "            for line in f:\n",
        "                if test_count >= target_count:\n",
        "                    break  # 5개 채우면 중단\n",
        "\n",
        "                article = json.loads(line)\n",
        "                body_text = article.get('bodyText', '')[:200] + \"...\" # 로그용으로 앞만 자름\n",
        "                article_id = article.get('id')\n",
        "\n",
        "                print(f\"\\n[{test_count + 1}/{target_count}] 기사 처리 중: {article_id}\")\n",
        "                print(f\"   - 본문 미리보기: {body_text}\")\n",
        "\n",
        "                # 1. 토큰 분할 (Chunking)\n",
        "                text_chunks = create_token_chunks(article.get('bodyText', ''), tokenizer, CHUNK_LENGTH, OVERLAP)\n",
        "                print(f\"   - 생성된 Chunk 개수: {len(text_chunks)}\")\n",
        "\n",
        "                if not text_chunks:\n",
        "                    print(\"   - Chunk 생성 실패 (본문이 너무 짧거나 없음)\")\n",
        "                    continue\n",
        "\n",
        "                # 2. 임베딩 생성\n",
        "                chunk_embeddings = generate_embeddings(text_chunks, model, tokenizer, batch_size=BATCH_SIZE)\n",
        "\n",
        "                # 3. 평균(Pooling)\n",
        "                final_vector = mean_pool_embeddings(chunk_embeddings)\n",
        "\n",
        "                # 결과 확인\n",
        "                if final_vector is not None:\n",
        "                    print(f\"   - 임베딩 성공! 벡터 모양(Shape): {final_vector.shape}\")\n",
        "                    # 벡터의 앞부분 5개만 살짝 출력해서 숫자가 잘 찼는지 확인\n",
        "                    print(f\"   - 벡터 값 예시: {final_vector[:5]} ...\")\n",
        "                    test_count += 1\n",
        "                else:\n",
        "                    print(\"   - 임베딩 실패\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"테스트 중 오류 발생: {e}\")\n",
        "        break\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "if test_count > 0:\n",
        "    print(f\"테스트 완료! {test_count}개의 기사가 정상적으로 임베딩 되었습니다.\")\n",
        "    print(\"이제 이 코드는 지우고, 원래의 [Cell 4]를 실행해서 전체를 돌리셔도 됩니다.\")\n",
        "else:\n",
        "    print(\"테스트 실패. 데이터를 읽지 못했거나 오류가 발생했습니다.\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMZ7OKbJSh5x",
        "outputId": "0628548b-fab1-4b01-c5f9-66f5e5eb75e5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rOverall Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "# === [최종 Main Cell 4] 전체 데이터 실행 ===\n",
        "\n",
        "logging.info(\"전체 데이터 임베딩 작업을 시작합니다.\")\n",
        "\n",
        "# 데이터 수집 및 embedding 생성 (인물별로 처리 및 즉시 저장)\n",
        "for idx, file_path in enumerate(tqdm(jsonl_files, desc=\"Overall Progress\")):\n",
        "    person = extract_person_name(file_path)\n",
        "\n",
        "    # 파일에서 기사 읽기\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            articles = [json.loads(line) for line in f]\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to read {file_path}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # 현재 인물의 기사별 처리\n",
        "    person_embeddings = []\n",
        "    person_metadata = []\n",
        "\n",
        "    for article in articles:\n",
        "        body_text = article.get('bodyText', '')\n",
        "        article_id = article.get('id')\n",
        "        pub_date_raw = article.get('webPublicationDate')\n",
        "\n",
        "        if not all([body_text, article_id, pub_date_raw]):\n",
        "            continue # 필수 정보 누락\n",
        "\n",
        "        # 1. 토큰 분할 (Chunking)\n",
        "        text_chunks = create_token_chunks(body_text, tokenizer, CHUNK_LENGTH, OVERLAP)\n",
        "        if not text_chunks:\n",
        "            continue\n",
        "\n",
        "        # 2. 임베딩 생성 (Batch 처리)\n",
        "        chunk_embeddings = generate_embeddings(text_chunks, model, tokenizer, batch_size=BATCH_SIZE)\n",
        "\n",
        "        # 3. 평균(Pooling)\n",
        "        article_embedding = mean_pool_embeddings(chunk_embeddings)\n",
        "\n",
        "        if article_embedding is not None:\n",
        "            person_embeddings.append(article_embedding)\n",
        "            person_metadata.append({\n",
        "                'person': person,\n",
        "                'article_id': article_id,\n",
        "                'pub_date': parse_pub_date(pub_date_raw)\n",
        "            })\n",
        "\n",
        "    # --- 한 인물 처리가 끝나면 즉시 파일에 추가 (Append) ---\n",
        "    if person_embeddings:\n",
        "        person_embeddings_array = np.array(person_embeddings)\n",
        "\n",
        "        # 기존 데이터와 합치기\n",
        "        if existing_embeddings is not None:\n",
        "            combined_embeddings = np.vstack([existing_embeddings, person_embeddings_array])\n",
        "        else:\n",
        "            combined_embeddings = person_embeddings_array\n",
        "\n",
        "        combined_metadata = existing_metadata + person_metadata\n",
        "\n",
        "        # 즉시 저장 (덮어쓰기)\n",
        "        try:\n",
        "            np.save(embeddings_file, combined_embeddings)\n",
        "            with open(metadata_file, 'w', encoding='utf-8') as f:\n",
        "                for meta in combined_metadata:\n",
        "                    f.write(json.dumps(meta, ensure_ascii=False) + '\\n')\n",
        "\n",
        "            # 다음 반복을 위해 업데이트\n",
        "            existing_embeddings = combined_embeddings\n",
        "            existing_metadata = combined_metadata\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to save data for {person}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Checkpoint 업데이트 (저장 성공 시)\n",
        "    processed_files.add(file_path.name)\n",
        "    with open(CHECKPOINT_FILE, 'w') as f:\n",
        "        json.dump({'processed_files': list(processed_files)}, f)\n",
        "\n",
        "# --- 모든 작업 완료 ---\n",
        "logging.info(f\"\\n All processing complete!\")\n",
        "if existing_embeddings is not None:\n",
        "    logging.info(f\"Final embeddings shape: {existing_embeddings.shape}\")\n",
        "    logging.info(f\"Total articles: {len(existing_metadata)}\")\n",
        "\n",
        "# Checkpoint 삭제\n",
        "if CHECKPOINT_FILE.exists():\n",
        "    CHECKPOINT_FILE.unlink()\n",
        "    logging.info(\"Checkpoint file removed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 혹시 속도 안 나오면 Cell 2의 함수를 이걸로 교체해서 FP16 적용.\n",
        "\n",
        "# @torch.no_grad()\n",
        "# def generate_embeddings(texts, model, tokenizer, batch_size=32):\n",
        "#     embeddings = []\n",
        "#     for i in range(0, len(texts), batch_size):\n",
        "#         batch = texts[i:i+batch_size]\n",
        "#         encoded = tokenizer(batch, padding=True, truncation=True, \n",
        "#                             max_length=512, return_tensors='pt')\n",
        "#         encoded = {k: v.to(DEVICE) for k, v in encoded.items()}\n",
        "        \n",
        "#         # FP16 적용 (속도 향상 핵심)\n",
        "#         with torch.cuda.amp.autocast():\n",
        "#             outputs = model(**encoded)\n",
        "#             batch_embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "#             batch_embeddings = torch.nn.functional.normalize(batch_embeddings, p=2, dim=1)\n",
        "            \n",
        "#         embeddings.append(batch_embeddings.cpu().numpy().astype(np.float32))\n",
        "    \n",
        "#     if not embeddings: return np.array([])\n",
        "#     return np.vstack(embeddings)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
