{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uz028ATXMbXM",
        "outputId": "34df8a0b-7953-45d8-8436-34cac45f66a2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhzPdzxXMf3O",
        "outputId": "87f8f2ae-0f3b-45b3-c749-c5f64e2bfbb3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip -q -o \"/content/drive/My Drive/ML_team_project_final/guardian_top100_scraping.zip\" -d \"/content/\""
      ],
      "metadata": {
        "id": "45ztn7h9Mi8Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 1: 라이브러리 임포트 및 기본 설정"
      ],
      "metadata": {
        "id": "nMF12ZaULeMn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1LWkdSsK65k",
        "outputId": "69f48077-e024-4ad2-b8d5-75ab8029e382"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "\n",
        "# 로깅 설정\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# 0. GPU 설정 (사용 가능한 GPU 번호로 변경하세요)\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logging.info(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# 1. 설정\n",
        "MODEL_NAME = \"BAAI/bge-large-en-v1.5\" # Headine/Chunking과 동일\n",
        "DATA_DIR = Path(\"guardian_top100_scraping\")\n",
        "OUTPUT_DIR = Path(\"vector_bodyText\")\n",
        "BATCH_SIZE = 32\n",
        "CHECKPOINT_FILE = OUTPUT_DIR / \"checkpoint.json\"\n",
        "\n",
        "# 2. BodyText Chunking 설정\n",
        "# (512 - 2 [CLS, SEP])\n",
        "CHUNK_LENGTH = 510\n",
        "# 토큰 기준 50개 겹치기\n",
        "OVERLAP = 50\n",
        "\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# 3. Model과 Tokenizer 로드\n",
        "logging.info(\"Loading model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModel.from_pretrained(MODEL_NAME)\n",
        "model = model.to(DEVICE)\n",
        "model.eval()\n",
        "logging.info(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 2: 헬퍼 함수 정의 (핵심 로직)"
      ],
      "metadata": {
        "id": "bw6BnysiLMPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_person_name(filename):\n",
        "    \"\"\"파일명에서 person 이름 추출 (예: alex_morgan.jsonl -> alex_morgan)\"\"\"\n",
        "    return filename.stem\n",
        "\n",
        "def parse_pub_date(web_pub_date):\n",
        "    \"\"\"webPublicationDate를 YYYY_MM_DD 형식으로 변환\"\"\"\n",
        "    try:\n",
        "        dt = datetime.fromisoformat(web_pub_date.replace('Z', '+00:00'))\n",
        "        return dt.strftime(\"%Y_%m_%d\")\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def create_token_chunks(text, tokenizer, chunk_length=510, overlap=50):\n",
        "    \"\"\"\n",
        "    긴 텍스트를 토큰 기준으로 'chunk_length'만큼 자르고 'overlap'만큼 겹치게 합니다.\n",
        "    Sentence-Transformer의 방식(토큰화 -> 디코딩)을 차용합니다.\n",
        "    \"\"\"\n",
        "    if not text or text.strip() == '':\n",
        "        return []\n",
        "\n",
        "    # 1. 전체 텍스트를 한 번에 토큰화 (특수 토큰 제외)\n",
        "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    if not tokens:\n",
        "        return []\n",
        "\n",
        "    # 2. 토큰 리스트를 기준으로 겹치는 윈도우 생성\n",
        "    step = chunk_length - overlap\n",
        "    token_chunks = []\n",
        "\n",
        "    for i in range(0, len(tokens), step):\n",
        "        chunk = tokens[i:i + chunk_length]\n",
        "        token_chunks.append(chunk)\n",
        "\n",
        "        # 마지막 chunk가 전체 길이를 넘어서면 중단\n",
        "        if i + chunk_length >= len(tokens):\n",
        "            break\n",
        "\n",
        "    # 3. 토큰 chunk들을 다시 텍스트(string)로 디코딩\n",
        "    # BGE 모델은 [CLS] 토큰이 필요하므로, 디코딩 후 다시 encode 함수에 넣습니다.\n",
        "    text_chunks = [tokenizer.decode(chunk) for chunk in token_chunks if chunk]\n",
        "\n",
        "    return text_chunks\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_embeddings(texts, model, tokenizer, batch_size=32):\n",
        "    \"\"\"Batch 단위로 embedding 생성\"\"\"\n",
        "    embeddings = []\n",
        "\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "\n",
        "        # Tokenize (BGE 모델은 max_length=512)\n",
        "        encoded = tokenizer(batch, padding=True, truncation=True,\n",
        "                            max_length=512, return_tensors='pt')\n",
        "        encoded = {k: v.to(DEVICE) for k, v in encoded.items()}\n",
        "\n",
        "        # Generate embeddings\n",
        "        outputs = model(**encoded)\n",
        "\n",
        "        # CLS token embedding 사용 및 Normalize\n",
        "        batch_embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "        batch_embeddings = torch.nn.functional.normalize(batch_embeddings, p=2, dim=1)\n",
        "\n",
        "        embeddings.append(batch_embeddings.cpu().numpy())\n",
        "\n",
        "    if not embeddings:\n",
        "        return np.array([])\n",
        "\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "def mean_pool_embeddings(embeddings):\n",
        "    \"\"\"여러 chunk embeddings를 mean pooling\"\"\"\n",
        "    if embeddings.size == 0:\n",
        "        return None\n",
        "    return np.mean(embeddings, axis=0)"
      ],
      "metadata": {
        "id": "7eWHTwflLJ0N"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 3: 이어달리기(Checkpoint) 설정"
      ],
      "metadata": {
        "id": "0qjoeqbALRRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checkpoint 확인\n",
        "processed_files = set()\n",
        "if CHECKPOINT_FILE.exists():\n",
        "    try:\n",
        "        with open(CHECKPOINT_FILE, 'r') as f:\n",
        "            checkpoint = json.load(f)\n",
        "            processed_files = set(checkpoint.get('processed_files', []))\n",
        "            logging.info(f\"Checkpoint found: {len(processed_files)} files already processed\")\n",
        "    except json.JSONDecodeError:\n",
        "        logging.warning(\"Checkpoint file is corrupted. Starting fresh.\")\n",
        "        processed_files = set()\n",
        "\n",
        "# 기존 저장된 데이터 로드\n",
        "embeddings_file = OUTPUT_DIR / \"embeddings.npy\"\n",
        "metadata_file = OUTPUT_DIR / \"metadata.jsonl\"\n",
        "\n",
        "if embeddings_file.exists() and metadata_file.exists():\n",
        "    logging.info(\"Loading existing data...\")\n",
        "    try:\n",
        "        existing_embeddings = np.load(embeddings_file)\n",
        "        existing_metadata = []\n",
        "        with open(metadata_file, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                existing_metadata.append(json.loads(line))\n",
        "\n",
        "        if len(existing_embeddings) == len(existing_metadata):\n",
        "            logging.info(f\"Loaded {len(existing_metadata)} existing entries\")\n",
        "        else:\n",
        "            logging.error(\"Data mismatch! embeddings and metadata have different lengths. Starting fresh.\")\n",
        "            existing_embeddings = None\n",
        "            existing_metadata = []\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading existing data: {e}. Starting fresh.\")\n",
        "        existing_embeddings = None\n",
        "        existing_metadata = []\n",
        "else:\n",
        "    logging.info(\"Starting fresh (no existing data found)\")\n",
        "    existing_embeddings = None\n",
        "    existing_metadata = []\n",
        "\n",
        "# 모든 .jsonl 파일 수집 (처리되지 않은 파일만)\n",
        "jsonl_files = sorted([f for f in DATA_DIR.glob(\"*.jsonl\") if f.name not in processed_files])\n",
        "logging.info(f\"Total files to process: {len(jsonl_files)}\")"
      ],
      "metadata": {
        "id": "OSATpHmGLL9-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 4: 메인 루프 (데이터 처리 및 저장)"
      ],
      "metadata": {
        "id": "o8OYQbNELWS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === [최종 Main Cell 4] 전체 데이터 실행 ===\n",
        "\n",
        "logging.info(\"전체 데이터 임베딩 작업을 시작합니다.\")\n",
        "\n",
        "# 데이터 수집 및 embedding 생성 (인물별로 처리 및 즉시 저장)\n",
        "for idx, file_path in enumerate(tqdm(jsonl_files, desc=\"Overall Progress\")):\n",
        "    person = extract_person_name(file_path)\n",
        "\n",
        "    # 파일에서 기사 읽기\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            articles = [json.loads(line) for line in f]\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to read {file_path}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # 현재 인물의 기사별 처리\n",
        "    person_embeddings = []\n",
        "    person_metadata = []\n",
        "\n",
        "    for article in articles:\n",
        "        body_text = article.get('bodyText', '')\n",
        "        article_id = article.get('id')\n",
        "        pub_date_raw = article.get('webPublicationDate')\n",
        "\n",
        "        if not all([body_text, article_id, pub_date_raw]):\n",
        "            continue # 필수 정보 누락\n",
        "\n",
        "        # 1. 토큰 분할 (Chunking)\n",
        "        text_chunks = create_token_chunks(body_text, tokenizer, CHUNK_LENGTH, OVERLAP)\n",
        "        if not text_chunks:\n",
        "            continue\n",
        "\n",
        "        # 2. 임베딩 생성 (Batch 처리)\n",
        "        chunk_embeddings = generate_embeddings(text_chunks, model, tokenizer, batch_size=BATCH_SIZE)\n",
        "\n",
        "        # 3. 평균(Pooling)\n",
        "        article_embedding = mean_pool_embeddings(chunk_embeddings)\n",
        "\n",
        "        if article_embedding is not None:\n",
        "            person_embeddings.append(article_embedding)\n",
        "            person_metadata.append({\n",
        "                'person': person,\n",
        "                'article_id': article_id,\n",
        "                'pub_date': parse_pub_date(pub_date_raw)\n",
        "            })\n",
        "\n",
        "    # --- 한 인물 처리가 끝나면 즉시 파일에 추가 (Append) ---\n",
        "    if person_embeddings:\n",
        "        person_embeddings_array = np.array(person_embeddings)\n",
        "\n",
        "        # 기존 데이터와 합치기\n",
        "        if existing_embeddings is not None:\n",
        "            combined_embeddings = np.vstack([existing_embeddings, person_embeddings_array])\n",
        "        else:\n",
        "            combined_embeddings = person_embeddings_array\n",
        "\n",
        "        combined_metadata = existing_metadata + person_metadata\n",
        "\n",
        "        # 즉시 저장 (덮어쓰기)\n",
        "        try:\n",
        "            np.save(embeddings_file, combined_embeddings)\n",
        "            with open(metadata_file, 'w', encoding='utf-8') as f:\n",
        "                for meta in combined_metadata:\n",
        "                    f.write(json.dumps(meta, ensure_ascii=False) + '\\n')\n",
        "\n",
        "            # 다음 반복을 위해 업데이트\n",
        "            existing_embeddings = combined_embeddings\n",
        "            existing_metadata = combined_metadata\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to save data for {person}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Checkpoint 업데이트 (저장 성공 시)\n",
        "    processed_files.add(file_path.name)\n",
        "    with open(CHECKPOINT_FILE, 'w') as f:\n",
        "        json.dump({'processed_files': list(processed_files)}, f)\n",
        "\n",
        "# --- 모든 작업 완료 ---\n",
        "logging.info(f\"\\n All processing complete!\")\n",
        "if existing_embeddings is not None:\n",
        "    logging.info(f\"Final embeddings shape: {existing_embeddings.shape}\")\n",
        "    logging.info(f\"Total articles: {len(existing_metadata)}\")\n",
        "\n",
        "# Checkpoint 삭제\n",
        "if CHECKPOINT_FILE.exists():\n",
        "    CHECKPOINT_FILE.unlink()\n",
        "    logging.info(\"Checkpoint file removed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMZ7OKbJSh5x",
        "outputId": "0628548b-fab1-4b01-c5f9-66f5e5eb75e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rOverall Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        }
      ]
    }
  ]
}