{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "uz028ATXMbXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers tqdm"
      ],
      "metadata": {
        "id": "WhzPdzxXMf3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip -q -o \"/content/drive/My Drive/ML_team_project_final/guardian_top100_scraping.zip\" -d \"/content/\""
      ],
      "metadata": {
        "id": "45ztn7h9Mi8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 1: 라이브러리 임포트 및 기본 설정"
      ],
      "metadata": {
        "id": "nMF12ZaULeMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [Cell 1] 설정 변경\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "\n",
        "# 로깅 설정\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# 0. GPU 설정\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logging.info(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# 1. 설정 (Jina v3 적용)\n",
        "MODEL_NAME = \"jinaai/jina-embeddings-v3\"\n",
        "\n",
        "# === [로컬 환경 호환 경로] ===\n",
        "DATA_DIR = Path(\"guardian_top100_scraping\")\n",
        "OUTPUT_DIR = Path(\"vector_bodyText\")\n",
        "\n",
        "# [중요] Jina v3는 모델이 크고 입력이 길어서 배치를 줄여야 합니다 (T4 GPU 기준)\n",
        "BATCH_SIZE = 1\n",
        "CHECKPOINT_FILE = OUTPUT_DIR / \"checkpoint.json\"\n",
        "\n",
        "# 2. BodyText Chunking 설정 (Jina v3는 8192까지 가능)\n",
        "# 8000으로 설정하면 뉴스 기사 99%는 안 잘리고 통째로 들어갑니다.\n",
        "CHUNK_LENGTH = 8192\n",
        "OVERLAP = 100\n",
        "\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# 3. Model과 Tokenizer 로드 (trust_remote_code=True 필수!)\n",
        "logging.info(\"Loading model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "model = AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "model = model.to(DEVICE)\n",
        "model.eval()\n",
        "logging.info(\"Model loaded successfully!\")"
      ],
      "metadata": {
        "id": "3Uo9HxlKVRdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 2: 헬퍼 함수 정의 (핵심 로직)"
      ],
      "metadata": {
        "id": "bw6BnysiLMPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [Cell 2] Helper Functions (Jina v3 Mean Pooling 적용)\n",
        "\n",
        "def extract_person_name(filename):\n",
        "    return filename.stem\n",
        "\n",
        "def parse_pub_date(web_pub_date):\n",
        "    try:\n",
        "        dt = datetime.fromisoformat(web_pub_date.replace('Z', '+00:00'))\n",
        "        return dt.strftime(\"%Y_%m_%d\")\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def create_token_chunks(text, tokenizer, chunk_length=8000, overlap=100):\n",
        "    if not text or text.strip() == '':\n",
        "        return []\n",
        "\n",
        "    # Jina tokenizer 사용 시 special token 처리\n",
        "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "    if not tokens:\n",
        "        return []\n",
        "\n",
        "    step = chunk_length - overlap\n",
        "    token_chunks = []\n",
        "\n",
        "    for i in range(0, len(tokens), step):\n",
        "        chunk = tokens[i:i + chunk_length]\n",
        "        token_chunks.append(chunk)\n",
        "        if i + chunk_length >= len(tokens):\n",
        "            break\n",
        "\n",
        "    text_chunks = [tokenizer.decode(chunk) for chunk in token_chunks if chunk]\n",
        "    return text_chunks\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_embeddings(texts, model, tokenizer, batch_size=4):\n",
        "    \"\"\"Batch 단위로 embedding 생성 (Jina v3: Mean Pooling + FP16)\"\"\"\n",
        "    embeddings = []\n",
        "\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "\n",
        "        # Jina v3는 8192까지 가능\n",
        "        encoded = tokenizer(batch, padding=True, truncation=True,\n",
        "                            max_length=8192, return_tensors='pt')\n",
        "        encoded = {k: v.to(DEVICE) for k, v in encoded.items()}\n",
        "\n",
        "        # FP16 적용\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            outputs = model(**encoded)\n",
        "\n",
        "            # === [변경] Mean Pooling ===\n",
        "            # attention_mask를 고려하여 평균을 구합니다.\n",
        "            input_mask_expanded = encoded['attention_mask'].unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()\n",
        "            sum_embeddings = torch.sum(outputs.last_hidden_state * input_mask_expanded, 1)\n",
        "            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "            batch_embeddings = sum_embeddings / sum_mask\n",
        "\n",
        "            # Normalize\n",
        "            batch_embeddings = torch.nn.functional.normalize(batch_embeddings, p=2, dim=1)\n",
        "\n",
        "        embeddings.append(batch_embeddings.cpu().numpy().astype(np.float32))\n",
        "\n",
        "    if not embeddings:\n",
        "        return np.array([])\n",
        "\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "def mean_pool_embeddings(embeddings):\n",
        "    if embeddings.size == 0:\n",
        "        return None\n",
        "    return np.mean(embeddings, axis=0)"
      ],
      "metadata": {
        "id": "OFmtkP5_VxFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 3: 이어달리기(Checkpoint) 설정"
      ],
      "metadata": {
        "id": "0qjoeqbALRRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checkpoint 확인\n",
        "processed_files = set()\n",
        "if CHECKPOINT_FILE.exists():\n",
        "    try:\n",
        "        with open(CHECKPOINT_FILE, 'r') as f:\n",
        "            checkpoint = json.load(f)\n",
        "            processed_files = set(checkpoint.get('processed_files', []))\n",
        "            logging.info(f\"Checkpoint found: {len(processed_files)} files already processed\")\n",
        "    except json.JSONDecodeError:\n",
        "        logging.warning(\"Checkpoint file is corrupted. Starting fresh.\")\n",
        "        processed_files = set()\n",
        "\n",
        "# 기존 저장된 데이터 로드\n",
        "embeddings_file = OUTPUT_DIR / \"embeddings.npy\"\n",
        "metadata_file = OUTPUT_DIR / \"metadata.jsonl\"\n",
        "\n",
        "if embeddings_file.exists() and metadata_file.exists():\n",
        "    logging.info(\"Loading existing data...\")\n",
        "    try:\n",
        "        existing_embeddings = np.load(embeddings_file)\n",
        "        existing_metadata = []\n",
        "        with open(metadata_file, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                existing_metadata.append(json.loads(line))\n",
        "\n",
        "        if len(existing_embeddings) == len(existing_metadata):\n",
        "            logging.info(f\"Loaded {len(existing_metadata)} existing entries\")\n",
        "        else:\n",
        "            logging.error(\"Data mismatch! embeddings and metadata have different lengths. Starting fresh.\")\n",
        "            existing_embeddings = None\n",
        "            existing_metadata = []\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading existing data: {e}. Starting fresh.\")\n",
        "        existing_embeddings = None\n",
        "        existing_metadata = []\n",
        "else:\n",
        "    logging.info(\"Starting fresh (no existing data found)\")\n",
        "    existing_embeddings = None\n",
        "    existing_metadata = []\n",
        "\n",
        "# 모든 .jsonl 파일 수집 (처리되지 않은 파일만)\n",
        "jsonl_files = sorted([f for f in DATA_DIR.glob(\"*.jsonl\") if f.name not in processed_files])\n",
        "logging.info(f\"Total files to process: {len(jsonl_files)}\")"
      ],
      "metadata": {
        "id": "OSATpHmGLL9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 4: 메인 루프 (데이터 처리 및 저장)"
      ],
      "metadata": {
        "id": "o8OYQbNELWS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === [최종 Main Cell 4] 전체 데이터 실행 ===\n",
        "\n",
        "logging.info(\"전체 데이터 임베딩 작업을 시작합니다.\")\n",
        "\n",
        "# 데이터 수집 및 embedding 생성 (인물별로 처리 및 즉시 저장)\n",
        "for idx, file_path in enumerate(tqdm(jsonl_files, desc=\"Overall Progress\")):\n",
        "    person = extract_person_name(file_path)\n",
        "\n",
        "    # 파일에서 기사 읽기\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            articles = [json.loads(line) for line in f]\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to read {file_path}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # 현재 인물의 기사별 처리\n",
        "    person_embeddings = []\n",
        "    person_metadata = []\n",
        "\n",
        "    for article in articles:\n",
        "        body_text = article.get('bodyText', '')\n",
        "        article_id = article.get('id')\n",
        "        pub_date_raw = article.get('webPublicationDate')\n",
        "\n",
        "        if not all([body_text, article_id, pub_date_raw]):\n",
        "            continue # 필수 정보 누락\n",
        "\n",
        "        # 1. 토큰 분할 (Chunking)\n",
        "        text_chunks = create_token_chunks(body_text, tokenizer, CHUNK_LENGTH, OVERLAP)\n",
        "        if not text_chunks:\n",
        "            continue\n",
        "\n",
        "        # 2. 임베딩 생성 (Batch 처리)\n",
        "        chunk_embeddings = generate_embeddings(text_chunks, model, tokenizer, batch_size=BATCH_SIZE)\n",
        "\n",
        "        # 3. 평균(Pooling)\n",
        "        article_embedding = mean_pool_embeddings(chunk_embeddings)\n",
        "\n",
        "        if article_embedding is not None:\n",
        "            person_embeddings.append(article_embedding)\n",
        "            person_metadata.append({\n",
        "                'person': person,\n",
        "                'article_id': article_id,\n",
        "                'pub_date': parse_pub_date(pub_date_raw)\n",
        "            })\n",
        "\n",
        "    # --- 한 인물 처리가 끝나면 즉시 파일에 추가 (Append) ---\n",
        "    if person_embeddings:\n",
        "        person_embeddings_array = np.array(person_embeddings)\n",
        "\n",
        "        # 기존 데이터와 합치기\n",
        "        if existing_embeddings is not None:\n",
        "            combined_embeddings = np.vstack([existing_embeddings, person_embeddings_array])\n",
        "        else:\n",
        "            combined_embeddings = person_embeddings_array\n",
        "\n",
        "        combined_metadata = existing_metadata + person_metadata\n",
        "\n",
        "        # 즉시 저장 (덮어쓰기)\n",
        "        try:\n",
        "            np.save(embeddings_file, combined_embeddings)\n",
        "            with open(metadata_file, 'w', encoding='utf-8') as f:\n",
        "                for meta in combined_metadata:\n",
        "                    f.write(json.dumps(meta, ensure_ascii=False) + '\\n')\n",
        "\n",
        "            # 다음 반복을 위해 업데이트\n",
        "            existing_embeddings = combined_embeddings\n",
        "            existing_metadata = combined_metadata\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to save data for {person}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Checkpoint 업데이트 (저장 성공 시)\n",
        "    processed_files.add(file_path.name)\n",
        "    with open(CHECKPOINT_FILE, 'w') as f:\n",
        "        json.dump({'processed_files': list(processed_files)}, f)\n",
        "\n",
        "# --- 모든 작업 완료 ---\n",
        "logging.info(f\"\\n All processing complete!\")\n",
        "if existing_embeddings is not None:\n",
        "    logging.info(f\"Final embeddings shape: {existing_embeddings.shape}\")\n",
        "    logging.info(f\"Total articles: {len(existing_metadata)}\")\n",
        "\n",
        "# Checkpoint 삭제\n",
        "if CHECKPOINT_FILE.exists():\n",
        "    CHECKPOINT_FILE.unlink()\n",
        "    logging.info(\"Checkpoint file removed.\")"
      ],
      "metadata": {
        "id": "gMZ7OKbJSh5x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}