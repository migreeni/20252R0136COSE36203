{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e002adc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def count_valid_fields(data):\n",
    "    \"\"\"각 field가 valid한지 확인하고 count한다\"\"\"\n",
    "    counts = {\n",
    "        'total': 0,\n",
    "        'valid_headline': 0,\n",
    "        'valid_webPublicationDate': 0,\n",
    "        'valid_bodyText': 0\n",
    "    }\n",
    "    \n",
    "    for item in data:\n",
    "        counts['total'] += 1\n",
    "        \n",
    "        # headline이 존재하고 비어있지 않은지 확인\n",
    "        if 'headline' in item and item['headline'] and isinstance(item['headline'], str) and item['headline'].strip():\n",
    "            counts['valid_headline'] += 1\n",
    "        \n",
    "        # webPublicationDate가 존재하고 비어있지 않은지 확인\n",
    "        if 'webPublicationDate' in item and item['webPublicationDate'] and isinstance(item['webPublicationDate'], str) and item['webPublicationDate'].strip():\n",
    "            counts['valid_webPublicationDate'] += 1\n",
    "        \n",
    "        # bodyText가 존재하고 비어있지 않은지 확인\n",
    "        if 'bodyText' in item and item['bodyText'] and isinstance(item['bodyText'], str) and item['bodyText'].strip():\n",
    "            counts['valid_bodyText'] += 1\n",
    "    \n",
    "    return counts\n",
    "\n",
    "def count_metadata_by_person(metadata_path):\n",
    "    \"\"\"metadata.jsonl에서 각 person별로 원소 개수를 count한다\"\"\"\n",
    "    person_counts = defaultdict(int)\n",
    "    \n",
    "    if not os.path.exists(metadata_path):\n",
    "        return person_counts\n",
    "    \n",
    "    with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                item = json.loads(line)\n",
    "                person = item.get('person', 'unknown')\n",
    "                person_counts[person] += 1\n",
    "    \n",
    "    return dict(person_counts)\n",
    "\n",
    "def count_embeddings(npy_path):\n",
    "    \"\"\"embeddings.npy 파일에서 embedding 개수를 count한다\"\"\"\n",
    "    if not os.path.exists(npy_path):\n",
    "        return 0\n",
    "    \n",
    "    try:\n",
    "        embeddings = np.load(npy_path)\n",
    "        return embeddings.shape[0]  # 첫 번째 dimension이 embedding 개수\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {npy_path}: {e}\")\n",
    "        return 0\n",
    "\n",
    "def main():\n",
    "    # 결과를 저장할 list\n",
    "    results = []\n",
    "    \n",
    "    # guardian_top100_scraping 폴더 처리\n",
    "    scraping_dir = Path('guardian_top100_scraping')\n",
    "    \n",
    "    if scraping_dir.exists():\n",
    "        for jsonl_file in sorted(scraping_dir.glob('*.jsonl')):\n",
    "            person_name = jsonl_file.stem  # 확장자를 제외한 파일명\n",
    "            \n",
    "            # JSONL 파일 읽기\n",
    "            data = []\n",
    "            with open(jsonl_file, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    if line.strip():\n",
    "                        data.append(json.loads(line))\n",
    "            \n",
    "            # valid field count\n",
    "            counts = count_valid_fields(data)\n",
    "            \n",
    "            results.append({\n",
    "                'person': person_name,\n",
    "                'scraping_total': counts['total'],\n",
    "                'scraping_valid_headline': counts['valid_headline'],\n",
    "                'scraping_valid_webPublicationDate': counts['valid_webPublicationDate'],\n",
    "                'scraping_valid_bodyText': counts['valid_bodyText']\n",
    "            })\n",
    "    \n",
    "    # vector_headlines/metadata.jsonl 처리\n",
    "    headlines_counts = count_metadata_by_person('vector_headlines/metadata.jsonl')\n",
    "    \n",
    "    # vector_chunking/metadata.jsonl 처리\n",
    "    chunking_counts = count_metadata_by_person('vector_chunking/metadata.jsonl')\n",
    "    \n",
    "    # embeddings.npy 개수 확인\n",
    "    headlines_embeddings_count = count_embeddings('vector_headlines/embeddings.npy')\n",
    "    chunking_embeddings_count = count_embeddings('vector_chunking/embeddings.npy')\n",
    "    \n",
    "    # results에 vector counts 추가\n",
    "    # 모든 person name을 수집\n",
    "    all_persons = set()\n",
    "    for result in results:\n",
    "        all_persons.add(result['person'])\n",
    "    all_persons.update(headlines_counts.keys())\n",
    "    all_persons.update(chunking_counts.keys())\n",
    "    \n",
    "    # results를 dict로 변환하여 쉽게 접근\n",
    "    results_dict = {r['person']: r for r in results}\n",
    "    \n",
    "    # 최종 results list 생성\n",
    "    final_results = []\n",
    "    for person in sorted(all_persons):\n",
    "        row = results_dict.get(person, {\n",
    "            'person': person,\n",
    "            'scraping_total': 0,\n",
    "            'scraping_valid_headline': 0,\n",
    "            'scraping_valid_webPublicationDate': 0,\n",
    "            'scraping_valid_bodyText': 0\n",
    "        })\n",
    "        \n",
    "        row['vector_headlines_count'] = headlines_counts.get(person, 0)\n",
    "        row['vector_chunking_count'] = chunking_counts.get(person, 0)\n",
    "        \n",
    "        final_results.append(row)\n",
    "    \n",
    "    # DataFrame 생성 및 CSV 저장\n",
    "    df = pd.DataFrame(final_results)\n",
    "    \n",
    "    # column 순서 정리\n",
    "    column_order = [\n",
    "        'person',\n",
    "        'scraping_total',\n",
    "        'scraping_valid_headline',\n",
    "        'scraping_valid_webPublicationDate',\n",
    "        'scraping_valid_bodyText',\n",
    "        'vector_headlines_count',\n",
    "        'vector_chunking_count'\n",
    "    ]\n",
    "    \n",
    "    df = df[column_order]\n",
    "    \n",
    "    # CSV 파일로 저장\n",
    "    output_file = 'data_summary.csv'\n",
    "    df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"분석 완료! 결과가 '{output_file}'에 저장되었다.\")\n",
    "    print(f\"\\n총 {len(final_results)}명의 person data를 처리했다.\")\n",
    "    print(f\"\\nPreview:\")\n",
    "    print(df.head(10))\n",
    "    \n",
    "    # 통계 요약\n",
    "    print(f\"\\n=== 통계 요약 ===\")\n",
    "    print(f\"총 scraping articles: {df['scraping_total'].sum()}\")\n",
    "    print(f\"총 vector_headlines entries: {df['vector_headlines_count'].sum()}\")\n",
    "    print(f\"총 vector_chunking entries: {df['vector_chunking_count'].sum()}\")\n",
    "    \n",
    "    # embeddings.npy 정보 출력\n",
    "    print(f\"\\n=== Embeddings.npy 정보 ===\")\n",
    "    print(f\"vector_headlines/embeddings.npy: {headlines_embeddings_count}개의 embeddings\")\n",
    "    print(f\"vector_chunking/embeddings.npy: {chunking_embeddings_count}개의 embeddings\")\n",
    "    \n",
    "    # 일관성 확인\n",
    "    print(f\"\\n=== 일관성 확인 ===\")\n",
    "    headlines_metadata_total = df['vector_headlines_count'].sum()\n",
    "    chunking_metadata_total = df['vector_chunking_count'].sum()\n",
    "    \n",
    "    print(f\"vector_headlines - metadata entries vs embeddings: {headlines_metadata_total} vs {headlines_embeddings_count}\", end=\"\")\n",
    "    if headlines_metadata_total == headlines_embeddings_count:\n",
    "        print(\" ✓ 일치\")\n",
    "    else:\n",
    "        print(f\" ✗ 불일치 (차이: {abs(headlines_metadata_total - headlines_embeddings_count)})\")\n",
    "    \n",
    "    print(f\"vector_chunking - metadata entries vs embeddings: {chunking_metadata_total} vs {chunking_embeddings_count}\", end=\"\")\n",
    "    if chunking_metadata_total == chunking_embeddings_count:\n",
    "        print(\" ✓ 일치\")\n",
    "    else:\n",
    "        print(f\" ✗ 불일치 (차이: {abs(chunking_metadata_total - chunking_embeddings_count)})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
