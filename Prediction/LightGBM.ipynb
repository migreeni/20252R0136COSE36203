{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61059e98",
   "metadata": {},
   "source": [
    "LightGBM_Modeling (Modified Version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948a9cba",
   "metadata": {},
   "source": [
    "2. 라이브러리 & 기본 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebbd86ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Source: ../feature_datasets\n",
      "Output Path: results_lightgbm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import gc\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 경로 설정 (Linear Regression과 동일)\n",
    "DATA_DIR = Path(\"../feature_datasets\")\n",
    "OUTPUT_DIR = Path(\"results_lightgbm\")\n",
    "RESULTS_DIR = OUTPUT_DIR / \"results\"\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Data Source: {DATA_DIR}\")\n",
    "print(f\"Output Path: {OUTPUT_DIR}\")\n",
    "\n",
    "# 재현성용 seed\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5afd4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"Parquet 파일 로드\"\"\"\n",
    "    print(f\"Loading {file_path.name}...\")\n",
    "    df = pd.read_parquet(file_path)\n",
    "    \n",
    "    # 날짜순 정렬\n",
    "    if 'date_index' in df.columns:\n",
    "        df = df.sort_values('date_index').reset_index(drop=True)\n",
    "    \n",
    "    # pub_date → Date 변환\n",
    "    if 'pub_date' not in df.columns:\n",
    "        raise ValueError(f\"'pub_date' column not found in {file_path.name}\")\n",
    "    \n",
    "    df['Date'] = pd.to_datetime(df['pub_date'], format='%Y_%m_%d')\n",
    "    \n",
    "    print(f\"   Loaded {len(df)} rows, {len(df.columns)} columns\")\n",
    "    print(f\"   Date range: {df['Date'].min().strftime('%Y-%m-%d')} to {df['Date'].max().strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc917b09",
   "metadata": {},
   "source": [
    "3. 파일 이름 파싱 함수\n",
    "- 파일명에서 Dataset/Method/Type 정보를 추출해서 결과 CSV에 넣어주기.\n",
    "    - `dataset_A.parquet` → Dataset = 'A', Method=None, Type=None\n",
    "    - `dataset_D_headlines_orig.parquet` → Dataset='D', Method='headlines', Type='orig'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cca1969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dataset_filename(fname: str):\n",
    "    \"\"\"\n",
    "    feature_datasets 안의 파일명을 파싱해\n",
    "    Dataset / Method / Type 정보를 반환한다.\n",
    "    \"\"\"\n",
    "    name = fname.replace(\"dataset_\", \"\").replace(\".parquet\", \"\")\n",
    "    parts = name.split(\"_\")\n",
    "\n",
    "    # Case 1: dataset_A.parquet → ['A']\n",
    "    if len(parts) == 1:\n",
    "        return {\n",
    "            \"Dataset\": parts[0],   # 'A'\n",
    "            \"Method\": None,\n",
    "            \"Type\": None,\n",
    "        }\n",
    "\n",
    "    # Case 2: dataset_B_headlines_orig.parquet → ['B', 'headlines', 'orig']\n",
    "    if len(parts) == 3:\n",
    "        Dataset, Method, Type = parts\n",
    "        return {\n",
    "            \"Dataset\": Dataset,\n",
    "            \"Method\": Method,\n",
    "            \"Type\": Type,\n",
    "        }\n",
    "\n",
    "    # 예상 밖 패턴이 나오면 None\n",
    "    return {\n",
    "        \"Dataset\": None,\n",
    "        \"Method\": None,\n",
    "        \"Type\": None,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acce6e0",
   "metadata": {},
   "source": [
    "5. Feature 컬럼 자동 선택 함수\n",
    "\n",
    "- 이 함수의 핵심 목적:\n",
    "    - **data leakage를 일으키는 컬럼, 메타데이터**를 자동으로 제거\n",
    "    - lag, fg_lag, PCA, embedding, person one-hot 등 **실제 feature만 선택**\n",
    "- 규칙은 Dataset D의 전체 스키마를 기준으로 설계."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6281be4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_columns(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    DataFrame에서 '모델에 넣을 feature 컬럼'만 선택해서 리스트로 반환한다.\n",
    "\n",
    "    Drop 대상:\n",
    "      - value (target)\n",
    "      - 날짜 관련: date_str, Date, date_index, article_date, pub_date\n",
    "      - 텍스트 메타데이터: person(문자열), article_id, person_id, idx,\n",
    "                           headline, trailText, bodyText, webTitle, webUrl,\n",
    "                           apiUrl, wordcount\n",
    "      - fg_value (현재 시점 Fear-Greed 지표 → leakage)\n",
    "\n",
    "    Keep 대상(명시적):\n",
    "      - lag_*, fg_lag_*, fg_value? (← 여기서는 드랍), pca_*, emb_*, person_*\n",
    "\n",
    "    그 외 숫자형 컬럼은 기본적으로 feature로 허용.\n",
    "    \"\"\"\n",
    "\n",
    "    DROP_PATTERNS = [\n",
    "        r\"^value$\",\n",
    "        r\"^date_str$\", r\"^date$\", r\"^date_index$\",\n",
    "        r\"article_date\", r\"pub_date\",\n",
    "        r\"article_id\", r\"person_id\", r\"idx$\",\n",
    "        r\"headline\", r\"trailText\", r\"bodyText\",\n",
    "        r\"webUrl\", r\"apiUrl\", r\"webTitle\", r\"wordcount\",\n",
    "        r\"^person$\",     # 문자열 person (이름)\n",
    "        r\"^fg_value$\",   # 현재 시점 fear-greed 값 → leakage\n",
    "    ]\n",
    "\n",
    "    KEEP_PATTERNS = [\n",
    "        r\"^lag_\\d+$\",\n",
    "        r\"^fg_lag_\\d+$\",\n",
    "        r\"^pca_\\d+$\",\n",
    "        r\"^emb_\\d+$\",\n",
    "        r\"^person_\\d+$\",  # one-hot (person_1 ~ person_100)\n",
    "    ]\n",
    "\n",
    "    def match_any(col: str, patterns):\n",
    "        return any(re.search(p, col) for p in patterns)\n",
    "\n",
    "    feature_cols = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        # 1) Drop 대상이면 무조건 제외\n",
    "        if match_any(col, DROP_PATTERNS):\n",
    "            continue\n",
    "\n",
    "        # 2) Keep 패턴에 걸리면 무조건 포함\n",
    "        if match_any(col, KEEP_PATTERNS):\n",
    "            feature_cols.append(col)\n",
    "            continue\n",
    "\n",
    "        # 3) 위에 걸리지 않았는데 numeric type이면 feature로 사용\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            feature_cols.append(col)\n",
    "\n",
    "    print(f\"   >>> Selected {len(feature_cols)} feature columns.\")\n",
    "    return feature_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bdd6ec",
   "metadata": {},
   "source": [
    "6. 전처리 + Train/Valid/Test Split + Sample Weight 계산\n",
    "### 6.1 날짜 처리 & Split 기준\n",
    "- `date_str` 형식: `\"YYYY_MM_DD\"`\n",
    "- 이를 `datetime`으로 변환한 `date` 컬럼을 생성\n",
    "- Split 기준:\n",
    "    - Train: 2017-01-01 ~ 2018-12-31\n",
    "    - Valid: 2019-01-01 ~ 2019-06-30\n",
    "    - Test : 2019-07-01 ~ 2019-12-31\n",
    "### 6.2 Sample Weight\n",
    "- 같은 날짜에 기사 N개 → 각 행의 가중치 = 1 / N\n",
    "- 이렇게 하면 하루 단위 loss contribution이 모두 비슷해짐."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4336f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_split(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    1) feature 컬럼 자동 선택\n",
    "    2) Train/Valid/Test Split\n",
    "    3) 날짜별 기사 수 기반 sample weight 계산\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Date 컬럼 확인 (이미 load_data에서 생성됨)\n",
    "    if 'Date' not in df.columns:\n",
    "        raise ValueError(\"'Date' column not found in dataframe\")\n",
    "    \n",
    "    # date 컬럼 생성 (기존 코드와 호환성 유지)\n",
    "    df[\"date\"] = df[\"Date\"]\n",
    "\n",
    "    # 2. feature 컬럼 선택 (data leakage 방지 핵심)\n",
    "    feature_cols = extract_feature_columns(df)\n",
    "\n",
    "    # 3. Split 마스크 정의\n",
    "    train_mask = (df[\"date\"] >= \"2017-01-01\") & (df[\"date\"] <= \"2018-12-31\")\n",
    "    valid_mask = (df[\"date\"] >= \"2019-01-01\") & (df[\"date\"] <= \"2019-06-30\")\n",
    "    test_mask  = (df[\"date\"] >= \"2019-07-01\") & (df[\"date\"] <= \"2019-12-31\")\n",
    "\n",
    "    # 5. 날짜별 sample weight 계산 함수\n",
    "    def make_X_y_w_dates(sub_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        subset DataFrame에 대해:\n",
    "          - X: feature matrix\n",
    "          - y: target (value)\n",
    "          - w: sample weight (1 / 날짜별 기사 수)\n",
    "          - d: 날짜 문자열 (JSON/평가 저장용)\n",
    "        \"\"\"\n",
    "        if len(sub_df) == 0:\n",
    "            return None, None, None, None\n",
    "\n",
    "        if \"value\" not in sub_df.columns:\n",
    "            raise ValueError(\"target 컬럼 'value'가 없습니다.\")\n",
    "\n",
    "        # 날짜 기준 기사 개수\n",
    "        # date_index가 있으면 그걸 쓰고, 없으면 date를 사용\n",
    "        if \"date_index\" in sub_df.columns:\n",
    "            key = sub_df[\"date_index\"]\n",
    "        else:\n",
    "            key = sub_df[\"date\"]\n",
    "\n",
    "        counts = key.value_counts()\n",
    "        weights = 1.0 / key.map(counts)\n",
    "\n",
    "        X = sub_df[feature_cols]\n",
    "        y = sub_df[\"value\"].astype(float)\n",
    "        d = sub_df[\"date\"].dt.strftime(\"%Y-%m-%d\")   # JSON 저장용 포맷\n",
    "\n",
    "        return X, y, weights, d\n",
    "\n",
    "    # 실제 split\n",
    "    train_df = df[train_mask].copy()\n",
    "    valid_df = df[valid_mask].copy()\n",
    "    test_df  = df[test_mask].copy()\n",
    "\n",
    "    X_train, y_train, w_train, _       = make_X_y_w_dates(train_df)\n",
    "    X_valid, y_valid, w_valid, _       = make_X_y_w_dates(valid_df)\n",
    "    X_test,  y_test,  w_test, d_test   = make_X_y_w_dates(test_df)\n",
    "\n",
    "    print(f\"   >>> Split sizes | train: {len(train_df)}, valid: {len(valid_df)}, test: {len(test_df)}\")\n",
    "    return (X_train, y_train, w_train), (X_valid, y_valid, w_valid), (X_test, y_test, w_test, d_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c39bd7",
   "metadata": {},
   "source": [
    "7. LightGBM 학습/예측 프로세스 (파일 하나 기준)\n",
    "\n",
    "- 이 함수는 **하나의 parquet 파일(dataset_*.parquet)** 을 입력으로 받아:\n",
    "    1. 데이터 로드\n",
    "    2. 전처리 + split + weight 계산\n",
    "    3. LightGBM 학습\n",
    "    4. 기사 단위 예측 → 날짜별 평균\n",
    "    5. 일별 MSE 계산\n",
    "    6. JSON / metrics 리스트에 결과 기록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc3bdf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lightgbm_for_file(fname: str, metrics: list):\n",
    "    \"\"\"\n",
    "    단일 feature dataset 파일에 대해 LightGBM 학습 및 평가를 수행하고,\n",
    "    결과를 metrics 리스트에 추가한다.\n",
    "    \"\"\"\n",
    "    info = parse_dataset_filename(fname)\n",
    "    Dataset = info[\"Dataset\"]\n",
    "    Method  = info[\"Method\"]\n",
    "    Type    = info[\"Type\"]\n",
    "\n",
    "    print(f\"\\n>>> Processing file: {fname}\")\n",
    "    print(f\"    Dataset={Dataset}, Method={Method}, Type={Type}\")\n",
    "\n",
    "    # 1. 데이터 로드 (load_data 함수 사용)\n",
    "    file_path = DATA_DIR / fname\n",
    "    df = load_data(file_path)\n",
    "\n",
    "    # 2. 전처리 + split + sample weight\n",
    "    train_set, valid_set, test_set = preprocess_and_split(df)\n",
    "    X_train, y_train, w_train = train_set\n",
    "    X_valid, y_valid, w_valid = valid_set\n",
    "    X_test,  y_test,  w_test, d_test = test_set\n",
    "\n",
    "    if X_train is None or X_valid is None or X_test is None:\n",
    "        print(\"   [Warning] Empty split (train/valid/test 중 일부가 비어 있음). 스킵합니다.\")\n",
    "        return\n",
    "\n",
    "    # 3. LightGBM Dataset 생성\n",
    "    dtrain = lgb.Dataset(X_train, label=y_train, weight=w_train)\n",
    "    dvalid = lgb.Dataset(X_valid, label=y_valid, weight=w_valid, reference=dtrain)\n",
    "\n",
    "    # 4. LightGBM 파라미터 (v4.x 호환)\n",
    "    params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"mse\",\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"num_leaves\": 31,\n",
    "        \"feature_fraction\": 0.9,\n",
    "        \"bagging_fraction\": 0.8,\n",
    "        \"bagging_freq\": 1,\n",
    "        \"seed\": RANDOM_SEED,\n",
    "        \"verbosity\": -1,\n",
    "        \"n_jobs\": -1,\n",
    "    }\n",
    "\n",
    "    callbacks = [\n",
    "        lgb.early_stopping(stopping_rounds=50, verbose=False),\n",
    "        lgb.log_evaluation(period=100),\n",
    "    ]\n",
    "\n",
    "    # 5. 학습\n",
    "    print(\"   >>> Training LightGBM ...\")\n",
    "    model = lgb.train(\n",
    "        params=params,\n",
    "        train_set=dtrain,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[dtrain, dvalid],\n",
    "        valid_names=[\"train\", \"valid\"],\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "    # 6. 기사 단위 예측\n",
    "    print(\"   >>> Predicting on test set ...\")\n",
    "    y_pred_raw = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "\n",
    "    # 7. 날짜별 집계\n",
    "    #    - 같은 날짜에 여러 기사가 있을 수 있으므로\n",
    "    #      actual은 first(), pred는 mean()으로 집계\n",
    "    res_df = pd.DataFrame({\n",
    "        \"date\": d_test.values,\n",
    "        \"actual\": y_test.values,\n",
    "        \"pred\": y_pred_raw,\n",
    "    })\n",
    "\n",
    "    daily_actual = res_df.groupby(\"date\")[\"actual\"].first()\n",
    "    daily_pred   = res_df.groupby(\"date\")[\"pred\"].mean()\n",
    "    daily_df     = pd.concat([daily_actual, daily_pred], axis=1)\n",
    "\n",
    "    # 8. 일별 MSE 계산\n",
    "    mse = mean_squared_error(daily_df[\"actual\"], daily_df[\"pred\"])\n",
    "    print(f\"   [Result] Daily MSE: {mse:.6f}\")\n",
    "\n",
    "    # 9. metrics 리스트에 기록 (나중에 CSV로 저장)\n",
    "    metrics.append({\n",
    "        \"Feature_set\": Dataset,\n",
    "        \"Embeddings\": Method if Method is not None else \"-\",\n",
    "        \"Dim_reduction\": Type if Type is not None else \"-\",\n",
    "        \"Model\": \"LightGBM\",\n",
    "        \"MSE\": mse,\n",
    "    })\n",
    "\n",
    "    # 10. 일별 예측 결과 JSON 저장\n",
    "    #     구조: { \"YYYY-MM-DD\": { \"actual\": ..., \"pred\": ... }, ... }\n",
    "    json_obj = {\n",
    "        date: {\n",
    "            \"actual\": float(row[\"actual\"]),\n",
    "            \"pred\": float(row[\"pred\"]),\n",
    "        }\n",
    "        for date, row in daily_df.iterrows()\n",
    "    }\n",
    "\n",
    "    # 데이터셋 이름 생성\n",
    "    if Dataset == \"A\":\n",
    "        dataset_name = \"A\"\n",
    "    else:\n",
    "        dataset_name = f\"{Dataset}_{Method or 'none'}_{Type or 'none'}\"\n",
    "    \n",
    "    json_name = f\"pred_LightGBM_{dataset_name}.json\"\n",
    "    json_path = RESULTS_DIR / json_name\n",
    "    \n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_obj, f, indent=2)\n",
    "    print(f\"   >>> Saved prediction JSON to: {json_path}\")\n",
    "\n",
    "    # 11. 메모리 정리\n",
    "    del df, dtrain, dvalid, model, X_train, X_valid, X_test\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478c37f5",
   "metadata": {},
   "source": [
    "8. 전체 파일에 대해 LightGBM 실행\n",
    "- feature_datasets 폴더 안의 모든 `dataset_*.parquet` 파일에 대해\n",
    "  위에서 정의한 `run_lightgbm_for_file()`를 순차 실행\n",
    "- 실행이 성공한 경우 metrics 리스트에 한 줄씩 기록\n",
    "- 마지막에 metrics를 하나의 CSV로 저장:\n",
    "    - Columns: Dataset / Method / Type / Model / MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0208a671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25 dataset files.\n",
      "\n",
      ">>> Processing file: dataset_A.parquet\n",
      "    Dataset=A, Method=None, Type=None\n",
      "Loading dataset_A.parquet...\n",
      "   Loaded 754 rows, 9 columns\n",
      "   Date range: 2017-01-03 to 2019-12-31\n",
      "   >>> Selected 5 feature columns.\n",
      "   >>> Split sizes | train: 502, valid: 124, test: 128\n",
      "   >>> Training LightGBM ...\n",
      "[100]\ttrain's l2: 244.718\tvalid's l2: 479.234\n",
      "[200]\ttrain's l2: 166.844\tvalid's l2: 467.409\n",
      "   >>> Predicting on test set ...\n",
      "   [Result] Daily MSE: 21608.745003\n",
      "   >>> Saved prediction JSON to: results_lightgbm/results/pred_LightGBM_A.json\n",
      "\n",
      ">>> Processing file: dataset_B_bodyText_orig.parquet\n",
      "    Dataset=B, Method=bodyText, Type=orig\n",
      "Loading dataset_B_bodyText_orig.parquet...\n",
      "   Loaded 460722 rows, 1037 columns\n",
      "   Date range: 2017-01-01 to 2019-12-31\n",
      "   >>> Selected 1029 feature columns.\n",
      "   >>> Split sizes | train: 312185, valid: 76136, test: 72401\n",
      "   >>> Training LightGBM ...\n",
      "[100]\ttrain's l2: 35.8323\tvalid's l2: 484.809\n",
      "   >>> Predicting on test set ...\n",
      "   [Result] Daily MSE: 20862.665131\n",
      "   >>> Saved prediction JSON to: results_lightgbm/results/pred_LightGBM_B_bodyText_orig.json\n",
      "\n",
      ">>> Processing file: dataset_B_bodyText_pca.parquet\n",
      "    Dataset=B, Method=bodyText, Type=pca\n",
      "Loading dataset_B_bodyText_pca.parquet...\n",
      "   Loaded 460722 rows, 204 columns\n",
      "   Date range: 2017-01-01 to 2019-12-31\n",
      "   >>> Selected 196 feature columns.\n",
      "   >>> Split sizes | train: 312185, valid: 76136, test: 72401\n",
      "   >>> Training LightGBM ...\n",
      "[100]\ttrain's l2: 36.7021\tvalid's l2: 488.125\n",
      "[200]\ttrain's l2: 8.57705\tvalid's l2: 469.605\n",
      "   >>> Predicting on test set ...\n",
      "   [Result] Daily MSE: 20820.148873\n",
      "   >>> Saved prediction JSON to: results_lightgbm/results/pred_LightGBM_B_bodyText_pca.json\n",
      "\n",
      ">>> Processing file: dataset_B_chunking_orig.parquet\n",
      "    Dataset=B, Method=chunking, Type=orig\n",
      "Loading dataset_B_chunking_orig.parquet...\n",
      "   Loaded 460722 rows, 1037 columns\n",
      "   Date range: 2017-01-01 to 2019-12-31\n",
      "   >>> Selected 1029 feature columns.\n",
      "   >>> Split sizes | train: 312185, valid: 76136, test: 72401\n",
      "   >>> Training LightGBM ...\n",
      "[100]\ttrain's l2: 35.8323\tvalid's l2: 484.809\n",
      "   >>> Predicting on test set ...\n",
      "   [Result] Daily MSE: 20862.665131\n",
      "   >>> Saved prediction JSON to: results_lightgbm/results/pred_LightGBM_B_chunking_orig.json\n",
      "\n",
      ">>> Processing file: dataset_B_chunking_pca.parquet\n",
      "    Dataset=B, Method=chunking, Type=pca\n",
      "Loading dataset_B_chunking_pca.parquet...\n",
      "   Loaded 460722 rows, 257 columns\n",
      "   Date range: 2017-01-01 to 2019-12-31\n",
      "   >>> Selected 249 feature columns.\n",
      "   >>> Split sizes | train: 312185, valid: 76136, test: 72401\n",
      "   >>> Training LightGBM ...\n",
      "[100]\ttrain's l2: 36.7054\tvalid's l2: 490.746\n",
      "   >>> Predicting on test set ...\n",
      "   [Result] Daily MSE: 20824.572243\n",
      "   >>> Saved prediction JSON to: results_lightgbm/results/pred_LightGBM_B_chunking_pca.json\n",
      "\n",
      ">>> Processing file: dataset_B_headlines_orig.parquet\n",
      "    Dataset=B, Method=headlines, Type=orig\n",
      "Loading dataset_B_headlines_orig.parquet...\n",
      "   Loaded 461270 rows, 1037 columns\n",
      "   Date range: 2017-01-01 to 2019-12-31\n",
      "   >>> Selected 1029 feature columns.\n",
      "   >>> Split sizes | train: 312535, valid: 76232, test: 72503\n",
      "   >>> Training LightGBM ...\n",
      "[100]\ttrain's l2: 36.33\tvalid's l2: 496.857\n",
      "   >>> Predicting on test set ...\n",
      "   [Result] Daily MSE: 20858.985063\n",
      "   >>> Saved prediction JSON to: results_lightgbm/results/pred_LightGBM_B_headlines_orig.json\n",
      "\n",
      ">>> Processing file: dataset_B_headlines_pca.parquet\n",
      "    Dataset=B, Method=headlines, Type=pca\n",
      "Loading dataset_B_headlines_pca.parquet...\n",
      "   Loaded 461270 rows, 317 columns\n",
      "   Date range: 2017-01-01 to 2019-12-31\n",
      "   >>> Selected 309 feature columns.\n",
      "   >>> Split sizes | train: 312535, valid: 76232, test: 72503\n",
      "   >>> Training LightGBM ...\n",
      "[100]\ttrain's l2: 37.7286\tvalid's l2: 486.248\n",
      "   >>> Predicting on test set ...\n",
      "   [Result] Daily MSE: 20764.420328\n",
      "   >>> Saved prediction JSON to: results_lightgbm/results/pred_LightGBM_B_headlines_pca.json\n",
      "\n",
      ">>> Processing file: dataset_B_paragraphs_orig.parquet\n",
      "    Dataset=B, Method=paragraphs, Type=orig\n",
      "Loading dataset_B_paragraphs_orig.parquet...\n",
      "   Loaded 460722 rows, 1037 columns\n",
      "   Date range: 2017-01-01 to 2019-12-31\n",
      "   >>> Selected 1029 feature columns.\n",
      "   >>> Split sizes | train: 312185, valid: 76136, test: 72401\n",
      "   >>> Training LightGBM ...\n",
      "[100]\ttrain's l2: 35.8323\tvalid's l2: 484.809\n",
      "   >>> Predicting on test set ...\n",
      "   [Result] Daily MSE: 20862.665131\n",
      "   >>> Saved prediction JSON to: results_lightgbm/results/pred_LightGBM_B_paragraphs_orig.json\n",
      "\n",
      ">>> Processing file: dataset_B_paragraphs_pca.parquet\n",
      "    Dataset=B, Method=paragraphs, Type=pca\n",
      "Loading dataset_B_paragraphs_pca.parquet...\n",
      "   Loaded 460722 rows, 221 columns\n",
      "   Date range: 2017-01-01 to 2019-12-31\n",
      "   >>> Selected 213 feature columns.\n",
      "   >>> Split sizes | train: 312185, valid: 76136, test: 72401\n",
      "   >>> Training LightGBM ...\n",
      "[100]\ttrain's l2: 36.9378\tvalid's l2: 479.799\n",
      "[200]\ttrain's l2: 8.83534\tvalid's l2: 472.454\n",
      "   >>> Predicting on test set ...\n",
      "   [Result] Daily MSE: 20613.334087\n",
      "   >>> Saved prediction JSON to: results_lightgbm/results/pred_LightGBM_B_paragraphs_pca.json\n",
      "\n",
      ">>> Processing file: dataset_C_bodyText_orig.parquet\n",
      "    Dataset=C, Method=bodyText, Type=orig\n",
      "Loading dataset_C_bodyText_orig.parquet...\n",
      "   Loaded 460722 rows, 1137 columns\n",
      "   Date range: 2017-01-01 to 2019-12-31\n",
      "   >>> Selected 1129 feature columns.\n",
      "   >>> Split sizes | train: 312185, valid: 76136, test: 72401\n",
      "   >>> Training LightGBM ...\n",
      "[100]\ttrain's l2: 36.3068\tvalid's l2: 464.28\n",
      "   >>> Predicting on test set ...\n",
      "   [Result] Daily MSE: 20863.789584\n",
      "   >>> Saved prediction JSON to: results_lightgbm/results/pred_LightGBM_C_bodyText_orig.json\n",
      "\n",
      ">>> Processing file: dataset_C_bodyText_pca.parquet\n",
      "    Dataset=C, Method=bodyText, Type=pca\n",
      "Loading dataset_C_bodyText_pca.parquet...\n",
      "   Loaded 460722 rows, 304 columns\n",
      "   Date range: 2017-01-01 to 2019-12-31\n",
      "   >>> Selected 296 feature columns.\n",
      "   >>> Split sizes | train: 312185, valid: 76136, test: 72401\n",
      "   >>> Training LightGBM ...\n",
      "[100]\ttrain's l2: 36.7497\tvalid's l2: 471.143\n",
      "   >>> Predicting on test set ...\n",
      "   [Result] Daily MSE: 20877.724835\n",
      "   >>> Saved prediction JSON to: results_lightgbm/results/pred_LightGBM_C_bodyText_pca.json\n",
      "\n",
      ">>> Processing file: dataset_C_chunking_orig.parquet\n",
      "    Dataset=C, Method=chunking, Type=orig\n",
      "Loading dataset_C_chunking_orig.parquet...\n",
      "   Loaded 460722 rows, 1137 columns\n",
      "   Date range: 2017-01-01 to 2019-12-31\n",
      "   >>> Selected 1129 feature columns.\n",
      "   >>> Split sizes | train: 312185, valid: 76136, test: 72401\n",
      "   >>> Training LightGBM ...\n",
      "[100]\ttrain's l2: 36.3068\tvalid's l2: 464.28\n",
      "   >>> Predicting on test set ...\n",
      "   [Result] Daily MSE: 20863.789584\n",
      "   >>> Saved prediction JSON to: results_lightgbm/results/pred_LightGBM_C_chunking_orig.json\n",
      "\n",
      ">>> Processing file: dataset_C_chunking_pca.parquet\n",
      "    Dataset=C, Method=chunking, Type=pca\n",
      "Loading dataset_C_chunking_pca.parquet...\n",
      "   Loaded 460722 rows, 357 columns\n",
      "   Date range: 2017-01-01 to 2019-12-31\n",
      "   >>> Selected 349 feature columns.\n",
      "   >>> Split sizes | train: 312185, valid: 76136, test: 72401\n",
      "   >>> Training LightGBM ...\n",
      "[100]\ttrain's l2: 36.815\tvalid's l2: 469.336\n",
      "[200]\ttrain's l2: 8.81534\tvalid's l2: 460.198\n",
      "   >>> Predicting on test set ...\n",
      "   [Result] Daily MSE: 20844.820314\n",
      "   >>> Saved prediction JSON to: results_lightgbm/results/pred_LightGBM_C_chunking_pca.json\n",
      "\n",
      ">>> Processing file: dataset_C_headlines_orig.parquet\n",
      "    Dataset=C, Method=headlines, Type=orig\n",
      "Loading dataset_C_headlines_orig.parquet...\n",
      "   Loaded 461270 rows, 1137 columns\n",
      "   Date range: 2017-01-01 to 2019-12-31\n",
      "   >>> Selected 1129 feature columns.\n",
      "   >>> Split sizes | train: 312535, valid: 76232, test: 72503\n",
      "   >>> Training LightGBM ...\n",
      "[100]\ttrain's l2: 36.6541\tvalid's l2: 479.657\n",
      "[200]\ttrain's l2: 8.98215\tvalid's l2: 470.332\n",
      "   >>> Predicting on test set ...\n",
      "   [Result] Daily MSE: 20822.424987\n",
      "   >>> Saved prediction JSON to: results_lightgbm/results/pred_LightGBM_C_headlines_orig.json\n",
      "\n",
      ">>> Processing file: dataset_C_headlines_pca.parquet\n",
      "    Dataset=C, Method=headlines, Type=pca\n",
      "Loading dataset_C_headlines_pca.parquet...\n",
      "   Loaded 461270 rows, 417 columns\n",
      "   Date range: 2017-01-01 to 2019-12-31\n",
      "   >>> Selected 409 feature columns.\n",
      "   >>> Split sizes | train: 312535, valid: 76232, test: 72503\n",
      "   >>> Training LightGBM ...\n",
      "[100]\ttrain's l2: 37.3412\tvalid's l2: 496.332\n",
      "   >>> Predicting on test set ...\n",
      "   [Result] Daily MSE: 20824.117793\n",
      "   >>> Saved prediction JSON to: results_lightgbm/results/pred_LightGBM_C_headlines_pca.json\n",
      "\n",
      ">>> Processing file: dataset_C_paragraphs_orig.parquet\n",
      "    Dataset=C, Method=paragraphs, Type=orig\n",
      "Loading dataset_C_paragraphs_orig.parquet...\n",
      "   Loaded 460722 rows, 1137 columns\n",
      "   Date range: 2017-01-01 to 2019-12-31\n",
      "   >>> Selected 1129 feature columns.\n",
      "   >>> Split sizes | train: 312185, valid: 76136, test: 72401\n",
      "   >>> Training LightGBM ...\n",
      "[100]\ttrain's l2: 36.3068\tvalid's l2: 464.28\n",
      "   >>> Predicting on test set ...\n",
      "   [Result] Daily MSE: 20863.789584\n",
      "   >>> Saved prediction JSON to: results_lightgbm/results/pred_LightGBM_C_paragraphs_orig.json\n",
      "\n",
      ">>> Processing file: dataset_C_paragraphs_pca.parquet\n",
      "    Dataset=C, Method=paragraphs, Type=pca\n",
      "Loading dataset_C_paragraphs_pca.parquet...\n",
      "   Loaded 460722 rows, 321 columns\n",
      "   Date range: 2017-01-01 to 2019-12-31\n",
      "   >>> Selected 313 feature columns.\n",
      "   >>> Split sizes | train: 312185, valid: 76136, test: 72401\n",
      "   >>> Training LightGBM ...\n",
      "[100]\ttrain's l2: 36.7616\tvalid's l2: 476.894\n",
      "   >>> Predicting on test set ...\n",
      "   [Result] Daily MSE: 20762.518331\n",
      "   >>> Saved prediction JSON to: results_lightgbm/results/pred_LightGBM_C_paragraphs_pca.json\n",
      "\n",
      ">>> Processing file: dataset_D_bodyText_orig.parquet\n",
      "    Dataset=D, Method=bodyText, Type=orig\n",
      "Loading dataset_D_bodyText_orig.parquet...\n",
      "   Loaded 460722 rows, 1143 columns\n",
      "   Date range: 2017-01-01 to 2019-12-31\n",
      "   >>> Selected 1134 feature columns.\n",
      "   >>> Split sizes | train: 312185, valid: 76136, test: 72401\n",
      "   >>> Training LightGBM ...\n",
      "[100]\ttrain's l2: 21.2348\tvalid's l2: 504.023\n",
      "   >>> Predicting on test set ...\n",
      "   [Result] Daily MSE: 20211.397074\n",
      "   >>> Saved prediction JSON to: results_lightgbm/results/pred_LightGBM_D_bodyText_orig.json\n",
      "\n",
      ">>> Processing file: dataset_D_bodyText_pca.parquet\n",
      "    Dataset=D, Method=bodyText, Type=pca\n",
      "Loading dataset_D_bodyText_pca.parquet...\n",
      "   Loaded 460722 rows, 310 columns\n",
      "   Date range: 2017-01-01 to 2019-12-31\n",
      "   >>> Selected 301 feature columns.\n",
      "   >>> Split sizes | train: 312185, valid: 76136, test: 72401\n",
      "   >>> Training LightGBM ...\n",
      "[100]\ttrain's l2: 21.7888\tvalid's l2: 499.538\n",
      "[200]\ttrain's l2: 3.04034\tvalid's l2: 483.46\n",
      "   >>> Predicting on test set ...\n",
      "   [Result] Daily MSE: 20582.037853\n",
      "   >>> Saved prediction JSON to: results_lightgbm/results/pred_LightGBM_D_bodyText_pca.json\n",
      "\n",
      ">>> Processing file: dataset_D_chunking_orig.parquet\n",
      "    Dataset=D, Method=chunking, Type=orig\n",
      "Loading dataset_D_chunking_orig.parquet...\n",
      "   Loaded 460722 rows, 1143 columns\n",
      "   Date range: 2017-01-01 to 2019-12-31\n",
      "   >>> Selected 1134 feature columns.\n",
      "   >>> Split sizes | train: 312185, valid: 76136, test: 72401\n",
      "   >>> Training LightGBM ...\n",
      "[100]\ttrain's l2: 21.2348\tvalid's l2: 504.023\n",
      "   >>> Predicting on test set ...\n",
      "   [Result] Daily MSE: 20211.397074\n",
      "   >>> Saved prediction JSON to: results_lightgbm/results/pred_LightGBM_D_chunking_orig.json\n",
      "\n",
      ">>> Processing file: dataset_D_chunking_pca.parquet\n",
      "    Dataset=D, Method=chunking, Type=pca\n",
      "Loading dataset_D_chunking_pca.parquet...\n",
      "   Loaded 460722 rows, 363 columns\n",
      "   Date range: 2017-01-01 to 2019-12-31\n",
      "   >>> Selected 354 feature columns.\n",
      "   >>> Split sizes | train: 312185, valid: 76136, test: 72401\n",
      "   >>> Training LightGBM ...\n",
      "[100]\ttrain's l2: 21.2917\tvalid's l2: 504.318\n",
      "[200]\ttrain's l2: 3.01738\tvalid's l2: 490.044\n",
      "[300]\ttrain's l2: 0.744932\tvalid's l2: 490.304\n",
      "   >>> Predicting on test set ...\n",
      "   [Result] Daily MSE: 19937.865797\n",
      "   >>> Saved prediction JSON to: results_lightgbm/results/pred_LightGBM_D_chunking_pca.json\n",
      "\n",
      ">>> Processing file: dataset_D_headlines_orig.parquet\n",
      "    Dataset=D, Method=headlines, Type=orig\n",
      "Loading dataset_D_headlines_orig.parquet...\n",
      "   Loaded 461270 rows, 1143 columns\n",
      "   Date range: 2017-01-01 to 2019-12-31\n",
      "   >>> Selected 1134 feature columns.\n",
      "   >>> Split sizes | train: 312535, valid: 76232, test: 72503\n",
      "   >>> Training LightGBM ...\n",
      "[100]\ttrain's l2: 21.607\tvalid's l2: 503.382\n",
      "[200]\ttrain's l2: 2.97269\tvalid's l2: 486.201\n",
      "   >>> Predicting on test set ...\n",
      "   [Result] Daily MSE: 20170.113705\n",
      "   >>> Saved prediction JSON to: results_lightgbm/results/pred_LightGBM_D_headlines_orig.json\n",
      "\n",
      ">>> Processing file: dataset_D_headlines_pca.parquet\n",
      "    Dataset=D, Method=headlines, Type=pca\n",
      "Loading dataset_D_headlines_pca.parquet...\n",
      "   Loaded 461270 rows, 423 columns\n",
      "   Date range: 2017-01-01 to 2019-12-31\n",
      "   >>> Selected 414 feature columns.\n",
      "   >>> Split sizes | train: 312535, valid: 76232, test: 72503\n",
      "   >>> Training LightGBM ...\n",
      "[100]\ttrain's l2: 20.8472\tvalid's l2: 518.693\n",
      "[200]\ttrain's l2: 2.99768\tvalid's l2: 503.759\n",
      "   >>> Predicting on test set ...\n",
      "   [Result] Daily MSE: 20390.842966\n",
      "   >>> Saved prediction JSON to: results_lightgbm/results/pred_LightGBM_D_headlines_pca.json\n",
      "\n",
      ">>> Processing file: dataset_D_paragraphs_orig.parquet\n",
      "    Dataset=D, Method=paragraphs, Type=orig\n",
      "Loading dataset_D_paragraphs_orig.parquet...\n",
      "   Loaded 460722 rows, 1143 columns\n",
      "   Date range: 2017-01-01 to 2019-12-31\n",
      "   >>> Selected 1134 feature columns.\n",
      "   >>> Split sizes | train: 312185, valid: 76136, test: 72401\n",
      "   >>> Training LightGBM ...\n",
      "[100]\ttrain's l2: 21.2348\tvalid's l2: 504.023\n",
      "   >>> Predicting on test set ...\n",
      "   [Result] Daily MSE: 20211.397074\n",
      "   >>> Saved prediction JSON to: results_lightgbm/results/pred_LightGBM_D_paragraphs_orig.json\n",
      "\n",
      ">>> Processing file: dataset_D_paragraphs_pca.parquet\n",
      "    Dataset=D, Method=paragraphs, Type=pca\n",
      "Loading dataset_D_paragraphs_pca.parquet...\n",
      "   Loaded 460722 rows, 327 columns\n",
      "   Date range: 2017-01-01 to 2019-12-31\n",
      "   >>> Selected 318 feature columns.\n",
      "   >>> Split sizes | train: 312185, valid: 76136, test: 72401\n",
      "   >>> Training LightGBM ...\n",
      "[100]\ttrain's l2: 21.2272\tvalid's l2: 512.03\n",
      "[200]\ttrain's l2: 3.06987\tvalid's l2: 499.99\n",
      "   >>> Predicting on test set ...\n",
      "   [Result] Daily MSE: 20304.188570\n",
      "   >>> Saved prediction JSON to: results_lightgbm/results/pred_LightGBM_D_paragraphs_pca.json\n",
      "\n",
      "[Done] All files processed.\n",
      "Metrics saved to: results_lightgbm/lightgbm_evaluation_metrics.csv\n",
      "   Feature_set  Embeddings Dim_reduction     Model           MSE\n",
      "0            A           -             -  LightGBM  21608.745003\n",
      "1            B    bodyText          orig  LightGBM  20862.665131\n",
      "2            B    bodyText           pca  LightGBM  20820.148873\n",
      "3            B    chunking          orig  LightGBM  20862.665131\n",
      "4            B    chunking           pca  LightGBM  20824.572243\n",
      "5            B   headlines          orig  LightGBM  20858.985063\n",
      "6            B   headlines           pca  LightGBM  20764.420328\n",
      "7            B  paragraphs          orig  LightGBM  20862.665131\n",
      "8            B  paragraphs           pca  LightGBM  20613.334087\n",
      "9            C    bodyText          orig  LightGBM  20863.789584\n",
      "10           C    bodyText           pca  LightGBM  20877.724835\n",
      "11           C    chunking          orig  LightGBM  20863.789584\n",
      "12           C    chunking           pca  LightGBM  20844.820314\n",
      "13           C   headlines          orig  LightGBM  20822.424987\n",
      "14           C   headlines           pca  LightGBM  20824.117793\n",
      "15           C  paragraphs          orig  LightGBM  20863.789584\n",
      "16           C  paragraphs           pca  LightGBM  20762.518331\n",
      "17           D    bodyText          orig  LightGBM  20211.397074\n",
      "18           D    bodyText           pca  LightGBM  20582.037853\n",
      "19           D    chunking          orig  LightGBM  20211.397074\n",
      "20           D    chunking           pca  LightGBM  19937.865797\n",
      "21           D   headlines          orig  LightGBM  20170.113705\n",
      "22           D   headlines           pca  LightGBM  20390.842966\n",
      "23           D  paragraphs          orig  LightGBM  20211.397074\n",
      "24           D  paragraphs           pca  LightGBM  20304.188570\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    if not DATA_DIR.exists():\n",
    "        print(\"[Error] feature_datasets 폴더를 찾을 수 없습니다.\")\n",
    "        return\n",
    "\n",
    "    # 처리할 파일 목록\n",
    "    file_list = sorted(\n",
    "        [f for f in os.listdir(DATA_DIR) if f.startswith(\"dataset_\") and f.endswith(\".parquet\")]\n",
    "    )\n",
    "    print(f\"Found {len(file_list)} dataset files.\")\n",
    "\n",
    "    metrics = []\n",
    "\n",
    "    for fname in file_list:\n",
    "        try:\n",
    "            run_lightgbm_for_file(fname, metrics)\n",
    "        except Exception as e:\n",
    "            print(f\"   [Error] Failed processing {fname}: {e}\")\n",
    "\n",
    "    # 결과를 CSV로 저장\n",
    "    if metrics:\n",
    "        metrics_df = pd.DataFrame(metrics)\n",
    "        metrics_df = metrics_df.sort_values([\"Feature_set\", \"Embeddings\", \"Dim_reduction\"])\n",
    "\n",
    "        csv_path = OUTPUT_DIR / \"lightgbm_evaluation_metrics.csv\"\n",
    "        metrics_df.to_csv(csv_path, index=False)\n",
    "        print(\"\\n[Done] All files processed.\")\n",
    "        print(\"Metrics saved to:\", csv_path)\n",
    "        print(metrics_df)\n",
    "    else:\n",
    "        print(\"No metrics were produced. (모든 파일이 에러로 스킵된 듯 합니다.)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
