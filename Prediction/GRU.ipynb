{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# GRU Stock Price Prediction\n",
        "\n",
        "## Configuration\n",
        "- Sequence Length: 5 days\n",
        "- Hidden Dim: 64\n",
        "- Num Layers: 2\n",
        "- Dropout: 0.3\n",
        "- Batch Size: 128\n",
        "- Epochs: 100 (Early Stopping: patience=10)\n",
        "- Sample Weighting: Enabled\n",
        "- Target: Next day stock value\n",
        "- Normalization: MinMaxScaler\n",
        "- Aggregation: Article-level prediction → Daily average"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import"
      },
      "source": [
        "## 0. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "path"
      },
      "source": [
        "## 1. Path Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paths"
      },
      "outputs": [],
      "source": [
        "# Path configuration\n",
        "# Google Colab:\n",
        "# DATA_DIR = Path(\"/content/drive/MyDrive/COSE362/data/feature_engineering\")\n",
        "# OUTPUT_DIR = Path(\"/content/drive/MyDrive/COSE362/data/prediction_output/results_gru\")\n",
        "\n",
        "# Local:\n",
        "DATA_DIR = Path(\"../feature_datasets\")\n",
        "OUTPUT_DIR = Path(\"results_gru\")\n",
        "\n",
        "RESULTS_DIR = OUTPUT_DIR / \"results\"\n",
        "\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Data Source: {DATA_DIR}\")\n",
        "print(f\"Output Path: {OUTPUT_DIR}\")\n",
        "print(f\"Results Path: {RESULTS_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "device"
      },
      "source": [
        "## 2. Device Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "device_config"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyperparams"
      },
      "source": [
        "## 3. Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config"
      },
      "outputs": [],
      "source": [
        "# GRU Hyperparameters\n",
        "CONFIG = {\n",
        "    'seq_length': 5,\n",
        "    'hidden_dim': 64,\n",
        "    'num_layers': 2,\n",
        "    'dropout': 0.3,\n",
        "    'batch_size': 128,\n",
        "    'epochs': 100,\n",
        "    'early_stopping_patience': 10,\n",
        "    'learning_rate': 0.001,\n",
        "}\n",
        "\n",
        "print(\"GRU Configuration:\")\n",
        "for key, value in CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_loading"
      },
      "source": [
        "## 4. Data Loading Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_data"
      },
      "outputs": [],
      "source": [
        "def load_data(file_path):\n",
        "    \"\"\"\n",
        "    Load parquet file and create Date column\n",
        "    \"\"\"\n",
        "    print(f\"Loading {file_path.name}...\")\n",
        "    df = pd.read_parquet(file_path)\n",
        "    \n",
        "    # Sort by date_index\n",
        "    if 'date_index' in df.columns:\n",
        "        df = df.sort_values('date_index').reset_index(drop=True)\n",
        "    \n",
        "    # Create Date column\n",
        "    if 'pub_date' in df.columns:\n",
        "        df['Date'] = pd.to_datetime(df['pub_date'], format='%Y_%m_%d')\n",
        "        print(f\"   Using 'pub_date' for Date column\")\n",
        "    elif 'date_str' in df.columns:\n",
        "        df['Date'] = pd.to_datetime(df['date_str'], format='%Y_%m_%d')\n",
        "        print(f\"   Using 'date_str' for Date column\")\n",
        "    elif 'date_index' in df.columns:\n",
        "        base_date = pd.to_datetime('2017-01-01')\n",
        "        df['Date'] = base_date + pd.to_timedelta(df['date_index'], unit='D')\n",
        "        print(f\"   Using 'date_index' for Date column (base: 2017-01-01)\")\n",
        "    else:\n",
        "        print(\"   [Warning] No date column found. Using default range.\")\n",
        "        df['Date'] = pd.date_range(start='2017-01-01', periods=len(df), freq='D')\n",
        "    \n",
        "    print(f\"   Loaded {len(df)} rows, {len(df.columns)} columns\")\n",
        "    print(f\"   ✅ Date range: {df['Date'].min().strftime('%Y-%m-%d')} to {df['Date'].max().strftime('%Y-%m-%d')}\")\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preprocessing"
      },
      "source": [
        "## 5. Preprocessing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "preprocess"
      },
      "outputs": [],
      "source": [
        "def preprocess_and_split(df, target_col='value'):\n",
        "    \"\"\"\n",
        "    Preprocess data and split into train/valid/test\n",
        "    \n",
        "    Features to DROP:\n",
        "    - lag_1, lag_2, lag_3, lag_4, lag_5 (GRU learns temporal patterns)\n",
        "    - fg_value (look-ahead bias)\n",
        "    - metadata columns\n",
        "    \n",
        "    Features to KEEP:\n",
        "    - date_index (temporal trend)\n",
        "    - emb_* or pca_* (text embeddings)\n",
        "    - person_* (one-hot vectors)\n",
        "    - fg_lag_* (economic indicators, lagged)\n",
        "    \"\"\"\n",
        "    \n",
        "    # ========================================\n",
        "    # 1. Target Creation (next day value)\n",
        "    # ========================================\n",
        "    daily_prices = df[['date_index', target_col]].drop_duplicates().sort_values('date_index')\n",
        "    daily_prices['target'] = daily_prices[target_col].shift(-1)\n",
        "    \n",
        "    df = df.drop(columns=['target'], errors='ignore')\n",
        "    df = df.merge(daily_prices[['date_index', 'target']], on='date_index', how='left')\n",
        "    df = df.dropna(subset=['target'])\n",
        "    \n",
        "    print(f\"   After target creation: {len(df)} rows\")\n",
        "    \n",
        "    # ========================================\n",
        "    # 2. Sample Weight Calculation\n",
        "    # ========================================\n",
        "    date_counts = df['date_index'].value_counts()\n",
        "    df['sample_weight'] = df['date_index'].map(lambda x: 1.0 / date_counts[x])\n",
        "    \n",
        "    print(f\"   Sample weights: min={df['sample_weight'].min():.4f}, \"\n",
        "          f\"max={df['sample_weight'].max():.4f}, mean={df['sample_weight'].mean():.4f}\")\n",
        "    \n",
        "    # ========================================\n",
        "    # 3. Extract Date for splitting\n",
        "    # ========================================\n",
        "    if 'Date' not in df.columns:\n",
        "        raise ValueError(\"'Date' column not found in dataframe\")\n",
        "    \n",
        "    dates = df['Date'].copy()\n",
        "    date_indices = df['date_index'].copy()\n",
        "    df = df.drop(columns=['Date'])\n",
        "    \n",
        "    # ========================================\n",
        "    # 4. Columns to Drop\n",
        "    # ========================================\n",
        "    cols_to_drop = [\n",
        "        # Metadata\n",
        "        'person', 'person_id', 'article_id',\n",
        "        \n",
        "        # Date columns\n",
        "        'pub_date', 'article_date',\n",
        "        \n",
        "        # Target related\n",
        "        'value',      # Current stock price\n",
        "        'target',     # Target (will be extracted separately)\n",
        "        \n",
        "        # Lag features (GRU learns temporal patterns itself)\n",
        "        'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5',\n",
        "        \n",
        "        # Fear-Greed current value (look-ahead bias)\n",
        "        'fg_value',\n",
        "        \n",
        "        # Weight (will be extracted separately)\n",
        "        'sample_weight',\n",
        "    ]\n",
        "    \n",
        "    actual_drop = [c for c in cols_to_drop if c in df.columns]\n",
        "    print(f\"   Dropping columns: {actual_drop}\")\n",
        "    \n",
        "    # ========================================\n",
        "    # 5. Extract X, y, weights\n",
        "    # ========================================\n",
        "    X = df.drop(columns=actual_drop, errors='ignore')\n",
        "    y = df['target'].copy()\n",
        "    weights = df['sample_weight'].copy()\n",
        "    \n",
        "    # Remove datetime/object columns\n",
        "    datetime_cols = X.select_dtypes(include=['datetime64']).columns.tolist()\n",
        "    if datetime_cols:\n",
        "        print(f\"   ⚠️ Removing datetime columns: {datetime_cols}\")\n",
        "        X = X.drop(columns=datetime_cols)\n",
        "    \n",
        "    object_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
        "    if object_cols:\n",
        "        print(f\"   ⚠️ WARNING: Found non-numeric columns: {object_cols}\")\n",
        "        print(f\"   Removing them...\")\n",
        "        X = X.drop(columns=object_cols)\n",
        "    \n",
        "    print(f\"   ✅ Feature columns ({len(X.columns)}): {list(X.columns[:10])}...\")\n",
        "    print(f\"   ✅ All features are numeric: {X.dtypes.apply(lambda x: x.kind in 'biufc').all()}\")\n",
        "    \n",
        "    # ========================================\n",
        "    # 6. Train/Valid/Test Split\n",
        "    # ========================================\n",
        "    train_mask = (dates <= '2018-12-31')\n",
        "    valid_mask = (dates >= '2019-01-01') & (dates <= '2019-06-30')\n",
        "    test_mask = (dates >= '2019-07-01')\n",
        "    \n",
        "    print(f\"   Train: {train_mask.sum()} rows\")\n",
        "    print(f\"   Valid: {valid_mask.sum()} rows\")\n",
        "    print(f\"   Test:  {test_mask.sum()} rows\")\n",
        "    \n",
        "    if train_mask.sum() == 0 or test_mask.sum() == 0:\n",
        "        raise ValueError(\"Train or Test set is empty!\")\n",
        "    \n",
        "    return (\n",
        "        (X[train_mask], y[train_mask], weights[train_mask], date_indices[train_mask]),\n",
        "        (X[valid_mask], y[valid_mask], weights[valid_mask], date_indices[valid_mask]),\n",
        "        (X[test_mask], y[test_mask], weights[test_mask], date_indices[test_mask], dates[test_mask])\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset"
      },
      "source": [
        "## 6. Weighted Time Series Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataset_class"
      },
      "outputs": [],
      "source": [
        "class WeightedTimeSeriesDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Time series dataset with sample weighting\n",
        "    \n",
        "    Returns sequences of length seq_length with corresponding weights\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, weights, seq_length):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            X: (n_samples, n_features) numpy array\n",
        "            y: (n_samples,) numpy array\n",
        "            weights: (n_samples,) numpy array\n",
        "            seq_length: int, sequence length\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.weights = weights\n",
        "        self.seq_length = seq_length\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.X) - self.seq_length + 1\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            X_seq: (seq_length, n_features)\n",
        "            y_target: (1,)\n",
        "            weight: (1,)\n",
        "        \"\"\"\n",
        "        # Sequence: idx ~ idx+seq_length-1\n",
        "        X_seq = self.X[idx : idx + self.seq_length]\n",
        "        \n",
        "        # Target: last time step\n",
        "        y_target = self.y[idx + self.seq_length - 1]\n",
        "        \n",
        "        # Weight: last time step\n",
        "        weight = self.weights[idx + self.seq_length - 1]\n",
        "        \n",
        "        return (\n",
        "            torch.FloatTensor(X_seq),\n",
        "            torch.FloatTensor([y_target]),\n",
        "            torch.FloatTensor([weight])\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model"
      },
      "source": [
        "## 7. GRU Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model_class"
      },
      "outputs": [],
      "source": [
        "class GRUModel(nn.Module):\n",
        "    \"\"\"\n",
        "    GRU-based stock price prediction model\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim=64, num_layers=2, dropout=0.3):\n",
        "        super(GRUModel, self).__init__()\n",
        "        \n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.gru = nn.GRU(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch, seq_length, input_dim)\n",
        "        Returns:\n",
        "            out: (batch, 1)\n",
        "        \"\"\"\n",
        "        # GRU forward\n",
        "        gru_out, _ = self.gru(x)\n",
        "        \n",
        "        # Use last time step output\n",
        "        last_out = gru_out[:, -1, :]  # (batch, hidden_dim)\n",
        "        \n",
        "        # Fully connected layer\n",
        "        out = self.fc(last_out)  # (batch, 1)\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "early_stopping"
      },
      "source": [
        "## 8. Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "early_stopping_class"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"\n",
        "    Early stopping to stop training when validation loss doesn't improve\n",
        "    \"\"\"\n",
        "    def __init__(self, patience=10, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "    \n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss - self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training"
      },
      "source": [
        "## 9. Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_function"
      },
      "outputs": [],
      "source": [
        "def train_gru_model(dataset_name, file_path):\n",
        "    \"\"\"\n",
        "    Train GRU model with early stopping and sample weighting\n",
        "    \n",
        "    Returns:\n",
        "        test_mse: float\n",
        "    \"\"\"\n",
        "    \n",
        "    # ========================================\n",
        "    # 0. Check if result already exists\n",
        "    # ========================================\n",
        "    output_path = RESULTS_DIR / f\"pred_gru_{dataset_name}.json\"\n",
        "    \n",
        "    if output_path.exists():\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"⏭️  SKIPPING: {dataset_name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"   Result already exists: {output_path.name}\")\n",
        "        \n",
        "        try:\n",
        "            with open(output_path, 'r') as f:\n",
        "                result_data = json.load(f)\n",
        "            \n",
        "            actuals = [item['actual'] for item in result_data]\n",
        "            preds = [item['predicted'] for item in result_data]\n",
        "            mse = mean_squared_error(actuals, preds)\n",
        "            \n",
        "            print(f\"   ✅ Cached Test MSE: {mse:.4f}\")\n",
        "            return mse\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ⚠️ Warning: Could not read cached MSE: {e}\")\n",
        "            print(f\"   Re-running experiment...\")\n",
        "    \n",
        "    # ========================================\n",
        "    # 1. Load and preprocess data\n",
        "    # ========================================\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Processing: {dataset_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    df = load_data(file_path)\n",
        "    \n",
        "    (X_train, y_train, w_train, date_idx_train), \\\n",
        "    (X_valid, y_valid, w_valid, date_idx_valid), \\\n",
        "    (X_test, y_test, w_test, date_idx_test, dates_test) = preprocess_and_split(df)\n",
        "    \n",
        "    del df\n",
        "    gc.collect()\n",
        "    \n",
        "    # ========================================\n",
        "    # 2. Normalization (MinMaxScaler)\n",
        "    # ========================================\n",
        "    print(f\"\\n   Normalizing features...\")\n",
        "    \n",
        "    scaler_x = MinMaxScaler()\n",
        "    scaler_y = MinMaxScaler()\n",
        "    \n",
        "    X_train_scaled = scaler_x.fit_transform(X_train.values).astype(np.float32)\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).flatten().astype(np.float32)\n",
        "    \n",
        "    X_valid_scaled = scaler_x.transform(X_valid.values).astype(np.float32)\n",
        "    y_valid_scaled = scaler_y.transform(y_valid.values.reshape(-1, 1)).flatten().astype(np.float32)\n",
        "    \n",
        "    X_test_scaled = scaler_x.transform(X_test.values).astype(np.float32)\n",
        "    y_test_raw = y_test.values.astype(np.float32)\n",
        "    \n",
        "    w_train = w_train.values.astype(np.float32)\n",
        "    w_valid = w_valid.values.astype(np.float32)\n",
        "    w_test = w_test.values.astype(np.float32)\n",
        "    \n",
        "    print(f\"   ✅ Features normalized to [0, 1]\")\n",
        "    \n",
        "    # ========================================\n",
        "    # 3. Create datasets and dataloaders\n",
        "    # ========================================\n",
        "    seq_length = CONFIG['seq_length']\n",
        "    batch_size = CONFIG['batch_size']\n",
        "    \n",
        "    train_dataset = WeightedTimeSeriesDataset(X_train_scaled, y_train_scaled, w_train, seq_length)\n",
        "    valid_dataset = WeightedTimeSeriesDataset(X_valid_scaled, y_valid_scaled, w_valid, seq_length)\n",
        "    test_dataset = WeightedTimeSeriesDataset(X_test_scaled, y_test_raw, w_test, seq_length)\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "    \n",
        "    print(f\"\\n   Dataset sizes:\")\n",
        "    print(f\"   - Train: {len(train_dataset)} samples\")\n",
        "    print(f\"   - Valid: {len(valid_dataset)} samples\")\n",
        "    print(f\"   - Test: {len(test_dataset)} samples\")\n",
        "    \n",
        "    # ========================================\n",
        "    # 4. Initialize model\n",
        "    # ========================================\n",
        "    input_dim = X_train_scaled.shape[1]\n",
        "    \n",
        "    model = GRUModel(\n",
        "        input_dim=input_dim,\n",
        "        hidden_dim=CONFIG['hidden_dim'],\n",
        "        num_layers=CONFIG['num_layers'],\n",
        "        dropout=CONFIG['dropout']\n",
        "    ).to(DEVICE)\n",
        "    \n",
        "    criterion = nn.MSELoss(reduction='none')\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])\n",
        "    early_stopping = EarlyStopping(patience=CONFIG['early_stopping_patience'])\n",
        "    \n",
        "    print(f\"\\n   Model initialized:\")\n",
        "    print(f\"   - Input dim: {input_dim}\")\n",
        "    print(f\"   - Hidden dim: {CONFIG['hidden_dim']}\")\n",
        "    print(f\"   - Num layers: {CONFIG['num_layers']}\")\n",
        "    print(f\"   - Dropout: {CONFIG['dropout']}\")\n",
        "    print(f\"   - Parameters: {sum(p.numel() for p in model.parameters())}\")\n",
        "    \n",
        "    # ========================================\n",
        "    # 5. Training loop\n",
        "    # ========================================\n",
        "    print(f\"\\n   Training started...\")\n",
        "    \n",
        "    best_val_loss = float('inf')\n",
        "    \n",
        "    for epoch in range(CONFIG['epochs']):\n",
        "        # Train\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_batches = 0\n",
        "        \n",
        "        for X_batch, y_batch, w_batch in train_loader:\n",
        "            X_batch = X_batch.to(DEVICE)\n",
        "            y_batch = y_batch.to(DEVICE)\n",
        "            w_batch = w_batch.to(DEVICE)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward\n",
        "            predictions = model(X_batch)\n",
        "            \n",
        "            # Weighted MSE loss\n",
        "            losses = criterion(predictions, y_batch)\n",
        "            weighted_loss = (losses * w_batch).mean()\n",
        "            \n",
        "            # Backward\n",
        "            weighted_loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += weighted_loss.item()\n",
        "            train_batches += 1\n",
        "        \n",
        "        train_loss /= train_batches\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_batches = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch, w_batch in valid_loader:\n",
        "                X_batch = X_batch.to(DEVICE)\n",
        "                y_batch = y_batch.to(DEVICE)\n",
        "                w_batch = w_batch.to(DEVICE)\n",
        "                \n",
        "                predictions = model(X_batch)\n",
        "                losses = criterion(predictions, y_batch)\n",
        "                weighted_loss = (losses * w_batch).mean()\n",
        "                \n",
        "                val_loss += weighted_loss.item()\n",
        "                val_batches += 1\n",
        "        \n",
        "        val_loss /= val_batches\n",
        "        \n",
        "        # Logging\n",
        "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "            print(f\"   Epoch [{epoch+1:3d}/{CONFIG['epochs']}] \"\n",
        "                  f\"Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
        "        \n",
        "        # Early stopping\n",
        "        early_stopping(val_loss)\n",
        "        if early_stopping.early_stop:\n",
        "            print(f\"\\n   ⚠️ Early stopping triggered at epoch {epoch+1}\")\n",
        "            break\n",
        "        \n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "    \n",
        "    print(f\"\\n   ✅ Training completed\")\n",
        "    print(f\"   ✅ Best validation loss: {best_val_loss:.6f}\")\n",
        "    \n",
        "    # ========================================\n",
        "    # 6. Test prediction\n",
        "    # ========================================\n",
        "    print(f\"\\n   Predicting on test set...\")\n",
        "    \n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for X_batch, _, _ in test_loader:\n",
        "            X_batch = X_batch.to(DEVICE)\n",
        "            predictions = model(X_batch)\n",
        "            all_preds.append(predictions.cpu().numpy())\n",
        "    \n",
        "    test_preds_scaled = np.vstack(all_preds).flatten()\n",
        "    test_preds = scaler_y.inverse_transform(test_preds_scaled.reshape(-1, 1)).flatten()\n",
        "    \n",
        "    # Align test data (first seq_length-1 samples are lost)\n",
        "    y_test_aligned = y_test_raw[seq_length-1:]\n",
        "    date_idx_test_aligned = date_idx_test.values[seq_length-1:]\n",
        "    dates_test_aligned = dates_test.values[seq_length-1:]\n",
        "    \n",
        "    # ========================================\n",
        "    # 7. Daily aggregation (article-level → daily)\n",
        "    # ========================================\n",
        "    results_df = pd.DataFrame({\n",
        "        'date_index': date_idx_test_aligned,\n",
        "        'date': dates_test_aligned,\n",
        "        'actual': y_test_aligned,\n",
        "        'predicted': test_preds\n",
        "    })\n",
        "    \n",
        "    # Aggregate by date\n",
        "    daily_results = results_df.groupby('date_index').agg({\n",
        "        'date': 'first',\n",
        "        'actual': 'first',  # Same value for all articles on same day\n",
        "        'predicted': 'mean'  # Average predictions\n",
        "    }).reset_index()\n",
        "    \n",
        "    # Calculate MSE\n",
        "    test_mse = mean_squared_error(daily_results['actual'], daily_results['predicted'])\n",
        "    \n",
        "    print(f\"\\n   ✅ Test MSE (Daily): {test_mse:.4f}\")\n",
        "    \n",
        "    # ========================================\n",
        "    # 8. Save results\n",
        "    # ========================================\n",
        "    result_data = []\n",
        "    for _, row in daily_results.iterrows():\n",
        "        result_data.append({\n",
        "            \"date\": pd.to_datetime(row['date']).strftime('%Y-%m-%d'),\n",
        "            \"actual\": float(row['actual']),\n",
        "            \"predicted\": float(row['predicted'])\n",
        "        })\n",
        "    \n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(result_data, f, indent=4)\n",
        "    \n",
        "    print(f\"   Saved predictions to: results/{output_path.name}\")\n",
        "    \n",
        "    # Memory cleanup\n",
        "    del model, optimizer, train_loader, valid_loader, test_loader\n",
        "    del X_train_scaled, X_valid_scaled, X_test_scaled\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    \n",
        "    return test_mse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "main"
      },
      "source": [
        "## 10. Main Execution Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "main_execution"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# Main Execution\n",
        "# ========================================\n",
        "\n",
        "levels = ['B', 'C', 'D']\n",
        "methods = ['headlines', 'chunking', 'bodyText', 'paragraphs']\n",
        "types = ['pca', 'orig']\n",
        "\n",
        "metrics_list = []\n",
        "\n",
        "# ========================================\n",
        "# Dataset A (Baseline)\n",
        "# ========================================\n",
        "path_A = DATA_DIR / \"dataset_A.parquet\"\n",
        "if path_A.exists():\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"BASELINE: Dataset A\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    try:\n",
        "        mse = train_gru_model(\"A\", path_A)\n",
        "        metrics_list.append({\n",
        "            \"Feature_set\": \"A\",\n",
        "            \"Embeddings\": \"-\",\n",
        "            \"Dim_reduction\": \"-\",\n",
        "            \"Model\": \"GRU\",\n",
        "            \"MSE\": mse\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error on Dataset A: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    finally:\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "else:\n",
        "    print(f\"Warning: {path_A} not found. Skipping Dataset A.\")\n",
        "\n",
        "# ========================================\n",
        "# Datasets B, C, D\n",
        "# ========================================\n",
        "total_datasets = len(levels) * len(methods) * len(types)\n",
        "current = 0\n",
        "\n",
        "for level in levels:\n",
        "    for method in methods:\n",
        "        for dtype in types:\n",
        "            current += 1\n",
        "            fname = f\"dataset_{level}_{method}_{dtype}.parquet\"\n",
        "            fpath = DATA_DIR / fname\n",
        "            \n",
        "            if not fpath.exists():\n",
        "                print(f\"\\n[{current}/{total_datasets}] Skipping {fname}: File not found.\")\n",
        "                continue\n",
        "            \n",
        "            dname = f\"{level}_{method}_{dtype}\"\n",
        "            \n",
        "            try:\n",
        "                mse = train_gru_model(dname, fpath)\n",
        "                metrics_list.append({\n",
        "                    \"Feature_set\": level,\n",
        "                    \"Embeddings\": method,\n",
        "                    \"Dim_reduction\": dtype,\n",
        "                    \"Model\": \"GRU\",\n",
        "                    \"MSE\": mse\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"\\n❌ Error on {dname}: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "            finally:\n",
        "                gc.collect()\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "# ========================================\n",
        "# Save final results\n",
        "# ========================================\n",
        "final_df = pd.DataFrame(metrics_list).sort_values(\"Test_MSE\")\n",
        "csv_path = OUTPUT_DIR / \"gru_evaluation_metrics.csv\"\n",
        "final_df.to_csv(csv_path, index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ALL TASKS COMPLETED\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nResults saved to: {csv_path}\")\n",
        "print(f\"\\nTop 10 Models by Test MSE:\")\n",
        "print(final_df.head(10))\n",
        "\n",
        "# 총 51분 정도"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mlproject",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
