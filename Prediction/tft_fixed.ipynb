{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFT (Temporal Fusion Transformer) Training\n",
    "\n",
    "25개 dataset variants에 대해 TFT 모델을 학습하고 MSE를 평가합니다.\n",
    "\n",
    "## 필요 패키지\n",
    "```bash\n",
    "pip install pytorch-forecasting lightning pandas numpy scikit-learn pyarrow torch\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu128\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "# ==================== IMPORTS ====================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import json\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gc\n",
    "\n",
    "# PyTorch Forecasting\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import QuantileLoss, MAE, RMSE\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration loaded!\n"
     ]
    }
   ],
   "source": [
    "# ==================== CONFIGURATION ====================\n",
    "CONFIG = {\n",
    "    # Paths\n",
    "    'data_dir': Path('../feature_datasets'),\n",
    "    'output_file': Path('./output_tft.csv'),\n",
    "    'log_dir': Path('./tft_logs'),\n",
    "    'checkpoint_dir': Path('./tft_checkpoints'),\n",
    "    \n",
    "    # Data settings\n",
    "    'remove_columns': ['person', 'article_id', 'pub_date', 'article_date', 'idx', 'date_str', 'person_id', 'headline'],\n",
    "    \n",
    "    # Time series settings\n",
    "    'time_idx': 'date_index',\n",
    "    'target': 'value',\n",
    "    'max_encoder_length': 30,\n",
    "    'max_prediction_length': 5,\n",
    "    \n",
    "    # Model hyperparameters\n",
    "    'hidden_size': 128,\n",
    "    'lstm_layers': 2,\n",
    "    'attention_head_size': 4,\n",
    "    'dropout': 0.1,\n",
    "    'hidden_continuous_size': 32,\n",
    "    \n",
    "    # Training settings\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 1e-3,\n",
    "    'max_epochs': 50,\n",
    "    'gradient_clip_val': 0.1,\n",
    "    'patience': 10,\n",
    "    \n",
    "    # Loss & metrics\n",
    "    'quantiles': [0.1, 0.5, 0.9],\n",
    "}\n",
    "\n",
    "# Create output directories\n",
    "CONFIG['log_dir'].mkdir(exist_ok=True, parents=True)\n",
    "CONFIG['checkpoint_dir'].mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"✓ Configuration loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All functions loaded!\n"
     ]
    }
   ],
   "source": [
    "# ==================== HELPER FUNCTIONS ====================\n",
    "\n",
    "def get_dataset_filename(variant: Dict) -> str:\n",
    "    if variant['type'] == 'A':\n",
    "        return 'dataset_A.parquet'\n",
    "    else:\n",
    "        emb = variant['embedding']\n",
    "        pca = variant['pca']\n",
    "        return f\"dataset_{variant['type']}_{emb}_{pca}.parquet\"\n",
    "\n",
    "def get_variant_name(variant: Dict) -> str:\n",
    "    if variant['type'] == 'A':\n",
    "        return 'A_sp500_only'\n",
    "    else:\n",
    "        return f\"{variant['type']}_{variant['embedding']}_{variant['pca']}\"\n",
    "\n",
    "def calculate_mse(actual: np.ndarray, predicted: np.ndarray) -> float:\n",
    "    return np.mean((actual - predicted) ** 2)\n",
    "\n",
    "def load_and_preprocess_data(filepath: Path, variant: Dict, config: Dict) -> pd.DataFrame:\n",
    "    print(f\"  Loading: {filepath.name}\")\n",
    "    df = pd.read_parquet(filepath)\n",
    "    print(f\"  Original shape: {df.shape}\")\n",
    "    \n",
    "    # Remove unnecessary columns\n",
    "    cols_to_remove = [col for col in config['remove_columns'] if col in df.columns]\n",
    "    df = df.drop(columns=cols_to_remove)\n",
    "    print(f\"  After removing columns: {df.shape}\")\n",
    "    \n",
    "    # Handle embedding column\n",
    "    if 'embedding' in df.columns:\n",
    "        print(f\"  Expanding embedding column...\")\n",
    "        emb_df = pd.DataFrame(df['embedding'].tolist(), \n",
    "                            index=df.index,\n",
    "                            columns=[f'emb_{i}' for i in range(len(df['embedding'].iloc[0]))])\n",
    "        df = pd.concat([df.drop('embedding', axis=1), emb_df], axis=1)\n",
    "        print(f\"  Expanded to {emb_df.shape[1]} columns\")\n",
    "    \n",
    "    # Ensure date_index is integer\n",
    "    if config['time_idx'] in df.columns:\n",
    "        df[config['time_idx']] = df[config['time_idx']].astype(int)\n",
    "    \n",
    "    # AGGREGATION BY DATE (for B, C, D)\n",
    "    if variant['type'] != 'A':\n",
    "        print(f\"  Aggregating by date...\")\n",
    "        agg_dict = {}\n",
    "        for col in df.columns:\n",
    "            if col == config['time_idx']:\n",
    "                continue\n",
    "            elif pd.api.types.is_numeric_dtype(df[col]):\n",
    "                agg_dict[col] = 'mean'\n",
    "            else:\n",
    "                agg_dict[col] = 'first'\n",
    "        df = df.groupby(config['time_idx']).agg(agg_dict).reset_index()\n",
    "        print(f\"  After aggregation: {len(df)} rows\")\n",
    "    \n",
    "    # Sort by date_index\n",
    "    df = df.sort_values(config['time_idx']).reset_index(drop=True)\n",
    "    print(f\"  Final shape: {df.shape}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def split_data_by_date(df: pd.DataFrame, config: Dict) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    unique_dates = sorted(df[config['time_idx']].unique())\n",
    "    total_days = len(unique_dates)\n",
    "    print(f\"  Total unique dates: {total_days}\")\n",
    "    \n",
    "    # 66.7% / 16.7% / 16.7%\n",
    "    train_end_idx = int(total_days * 0.667)\n",
    "    valid_end_idx = int(total_days * 0.833)\n",
    "    \n",
    "    train_dates = unique_dates[:train_end_idx]\n",
    "    valid_dates = unique_dates[train_end_idx:valid_end_idx]\n",
    "    test_dates = unique_dates[valid_end_idx:]\n",
    "    \n",
    "    train_df = df[df[config['time_idx']].isin(train_dates)].copy()\n",
    "    valid_df = df[df[config['time_idx']].isin(valid_dates)].copy()\n",
    "    test_df = df[df[config['time_idx']].isin(test_dates)].copy()\n",
    "    \n",
    "    print(f\"  Train: {len(train_df)} rows, {len(train_dates)} dates\")\n",
    "    print(f\"  Valid: {len(valid_df)} rows, {len(valid_dates)} dates\")\n",
    "    print(f\"  Test: {len(test_df)} rows, {len(test_dates)} dates\")\n",
    "    \n",
    "    return train_df, valid_df, test_df\n",
    "\n",
    "def create_time_series_dataset(\n",
    "    df: pd.DataFrame,\n",
    "    config: Dict,\n",
    "    variant: Dict,\n",
    ") -> TimeSeriesDataSet:\n",
    "    # NOTE: series_id must be already added to df!\n",
    "    \n",
    "    # Identify features\n",
    "    time_varying_unknown = []\n",
    "    static_reals = []\n",
    "    \n",
    "    exclude_cols = {config['time_idx'], config['target'], 'series_id'}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col in exclude_cols:\n",
    "            continue\n",
    "        elif col.startswith('person_'):\n",
    "            static_reals.append(col)\n",
    "        elif pd.api.types.is_numeric_dtype(df[col]):\n",
    "            time_varying_unknown.append(col)\n",
    "    \n",
    "    time_varying_unknown = sorted(list(set(time_varying_unknown)))\n",
    "    static_reals = sorted(list(set(static_reals)))\n",
    "    \n",
    "    print(f\"  Time-varying unknown: {len(time_varying_unknown)}\")\n",
    "    print(f\"  Static reals: {len(static_reals)}\")\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = TimeSeriesDataSet(\n",
    "        df,\n",
    "        time_idx=config['time_idx'],\n",
    "        target=config['target'],\n",
    "        group_ids=['series_id'],\n",
    "        max_encoder_length=config['max_encoder_length'],\n",
    "        max_prediction_length=config['max_prediction_length'],\n",
    "        static_reals=static_reals if static_reals else None,\n",
    "        time_varying_unknown_reals=time_varying_unknown if time_varying_unknown else None,\n",
    "        target_normalizer=GroupNormalizer(\n",
    "            groups=['series_id'], \n",
    "            transformation=\"softplus\"\n",
    "        ),\n",
    "        add_relative_time_idx=True,\n",
    "        add_target_scales=True,\n",
    "        add_encoder_length=True,\n",
    "        allow_missing_timesteps=False,\n",
    "    )\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def train_tft_model(\n",
    "    train_dataset: TimeSeriesDataSet,\n",
    "    valid_dataset: TimeSeriesDataSet,\n",
    "    config: Dict,\n",
    "    variant: Dict,\n",
    ") -> TemporalFusionTransformer:\n",
    "    # Create dataloaders\n",
    "    train_dataloader = train_dataset.to_dataloader(\n",
    "        train=True, \n",
    "        batch_size=config['batch_size'],\n",
    "        num_workers=0\n",
    "    )\n",
    "    valid_dataloader = valid_dataset.to_dataloader(\n",
    "        train=False,\n",
    "        batch_size=config['batch_size'] * 2,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    # Define model\n",
    "    tft = TemporalFusionTransformer.from_dataset(\n",
    "        train_dataset,\n",
    "        learning_rate=config['learning_rate'],\n",
    "        hidden_size=config['hidden_size'],\n",
    "        attention_head_size=config['attention_head_size'],\n",
    "        dropout=config['dropout'],\n",
    "        hidden_continuous_size=config['hidden_continuous_size'],\n",
    "        lstm_layers=config['lstm_layers'],\n",
    "        loss=QuantileLoss(quantiles=config['quantiles']),\n",
    "        reduce_on_plateau_patience=4,\n",
    "    )\n",
    "    \n",
    "    print(f\"  Model parameters: {sum(p.numel() for p in tft.parameters()):,}\")\n",
    "    \n",
    "    # Setup callbacks\n",
    "    variant_name = get_variant_name(variant)\n",
    "    \n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        min_delta=1e-4,\n",
    "        patience=config['patience'],\n",
    "        verbose=False,\n",
    "        mode=\"min\"\n",
    "    )\n",
    "    \n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        dirpath=config['checkpoint_dir'] / variant_name,\n",
    "        filename='best',\n",
    "        save_top_k=1,\n",
    "        mode='min',\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=config['max_epochs'],\n",
    "        accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "        devices=1,\n",
    "        gradient_clip_val=config['gradient_clip_val'],\n",
    "        callbacks=[early_stop_callback, checkpoint_callback],\n",
    "        enable_progress_bar=True,\n",
    "        enable_model_summary=False,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(\"  Training...\")\n",
    "    trainer.fit(\n",
    "        tft,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=valid_dataloader,\n",
    "    )\n",
    "    \n",
    "    # Load best model\n",
    "    best_model_path = checkpoint_callback.best_model_path\n",
    "    best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "    \n",
    "    return best_tft\n",
    "\n",
    "def predict_and_evaluate(\n",
    "    model: TemporalFusionTransformer,\n",
    "    dataset: TimeSeriesDataSet,\n",
    "    config: Dict,\n",
    ") -> float:\n",
    "    dataloader = dataset.to_dataloader(\n",
    "        train=False,\n",
    "        batch_size=config['batch_size'] * 4,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions, x = model.predict(dataloader, mode=\"prediction\", return_x=True, return_index=True)\n",
    "    \n",
    "    # Extract median (index 1)\n",
    "    pred_raw = predictions[:, :, 1].cpu().numpy()\n",
    "    actual_raw = x[\"decoder_target\"].cpu().numpy()\n",
    "    \n",
    "    # Flatten\n",
    "    actual_flat = actual_raw.flatten()\n",
    "    pred_flat = pred_raw.flatten()\n",
    "    \n",
    "    # Remove NaN\n",
    "    valid_mask = ~(np.isnan(actual_flat) | np.isnan(pred_flat))\n",
    "    actual_clean = actual_flat[valid_mask]\n",
    "    pred_clean = pred_flat[valid_mask]\n",
    "    \n",
    "    # Calculate MSE\n",
    "    mse = calculate_mse(actual_clean, pred_clean)\n",
    "    \n",
    "    print(f\"  Valid predictions: {len(actual_clean)}\")\n",
    "    print(f\"  Test MSE: {mse:.8f}\")\n",
    "    \n",
    "    return mse\n",
    "\n",
    "print(\"✓ All functions loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total variants: 25\n"
     ]
    }
   ],
   "source": [
    "# ==================== DATASET VARIANTS ====================\n",
    "\n",
    "DATASET_VARIANTS = [\n",
    "    # A: SP500만 (1가지)\n",
    "    {'type': 'A', 'embedding': None, 'pca': None},\n",
    "    \n",
    "    # B: SP500 + embeddings (8가지)\n",
    "    {'type': 'B', 'embedding': 'headlines', 'pca': 'orig'},\n",
    "    {'type': 'B', 'embedding': 'headlines', 'pca': 'pca'},\n",
    "    {'type': 'B', 'embedding': 'chunking', 'pca': 'orig'},\n",
    "    {'type': 'B', 'embedding': 'chunking', 'pca': 'pca'},\n",
    "    {'type': 'B', 'embedding': 'bodyText', 'pca': 'orig'},\n",
    "    {'type': 'B', 'embedding': 'bodyText', 'pca': 'pca'},\n",
    "    {'type': 'B', 'embedding': 'paragraphs', 'pca': 'orig'},\n",
    "    {'type': 'B', 'embedding': 'paragraphs', 'pca': 'pca'},\n",
    "    \n",
    "    # C: B + person one-hot (8가지)\n",
    "    {'type': 'C', 'embedding': 'headlines', 'pca': 'orig'},\n",
    "    {'type': 'C', 'embedding': 'headlines', 'pca': 'pca'},\n",
    "    {'type': 'C', 'embedding': 'chunking', 'pca': 'orig'},\n",
    "    {'type': 'C', 'embedding': 'chunking', 'pca': 'pca'},\n",
    "    {'type': 'C', 'embedding': 'bodyText', 'pca': 'orig'},\n",
    "    {'type': 'C', 'embedding': 'bodyText', 'pca': 'pca'},\n",
    "    {'type': 'C', 'embedding': 'paragraphs', 'pca': 'orig'},\n",
    "    {'type': 'C', 'embedding': 'paragraphs', 'pca': 'pca'},\n",
    "    \n",
    "    # D: C + FG index (8가지)\n",
    "    {'type': 'D', 'embedding': 'headlines', 'pca': 'orig'},\n",
    "    {'type': 'D', 'embedding': 'headlines', 'pca': 'pca'},\n",
    "    {'type': 'D', 'embedding': 'chunking', 'pca': 'orig'},\n",
    "    {'type': 'D', 'embedding': 'chunking', 'pca': 'pca'},\n",
    "    {'type': 'D', 'embedding': 'bodyText', 'pca': 'orig'},\n",
    "    {'type': 'D', 'embedding': 'bodyText', 'pca': 'pca'},\n",
    "    {'type': 'D', 'embedding': 'paragraphs', 'pca': 'orig'},\n",
    "    {'type': 'D', 'embedding': 'paragraphs', 'pca': 'pca'},\n",
    "]\n",
    "\n",
    "print(f\"Total variants: {len(DATASET_VARIANTS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전체 학습 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Starting TFT training for 25 variants\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Variant 1/25: A_sp500_only\n",
      "================================================================================\n",
      "  Loading: dataset_A.parquet\n",
      "  Original shape: (754, 8)\n",
      "  After removing columns: (754, 7)\n",
      "  Final shape: (754, 7)\n",
      "\n",
      "  Splitting data...\n",
      "  Total unique dates: 754\n",
      "  Train: 502 rows, 502 dates\n",
      "  Valid: 126 rows, 126 dates\n",
      "  Test: 126 rows, 126 dates\n",
      "\n",
      "  Creating TimeSeriesDataSet...\n",
      "  Time-varying unknown: 5\n",
      "  Static reals: 0\n",
      "\n",
      "  Training TFT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model parameters: 1,199,067\n",
      "  Training...\n",
      "Epoch 15: 100%|██████████| 14/14 [00:01<00:00,  8.48it/s, v_num=0, train_loss_step=17.70, val_loss=66.40, train_loss_epoch=19.20]\n",
      "  ERROR: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL pytorch_forecasting.data.encoders.GroupNormalizer was not an allowed global by default. Please use `torch.serialization.add_safe_globals([pytorch_forecasting.data.encoders.GroupNormalizer])` or the `torch.serialization.safe_globals([pytorch_forecasting.data.encoders.GroupNormalizer])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "\n",
      "================================================================================\n",
      "Variant 2/25: B_headlines_orig\n",
      "================================================================================\n",
      "  Loading: dataset_B_headlines_orig.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1048944/2841510984.py\", line 42, in <module>\n",
      "    model = train_tft_model(train_dataset, valid_dataset, CONFIG, variant)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_1048944/3957883494.py\", line 208, in train_tft_model\n",
      "    best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/wonjun/.conda/envs/mlproject/lib/python3.11/site-packages/lightning/pytorch/utilities/model_helpers.py\", line 130, in wrapper\n",
      "    return self.method(cls_type, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/wonjun/.conda/envs/mlproject/lib/python3.11/site-packages/lightning/pytorch/core/module.py\", line 1781, in load_from_checkpoint\n",
      "    loaded = _load_from_checkpoint(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/wonjun/.conda/envs/mlproject/lib/python3.11/site-packages/lightning/pytorch/core/saving.py\", line 65, in _load_from_checkpoint\n",
      "    checkpoint = pl_load(checkpoint_path, map_location=map_location, weights_only=weights_only)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/wonjun/.conda/envs/mlproject/lib/python3.11/site-packages/lightning/fabric/utilities/cloud_io.py\", line 73, in _load\n",
      "    return torch.load(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/wonjun/.conda/envs/mlproject/lib/python3.11/site-packages/torch/serialization.py\", line 1529, in load\n",
      "    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n",
      "_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL pytorch_forecasting.data.encoders.GroupNormalizer was not an allowed global by default. Please use `torch.serialization.add_safe_globals([pytorch_forecasting.data.encoders.GroupNormalizer])` or the `torch.serialization.safe_globals([pytorch_forecasting.data.encoders.GroupNormalizer])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Original shape: (461270, 16)\n",
      "  After removing columns: (461270, 8)\n",
      "  Expanding embedding column...\n",
      "  Expanded to 1024 columns\n",
      "  Aggregating by date...\n",
      "  After aggregation: 754 rows\n",
      "  Final shape: (754, 1031)\n",
      "\n",
      "  Splitting data...\n",
      "  Total unique dates: 754\n",
      "  Train: 502 rows, 502 dates\n",
      "  Valid: 126 rows, 126 dates\n",
      "  Test: 126 rows, 126 dates\n",
      "\n",
      "  Creating TimeSeriesDataSet...\n",
      "  Time-varying unknown: 1029\n",
      "  Static reals: 0\n",
      "\n",
      "  Training TFT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model parameters: 17,254,115\n",
      "  Training...\n",
      "Epoch 2:   7%|▋         | 1/14 [00:02<00:30,  0.42it/s, v_num=1, train_loss_step=28.20, val_loss=86.60, train_loss_epoch=55.10] "
     ]
    }
   ],
   "source": [
    "# ==================== MAIN TRAINING LOOP ====================\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"Starting TFT training for {len(DATASET_VARIANTS)} variants\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, variant in enumerate(DATASET_VARIANTS, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Variant {i}/{len(DATASET_VARIANTS)}: {get_variant_name(variant)}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Load data\n",
    "        filename = get_dataset_filename(variant)\n",
    "        filepath = CONFIG['data_dir'] / filename\n",
    "        \n",
    "        if not filepath.exists():\n",
    "            print(f\"  WARNING: File not found: {filepath}\")\n",
    "            continue\n",
    "        \n",
    "        df = load_and_preprocess_data(filepath, variant, CONFIG)\n",
    "        \n",
    "        # 2. Split data\n",
    "        print(\"\\n  Splitting data...\")\n",
    "        train_df, valid_df, test_df = split_data_by_date(df, CONFIG)\n",
    "        \n",
    "        # 3. Add series_id (CRITICAL!)\n",
    "        train_df['series_id'] = 0\n",
    "        valid_df['series_id'] = 0\n",
    "        test_df['series_id'] = 0\n",
    "        \n",
    "        # 4. Create datasets\n",
    "        print(\"\\n  Creating TimeSeriesDataSet...\")\n",
    "        train_dataset = create_time_series_dataset(train_df, CONFIG, variant)\n",
    "        valid_dataset = TimeSeriesDataSet.from_dataset(train_dataset, valid_df, predict=True, stop_randomization=True)\n",
    "        test_dataset = TimeSeriesDataSet.from_dataset(train_dataset, test_df, predict=True, stop_randomization=True)\n",
    "        \n",
    "        # 5. Train model\n",
    "        print(\"\\n  Training TFT model...\")\n",
    "        model = train_tft_model(train_dataset, valid_dataset, CONFIG, variant)\n",
    "        \n",
    "        # 6. Evaluate\n",
    "        print(\"\\n  Evaluating...\")\n",
    "        test_mse = predict_and_evaluate(model, test_dataset, CONFIG)\n",
    "        \n",
    "        # 7. Save result\n",
    "        result = {\n",
    "            'Dataset': variant['type'],\n",
    "            'Method': variant.get('embedding', '-'),\n",
    "            'Type': variant.get('pca', '-'),\n",
    "            'Model': 'TFT',\n",
    "            'MSE': test_mse\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"\\n  ✓ Completed: MSE={test_mse:.8f}\")\n",
    "        \n",
    "        # Clean up\n",
    "        del df, train_df, valid_df, test_df\n",
    "        del train_dataset, valid_dataset, test_dataset, model\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Training completed!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== SAVE RESULTS ====================\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('MSE').reset_index(drop=True)\n",
    "results_df.to_csv(CONFIG['output_file'], index=False)\n",
    "\n",
    "print(f\"\\n✓ Results saved to: {CONFIG['output_file']}\")\n",
    "print(f\"\\nResults:\")\n",
    "print(results_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== SUMMARY ====================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nTotal variants: {len(results_df)}\")\n",
    "print(f\"\\nBest Model:\")\n",
    "best = results_df.iloc[0]\n",
    "print(f\"  Dataset: {best['Dataset']}\")\n",
    "print(f\"  Method: {best['Method']}\")\n",
    "print(f\"  Type: {best['Type']}\")\n",
    "print(f\"  MSE: {best['MSE']:.8f}\")\n",
    "\n",
    "print(f\"\\nTop 5:\")\n",
    "print(results_df.head(5).to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
