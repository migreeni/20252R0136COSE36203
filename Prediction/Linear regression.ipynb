{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHnDibxQeOWz"
      },
      "source": [
        "### **Linear Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94BbcRQ-eJae"
      },
      "source": [
        "**0. Import**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Bm3z6s5ObJin"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOYzkY32bww4"
      },
      "source": [
        "**1. 경로 설정**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "dl0U29klbyyG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Source: ../feature_datasets\n",
            "Output Path: results_lr\n"
          ]
        }
      ],
      "source": [
        "DATA_DIR = Path(\"../feature_datasets\")\n",
        "OUTPUT_DIR = Path(\"results_lr\")\n",
        "RESULTS_DIR = OUTPUT_DIR / \"results\"  \n",
        "\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "RESULTS_DIR.mkdir(parents=True, exist_ok=True) \n",
        "\n",
        "\n",
        "print(f\"Data Source: {DATA_DIR}\")\n",
        "print(f\"Output Path: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvByCANab0-K"
      },
      "source": [
        "**2. 데이터 로드**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "aUEGo0z2b2Ro"
      },
      "outputs": [],
      "source": [
        "def load_data(file_path):\n",
        "    \"\"\"Parquet 파일 로드\"\"\"\n",
        "    print(f\"Loading {file_path.name}...\")\n",
        "    df = pd.read_parquet(file_path)\n",
        "    \n",
        "    # 날짜순 정렬\n",
        "    if 'date_index' in df.columns:\n",
        "        df = df.sort_values('date_index').reset_index(drop=True)\n",
        "    \n",
        "    # pub_date → Date 변환\n",
        "    if 'pub_date' not in df.columns:\n",
        "        raise ValueError(f\"'pub_date' column not found in {file_path.name}\")\n",
        "    \n",
        "    df['Date'] = pd.to_datetime(df['pub_date'], format='%Y_%m_%d')\n",
        "    \n",
        "    print(f\"   Loaded {len(df)} rows, {len(df.columns)} columns\")\n",
        "    print(f\"   Date range: {df['Date'].min().strftime('%Y-%m-%d')} to {df['Date'].max().strftime('%Y-%m-%d')}\")\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XB5E2PUyb8wH"
      },
      "source": [
        "**3. 전처리 & 가중치 생성 함수**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4Go3h3xlb_n8"
      },
      "outputs": [],
      "source": [
        "def preprocess_and_split(df, target_col='value'):\n",
        "    \"\"\"\n",
        "    전처리 및 Train/Valid/Test 분할\n",
        "    \"\"\"\n",
        "    \n",
        "    # 1. Target 생성\n",
        "    daily_prices = df[['date_index', target_col]].drop_duplicates().sort_values('date_index')\n",
        "    daily_prices['target'] = daily_prices[target_col].shift(-1)\n",
        "    \n",
        "    df = df.drop(columns=['target'], errors='ignore')\n",
        "    df = df.merge(daily_prices[['date_index', 'target']], on='date_index', how='left')\n",
        "    df = df.dropna(subset=['target'])\n",
        "    \n",
        "    print(f\"   After target creation: {len(df)} rows\")\n",
        "    \n",
        "    # 2. Sample Weight 계산\n",
        "    date_counts = df['date_index'].value_counts()\n",
        "    df['sample_weight'] = df['date_index'].map(lambda x: 1.0 / date_counts[x])\n",
        "    \n",
        "    print(f\"   Sample weights: min={df['sample_weight'].min():.4f}, \"\n",
        "          f\"max={df['sample_weight'].max():.4f}, mean={df['sample_weight'].mean():.4f}\")\n",
        "    \n",
        "    # 3. Date 먼저 추출 (split용)\n",
        "    if 'Date' not in df.columns:\n",
        "        raise ValueError(\"'Date' column not found in dataframe\")\n",
        "    \n",
        "    dates = df['Date'].copy()\n",
        "    \n",
        "    # 즉시 Date 컬럼 제거\n",
        "    df = df.drop(columns=['Date'])\n",
        "    \n",
        "    # 4. 드롭할 컬럼 정의\n",
        "    # ========================================\n",
        "    cols_to_drop = [\n",
        "        # 메타데이터\n",
        "        'person',\n",
        "        'person_id', \n",
        "        'article_id',\n",
        "        \n",
        "        # 날짜 (이미 제거됨)\n",
        "        'pub_date',\n",
        "        # 'Date',  ← 이미 위에서 제거했으므로 불필요\n",
        "        \n",
        "        # Target 관련\n",
        "        'value',\n",
        "        'target',     # y로 사용 (X에서만 제외)\n",
        "        \n",
        "        # Fear-Greed\n",
        "        'fg_value',\n",
        "        \n",
        "        # Weight\n",
        "        'sample_weight',\n",
        "    ]\n",
        "    \n",
        "    # 실제 존재하는 컬럼만 필터링\n",
        "    actual_drop = [c for c in cols_to_drop if c in df.columns]\n",
        "    print(f\"   Dropping columns: {actual_drop}\")\n",
        "    \n",
        "    # ========================================\n",
        "    # 5. X, y, weights 추출\n",
        "    # ========================================\n",
        "    X = df.drop(columns=actual_drop, errors='ignore')\n",
        "    y = df['target'].copy()\n",
        "    weights = df['sample_weight'].copy()\n",
        "    \n",
        "    # ✅ X에 datetime 컬럼이 없는지 확인\n",
        "    datetime_cols = X.select_dtypes(include=['datetime64']).columns.tolist()\n",
        "    if datetime_cols:\n",
        "        print(f\"   ⚠️ WARNING: Found datetime columns in X: {datetime_cols}\")\n",
        "        X = X.drop(columns=datetime_cols)\n",
        "        print(f\"   Removed datetime columns from X\")\n",
        "    \n",
        "    print(f\"   Feature columns ({len(X.columns)}): {list(X.columns[:10])}...\")\n",
        "    \n",
        "    # ========================================\n",
        "    # 6. Train/Valid/Test Split\n",
        "    # ========================================\n",
        "    train_mask = (dates <= '2018-12-31')\n",
        "    valid_mask = (dates >= '2019-01-01') & (dates <= '2019-06-30')\n",
        "    test_mask  = (dates >= '2019-07-01')\n",
        "    \n",
        "    print(f\"   Train: {train_mask.sum()} rows\")\n",
        "    print(f\"   Valid: {valid_mask.sum()} rows\")\n",
        "    print(f\"   Test:  {test_mask.sum()} rows\")\n",
        "    \n",
        "    return (\n",
        "        (X[train_mask], y[train_mask], weights[train_mask]),\n",
        "        (X[valid_mask], y[valid_mask], weights[valid_mask]),\n",
        "        (X[test_mask], y[test_mask], weights[test_mask], dates[test_mask])\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r6OTeYYcMpV"
      },
      "source": [
        "**4. 모델 튜닝 및 학습 함수**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "A8iAvnx2cOed"
      },
      "outputs": [],
      "source": [
        "def sanitize_model_name(model_name):\n",
        "    \"\"\"모델명을 파일명으로 사용 가능하게 변환\"\"\"\n",
        "    return model_name.replace('(', '_').replace(')', '').replace('=', '').replace(' ', '')\n",
        "\n",
        "def train_linear_models(dataset_name, file_path):\n",
        "    \"\"\"\n",
        "    Linear, Ridge, Lasso Grid Search (alpha = [0.1, 1.0, 10.0])\n",
        "    총 7개 조합: Linear(1) + Ridge(3) + Lasso(3)\n",
        "    \n",
        "    ✅ 이미 결과 파일이 존재하면 스킵\n",
        "    \"\"\"\n",
        "    \n",
        "    # ========================================\n",
        "    # 0. ✅ 결과 파일 존재 여부 확인 (패턴 매칭)\n",
        "    # ========================================\n",
        "    existing_files = list(RESULTS_DIR.glob(f\"pred_*_{dataset_name}.json\"))\n",
        "    \n",
        "    if existing_files:\n",
        "        output_path = existing_files[0]\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"⏭️  SKIPPING: {dataset_name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"   Result already exists: {output_path.name}\")\n",
        "        \n",
        "        try:\n",
        "            # 파일명에서 모델명 추출\n",
        "            # pred_Linear_A.json → \"Linear\"\n",
        "            # pred_Ridge_alpha0.1_B_headlines_pca.json → \"Ridge_alpha0.1\"\n",
        "            filename_parts = output_path.stem.split('_')\n",
        "            # 'pred' 다음부터 dataset_name 직전까지가 모델명\n",
        "            dataset_parts = dataset_name.split('_')\n",
        "            model_name = '_'.join(filename_parts[1:-len(dataset_parts)])\n",
        "            \n",
        "            # MSE 계산\n",
        "            with open(output_path, 'r') as f:\n",
        "                result_data = json.load(f)\n",
        "            \n",
        "            actuals = [item['actual'] for item in result_data]\n",
        "            preds = [item['predicted'] for item in result_data]\n",
        "            mse = mean_squared_error(actuals, preds)\n",
        "            \n",
        "            print(f\"   ✅ Cached Model: {model_name}\")\n",
        "            print(f\"   ✅ Cached Test MSE: {mse:.4f}\")\n",
        "            \n",
        "            return model_name, mse\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ⚠️ Warning: Could not read cached result: {e}\")\n",
        "            print(f\"   Re-running experiment...\")\n",
        "            # 에러 시 계속 진행\n",
        "    \n",
        "    # ========================================\n",
        "    # 1. 데이터 로드 및 전처리\n",
        "    # ========================================\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Processing: {dataset_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    df = load_data(file_path)\n",
        "    \n",
        "    (X_train, y_train, w_train), \\\n",
        "    (X_valid, y_valid, w_valid), \\\n",
        "    (X_test, y_test, w_test, dates_test) = preprocess_and_split(df)\n",
        "    \n",
        "    del df\n",
        "    gc.collect()\n",
        "    \n",
        "    # ========================================\n",
        "    # 2. Grid Search 설정\n",
        "    # ========================================\n",
        "    alphas = [0.1]\n",
        "    \n",
        "    models_config = [\n",
        "        ('Linear', LinearRegression(), None),\n",
        "        ('Ridge', Ridge(), alphas),\n",
        "        ('Lasso', Lasso(max_iter=2000), alphas)\n",
        "    ]\n",
        "    \n",
        "    print(f\"   Grid Search Configuration:\")\n",
        "    print(f\"   - Linear: 1 combination\")\n",
        "    print(f\"   - Ridge: {len(alphas)} combinations (alphas={alphas})\")\n",
        "    print(f\"   - Lasso: {len(alphas)} combinations (alphas={alphas})\")\n",
        "    print(f\"   - Total: {1 + len(alphas)*2} combinations\\n\")\n",
        "    \n",
        "    # ========================================\n",
        "    # 3. Grid Search 실행\n",
        "    # ========================================\n",
        "    best_mse = float('inf')\n",
        "    best_model = None\n",
        "    best_info = \"\"\n",
        "    \n",
        "    for model_name, model_base, alpha_list in models_config:\n",
        "        \n",
        "        if alpha_list is None:\n",
        "            print(f\"   Training {model_name}...\")\n",
        "            \n",
        "            pipeline = Pipeline([\n",
        "                ('scaler', StandardScaler()),\n",
        "                ('model', model_base)\n",
        "            ])\n",
        "            \n",
        "            pipeline.fit(X_train, y_train, model__sample_weight=w_train)\n",
        "            val_pred = pipeline.predict(X_valid)\n",
        "            val_mse = mean_squared_error(y_valid, val_pred)\n",
        "            \n",
        "            print(f\"      Valid MSE: {val_mse:.4f}\")\n",
        "            \n",
        "            if val_mse < best_mse:\n",
        "                best_mse = val_mse\n",
        "                best_model = pipeline\n",
        "                best_info = model_name\n",
        "        \n",
        "        else:\n",
        "            for alpha in alpha_list:\n",
        "                print(f\"   Training {model_name}(alpha={alpha})...\")\n",
        "                \n",
        "                if model_name == 'Ridge':\n",
        "                    current_model = Ridge(alpha=alpha)\n",
        "                else:\n",
        "                    current_model = Lasso(alpha=alpha, max_iter=2000)\n",
        "                \n",
        "                pipeline = Pipeline([\n",
        "                    ('scaler', StandardScaler()),\n",
        "                    ('model', current_model)\n",
        "                ])\n",
        "                \n",
        "                pipeline.fit(X_train, y_train, model__sample_weight=w_train)\n",
        "                val_pred = pipeline.predict(X_valid)\n",
        "                val_mse = mean_squared_error(y_valid, val_pred)\n",
        "                \n",
        "                print(f\"      Valid MSE: {val_mse:.4f}\")\n",
        "                \n",
        "                if val_mse < best_mse:\n",
        "                    best_mse = val_mse\n",
        "                    best_model = pipeline\n",
        "                    best_info = f\"{model_name}(alpha={alpha})\"\n",
        "    \n",
        "    # ========================================\n",
        "    # 4. Best Model 평가\n",
        "    # ========================================\n",
        "    print(f\"\\n   ✅ Best Model: {best_info}\")\n",
        "    print(f\"   ✅ Validation MSE: {best_mse:.4f}\")\n",
        "    \n",
        "    test_pred = best_model.predict(X_test)\n",
        "    test_mse = mean_squared_error(y_test, test_pred)\n",
        "    print(f\"   ✅ Test MSE: {test_mse:.4f}\")\n",
        "    \n",
        "    # ========================================\n",
        "    # 5. ✅ 결과 저장 (RESULTS_DIR)\n",
        "    # ========================================\n",
        "    result_data = []\n",
        "    for date, actual, pred in zip(dates_test, y_test, test_pred):\n",
        "        result_data.append({\n",
        "            \"date\": date.strftime('%Y-%m-%d'),\n",
        "            \"actual\": float(actual),\n",
        "            \"predicted\": float(pred)\n",
        "        })\n",
        "    \n",
        "    # ✅ 파일명에 모델명 포함\n",
        "    safe_model_name = sanitize_model_name(best_info)\n",
        "    output_path = RESULTS_DIR / f\"pred_{safe_model_name}_{dataset_name}.json\"\n",
        "    \n",
        "    with open(output_path, \"w\") as f:\n",
        "        json.dump(result_data, f, indent=4)\n",
        "    \n",
        "    print(f\"   Saved predictions to: results/{output_path.name}\")\n",
        "    \n",
        "    # 메모리 정리\n",
        "    del X_train, y_train, X_valid, y_valid, X_test, y_test\n",
        "    gc.collect()\n",
        "    \n",
        "    return best_info, test_mse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnBbrmCUcicE"
      },
      "source": [
        "**5. Main Execution Loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "45F_IT8WcmcV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "BASELINE: Dataset A\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: A\n",
            "============================================================\n",
            "   Result already exists: pred_Lasso_alpha0.1_A.json\n",
            "   ✅ Cached Model: Lasso_alpha0.1\n",
            "   ✅ Cached Test MSE: 1015.6601\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: B_headlines_pca\n",
            "============================================================\n",
            "   Result already exists: pred_Linear_B_headlines_pca.json\n",
            "   ✅ Cached Model: Linear\n",
            "   ✅ Cached Test MSE: 1105.7667\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: B_headlines_orig\n",
            "============================================================\n",
            "   Result already exists: pred_Lasso_alpha0.1_B_headlines_orig.json\n",
            "   ✅ Cached Model: Lasso_alpha0.1\n",
            "   ✅ Cached Test MSE: 1098.6073\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: B_chunking_pca\n",
            "============================================================\n",
            "   Result already exists: pred_Linear_B_chunking_pca.json\n",
            "   ✅ Cached Model: Linear\n",
            "   ✅ Cached Test MSE: 1104.6235\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: B_chunking_orig\n",
            "============================================================\n",
            "   Result already exists: pred_Lasso_alpha0.1_B_chunking_orig.json\n",
            "   ✅ Cached Model: Lasso_alpha0.1\n",
            "   ✅ Cached Test MSE: 1099.9708\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: B_bodyText_pca\n",
            "============================================================\n",
            "   Result already exists: pred_Linear_B_bodyText_pca.json\n",
            "   ✅ Cached Model: Linear\n",
            "   ✅ Cached Test MSE: 1096.2534\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: B_bodyText_orig\n",
            "============================================================\n",
            "   Result already exists: pred_Lasso_alpha0.1_B_bodyText_orig.json\n",
            "   ✅ Cached Model: Lasso_alpha0.1\n",
            "   ✅ Cached Test MSE: 1098.0591\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: B_paragraphs_pca\n",
            "============================================================\n",
            "   Result already exists: pred_Linear_B_paragraphs_pca.json\n",
            "   ✅ Cached Model: Linear\n",
            "   ✅ Cached Test MSE: 1101.5742\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: B_paragraphs_orig\n",
            "============================================================\n",
            "   Result already exists: pred_Lasso_alpha0.1_B_paragraphs_orig.json\n",
            "   ✅ Cached Model: Lasso_alpha0.1\n",
            "   ✅ Cached Test MSE: 1102.7649\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: C_headlines_pca\n",
            "============================================================\n",
            "   Result already exists: pred_Linear_C_headlines_pca.json\n",
            "   ✅ Cached Model: Linear\n",
            "   ✅ Cached Test MSE: 1105.7042\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: C_headlines_orig\n",
            "============================================================\n",
            "   Result already exists: pred_Lasso_alpha0.1_C_headlines_orig.json\n",
            "   ✅ Cached Model: Lasso_alpha0.1\n",
            "   ✅ Cached Test MSE: 1098.6348\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: C_chunking_pca\n",
            "============================================================\n",
            "   Result already exists: pred_Linear_C_chunking_pca.json\n",
            "   ✅ Cached Model: Linear\n",
            "   ✅ Cached Test MSE: 1104.1838\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: C_chunking_orig\n",
            "============================================================\n",
            "   Result already exists: pred_Lasso_alpha0.1_C_chunking_orig.json\n",
            "   ✅ Cached Model: Lasso_alpha0.1\n",
            "   ✅ Cached Test MSE: 1099.9154\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: C_bodyText_pca\n",
            "============================================================\n",
            "   Result already exists: pred_Linear_C_bodyText_pca.json\n",
            "   ✅ Cached Model: Linear\n",
            "   ✅ Cached Test MSE: 1096.2036\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: C_bodyText_orig\n",
            "============================================================\n",
            "   Result already exists: pred_Lasso_alpha0.1_C_bodyText_orig.json\n",
            "   ✅ Cached Model: Lasso_alpha0.1\n",
            "   ✅ Cached Test MSE: 1098.0741\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: C_paragraphs_pca\n",
            "============================================================\n",
            "   Result already exists: pred_Linear_C_paragraphs_pca.json\n",
            "   ✅ Cached Model: Linear\n",
            "   ✅ Cached Test MSE: 1101.5813\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: C_paragraphs_orig\n",
            "============================================================\n",
            "   Result already exists: pred_Lasso_alpha0.1_C_paragraphs_orig.json\n",
            "   ✅ Cached Model: Lasso_alpha0.1\n",
            "   ✅ Cached Test MSE: 1102.6769\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: D_headlines_pca\n",
            "============================================================\n",
            "   Result already exists: pred_Linear_D_headlines_pca.json\n",
            "   ✅ Cached Model: Linear\n",
            "   ✅ Cached Test MSE: 889.9465\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: D_headlines_orig\n",
            "============================================================\n",
            "   Result already exists: pred_Lasso_alpha0.1_D_headlines_orig.json\n",
            "   ✅ Cached Model: Lasso_alpha0.1\n",
            "   ✅ Cached Test MSE: 881.8738\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: D_chunking_pca\n",
            "============================================================\n",
            "   Result already exists: pred_Linear_D_chunking_pca.json\n",
            "   ✅ Cached Model: Linear\n",
            "   ✅ Cached Test MSE: 886.4355\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: D_chunking_orig\n",
            "============================================================\n",
            "   Result already exists: pred_Linear_D_chunking_orig.json\n",
            "   ✅ Cached Model: Linear\n",
            "   ✅ Cached Test MSE: 898.5955\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: D_bodyText_pca\n",
            "============================================================\n",
            "   Result already exists: pred_Linear_D_bodyText_pca.json\n",
            "   ✅ Cached Model: Linear\n",
            "   ✅ Cached Test MSE: 881.7619\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: D_bodyText_orig\n",
            "============================================================\n",
            "   Result already exists: pred_Linear_D_bodyText_orig.json\n",
            "   ✅ Cached Model: Linear\n",
            "   ✅ Cached Test MSE: 895.5691\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: D_paragraphs_pca\n",
            "============================================================\n",
            "   Result already exists: pred_Linear_D_paragraphs_pca.json\n",
            "   ✅ Cached Model: Linear\n",
            "   ✅ Cached Test MSE: 886.3165\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: D_paragraphs_orig\n",
            "============================================================\n",
            "   Result already exists: pred_Linear_D_paragraphs_orig.json\n",
            "   ✅ Cached Model: Linear\n",
            "   ✅ Cached Test MSE: 896.3980\n",
            "\n",
            "============================================================\n",
            "ALL TASKS COMPLETED\n",
            "============================================================\n",
            "\n",
            "Results saved to: results_lr/linear_evaluation_metrics.csv\n",
            "\n",
            "Top 10 Models by Test MSE:\n",
            "   Feature_set  Embeddings Dim_reduction           Model          MSE\n",
            "21           D    bodyText           pca          Linear   881.761903\n",
            "18           D   headlines          orig  Lasso_alpha0.1   881.873831\n",
            "23           D  paragraphs           pca          Linear   886.316460\n",
            "19           D    chunking           pca          Linear   886.435477\n",
            "17           D   headlines           pca          Linear   889.946503\n",
            "22           D    bodyText          orig          Linear   895.569087\n",
            "24           D  paragraphs          orig          Linear   896.398034\n",
            "20           D    chunking          orig          Linear   898.595455\n",
            "0            A           -             -  Lasso_alpha0.1  1015.660133\n",
            "13           C    bodyText           pca          Linear  1096.203566\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# Main Execution Loop\n",
        "# ========================================\n",
        "\n",
        "levels = ['B', 'C', 'D']\n",
        "methods = ['headlines', 'chunking', 'bodyText', 'paragraphs']\n",
        "types = ['pca', 'orig']\n",
        "\n",
        "metrics_list = []\n",
        "\n",
        "# Dataset A (Baseline)\n",
        "path_A = DATA_DIR / \"dataset_A.parquet\"\n",
        "if path_A.exists():\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"BASELINE: Dataset A\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    try:\n",
        "        info, mse = train_linear_models(\"A\", path_A)\n",
        "        metrics_list.append({\n",
        "            \"Feature_set\": \"A\",\n",
        "            \"Embeddings\": \"-\",\n",
        "            \"Dim_reduction\": \"-\",\n",
        "            \"Model\": info,\n",
        "            \"MSE\": mse\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error on Dataset A: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    finally:\n",
        "        gc.collect()\n",
        "else:\n",
        "    print(f\"Warning: {path_A} not found. Skipping Dataset A.\")\n",
        "\n",
        "# Dataset B, C, D\n",
        "total_datasets = len(levels) * len(methods) * len(types)\n",
        "current = 0\n",
        "\n",
        "for level in levels:\n",
        "    for method in methods:\n",
        "        for dtype in types:\n",
        "            current += 1\n",
        "            fname = f\"dataset_{level}_{method}_{dtype}.parquet\"\n",
        "            fpath = DATA_DIR / fname\n",
        "            \n",
        "            if not fpath.exists():\n",
        "                print(f\"\\n[{current}/{total_datasets}] Skipping {fname}: File not found.\")\n",
        "                continue\n",
        "            \n",
        "            dname = f\"{level}_{method}_{dtype}\"\n",
        "            \n",
        "            try:\n",
        "                info, mse = train_linear_models(dname, fpath)\n",
        "                metrics_list.append({\n",
        "                    \"Feature_set\": level,\n",
        "                    \"Embeddings\": method,\n",
        "                    \"Dim_reduction\": dtype,\n",
        "                    \"Model\": info,\n",
        "                    \"MSE\": mse\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"\\n❌ Error on {dname}: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "            finally:\n",
        "                gc.collect()\n",
        "\n",
        "# ========================================\n",
        "# 최종 결과 정리 및 저장\n",
        "# ========================================\n",
        "final_df = pd.DataFrame(metrics_list).sort_values(\"MSE\")\n",
        "\n",
        "# ✅ OUTPUT_DIR (results_lr/)에 CSV 저장\n",
        "csv_path = OUTPUT_DIR / \"linear_evaluation_metrics.csv\"\n",
        "final_df.to_csv(csv_path, index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ALL TASKS COMPLETED\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nResults saved to: {csv_path}\")\n",
        "print(f\"\\nTop 10 Models by Test MSE:\")\n",
        "print(final_df.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mlproject",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
