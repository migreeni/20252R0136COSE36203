{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61059e98",
   "metadata": {},
   "source": [
    "LightGBM_Modeling (Modified Version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948a9cba",
   "metadata": {},
   "source": [
    "2. 라이브러리 & 기본 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebbd86ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT ROOT: /NAS/wonjun/20252R0136COSE36203/Prediction\n",
      "FEATURE DIR : /NAS/wonjun/20252R0136COSE36203/Prediction/../feature_datasets\n",
      "OUTPUT DIR  : /NAS/wonjun/20252R0136COSE36203/Prediction/results_lightgbm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import gc\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 프로젝트 루트 및 경로 설정\n",
    "ROOT = Path.cwd()\n",
    "FEATURE_DIR = ROOT / \"../feature_datasets\"      # feature_generation 결과 폴더\n",
    "OUTPUT_DIR = ROOT / \"results_lightgbm\"       # 모델 결과 저장 폴더\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"PROJECT ROOT:\", ROOT)\n",
    "print(\"FEATURE DIR :\", FEATURE_DIR)\n",
    "print(\"OUTPUT DIR  :\", OUTPUT_DIR)\n",
    "\n",
    "# 재현성용 seed\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc917b09",
   "metadata": {},
   "source": [
    "3. 파일 이름 파싱 함수\n",
    "- 파일명에서 Dataset/Method/Type 정보를 추출해서 결과 CSV에 넣어주기.\n",
    "    - `dataset_A.parquet` → Dataset = 'A', Method=None, Type=None\n",
    "    - `dataset_D_headlines_orig.parquet` → Dataset='D', Method='headlines', Type='orig'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cca1969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dataset_filename(fname: str):\n",
    "    \"\"\"\n",
    "    feature_datasets 안의 파일명을 파싱해\n",
    "    Dataset / Method / Type 정보를 반환한다.\n",
    "    \"\"\"\n",
    "    name = fname.replace(\"dataset_\", \"\").replace(\".parquet\", \"\")\n",
    "    parts = name.split(\"_\")\n",
    "\n",
    "    # Case 1: dataset_A.parquet → ['A']\n",
    "    if len(parts) == 1:\n",
    "        return {\n",
    "            \"Dataset\": parts[0],   # 'A'\n",
    "            \"Method\": None,\n",
    "            \"Type\": None,\n",
    "        }\n",
    "\n",
    "    # Case 2: dataset_B_headlines_orig.parquet → ['B', 'headlines', 'orig']\n",
    "    if len(parts) == 3:\n",
    "        Dataset, Method, Type = parts\n",
    "        return {\n",
    "            \"Dataset\": Dataset,\n",
    "            \"Method\": Method,\n",
    "            \"Type\": Type,\n",
    "        }\n",
    "\n",
    "    # 예상 밖 패턴이 나오면 None\n",
    "    return {\n",
    "        \"Dataset\": None,\n",
    "        \"Method\": None,\n",
    "        \"Type\": None,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ff8db3",
   "metadata": {},
   "source": [
    "4. 임베딩 확장 함수\n",
    "- orig 데이터셋에는 `embedding` 컬럼이 `np.ndarray`로 들어있음\n",
    "- 이를 LightGBM이 사용할 수 있도록\n",
    "  - `emb_0, emb_1, ..., emb_{d-1}` 컬럼으로 펼쳐준다\n",
    "- PCA 버전(pca_0~)은 이미 펼쳐져 있으므로 이 함수가 필요 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5984ca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_embeddings(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    'embedding' 컬럼이 존재하면, np.ndarray 리스트를\n",
    "    emb_0, emb_1, ... 형태의 feature 컬럼으로 확장한다.\n",
    "    확장 후 원래 'embedding' 컬럼은 제거한다.\n",
    "    \"\"\"\n",
    "    if \"embedding\" not in df.columns:\n",
    "        return df\n",
    "\n",
    "    print(\"   >>> Expanding 'embedding' column into emb_0, emb_1, ...\")\n",
    "\n",
    "    # embedding 컬럼: 각 row가 np.ndarray 또는 list\n",
    "    emb_array = np.stack(df[\"embedding\"].values)   # (N, d)\n",
    "    dim = emb_array.shape[1]\n",
    "\n",
    "    emb_cols = [f\"emb_{i}\" for i in range(dim)]\n",
    "    emb_df = pd.DataFrame(emb_array, columns=emb_cols, index=df.index)\n",
    "\n",
    "    # 원본에서 'embedding' 제거 후 확장된 embedding 결합\n",
    "    df = pd.concat([df.drop(columns=[\"embedding\"]), emb_df], axis=1)\n",
    "\n",
    "    # 메모리 정리\n",
    "    del emb_array, emb_df\n",
    "    gc.collect()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acce6e0",
   "metadata": {},
   "source": [
    "5. Feature 컬럼 자동 선택 함수\n",
    "\n",
    "- 이 함수의 핵심 목적:\n",
    "    - **data leakage를 일으키는 컬럼, 메타데이터**를 자동으로 제거\n",
    "    - lag, fg_lag, PCA, embedding, person one-hot 등 **실제 feature만 선택**\n",
    "- 규칙은 Dataset D의 전체 스키마를 기준으로 설계."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6281be4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_columns(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    DataFrame에서 '모델에 넣을 feature 컬럼'만 선택해서 리스트로 반환한다.\n",
    "\n",
    "    Drop 대상:\n",
    "      - value (target)\n",
    "      - 날짜 관련: date_str, Date, date_index, article_date, pub_date\n",
    "      - 텍스트 메타데이터: person(문자열), article_id, person_id, idx,\n",
    "                           headline, trailText, bodyText, webTitle, webUrl,\n",
    "                           apiUrl, wordcount\n",
    "      - fg_value (현재 시점 Fear-Greed 지표 → leakage)\n",
    "\n",
    "    Keep 대상(명시적):\n",
    "      - lag_*, fg_lag_*, fg_value? (← 여기서는 드랍), pca_*, emb_*, person_*\n",
    "\n",
    "    그 외 숫자형 컬럼은 기본적으로 feature로 허용.\n",
    "    \"\"\"\n",
    "\n",
    "    DROP_PATTERNS = [\n",
    "        r\"^value$\",\n",
    "        r\"^date_str$\", r\"^date$\", r\"^date_index$\",\n",
    "        r\"article_date\", r\"pub_date\",\n",
    "        r\"article_id\", r\"person_id\", r\"idx$\",\n",
    "        r\"headline\", r\"trailText\", r\"bodyText\",\n",
    "        r\"webUrl\", r\"apiUrl\", r\"webTitle\", r\"wordcount\",\n",
    "        r\"^person$\",     # 문자열 person (이름)\n",
    "        r\"^fg_value$\",   # 현재 시점 fear-greed 값 → leakage\n",
    "    ]\n",
    "\n",
    "    KEEP_PATTERNS = [\n",
    "        r\"^lag_\\d+$\",\n",
    "        r\"^fg_lag_\\d+$\",\n",
    "        r\"^pca_\\d+$\",\n",
    "        r\"^emb_\\d+$\",\n",
    "        r\"^person_\\d+$\",  # one-hot (person_1 ~ person_100)\n",
    "    ]\n",
    "\n",
    "    def match_any(col: str, patterns):\n",
    "        return any(re.search(p, col) for p in patterns)\n",
    "\n",
    "    feature_cols = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        # 1) Drop 대상이면 무조건 제외\n",
    "        if match_any(col, DROP_PATTERNS):\n",
    "            continue\n",
    "\n",
    "        # 2) Keep 패턴에 걸리면 무조건 포함\n",
    "        if match_any(col, KEEP_PATTERNS):\n",
    "            feature_cols.append(col)\n",
    "            continue\n",
    "\n",
    "        # 3) 위에 걸리지 않았는데 numeric type이면 feature로 사용\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            feature_cols.append(col)\n",
    "\n",
    "    print(f\"   >>> Selected {len(feature_cols)} feature columns.\")\n",
    "    return feature_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bdd6ec",
   "metadata": {},
   "source": [
    "6. 전처리 + Train/Valid/Test Split + Sample Weight 계산\n",
    "### 6.1 날짜 처리 & Split 기준\n",
    "- `date_str` 형식: `\"YYYY_MM_DD\"`\n",
    "- 이를 `datetime`으로 변환한 `date` 컬럼을 생성\n",
    "- Split 기준:\n",
    "    - Train: 2017-01-01 ~ 2018-12-31\n",
    "    - Valid: 2019-01-01 ~ 2019-06-30\n",
    "    - Test : 2019-07-01 ~ 2019-12-31\n",
    "### 6.2 Sample Weight\n",
    "- 같은 날짜에 기사 N개 → 각 행의 가중치 = 1 / N\n",
    "- 이렇게 하면 하루 단위 loss contribution이 모두 비슷해짐."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4336f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_split(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    1) (필요시) embedding 확장\n",
    "    2) date_str → datetime 변환\n",
    "    3) feature 컬럼 자동 선택\n",
    "    4) Train/Valid/Test Split\n",
    "    5) 날짜별 기사 수 기반 sample weight 계산\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. embedding 확장 (orig 데이터셋일 경우)\n",
    "    if \"embedding\" in df.columns:\n",
    "        df = expand_embeddings(df)\n",
    "\n",
    "    # 2. date_str → datetime 변환\n",
    "    #    예: '2017_01_03' → Timestamp('2017-01-03')\n",
    "    if \"pub_date\" not in df.columns:\n",
    "        raise ValueError(\"pub_date 컬럼이 없습니다. feature_generation 단계 확인 필요.\")\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"pub_date\"].str.replace(\"_\", \"-\"))\n",
    "\n",
    "    # 3. feature 컬럼 선택 (data leakage 방지 핵심)\n",
    "    feature_cols = extract_feature_columns(df)\n",
    "\n",
    "    # 4. Split 마스크 정의\n",
    "    train_mask = (df[\"date\"] >= \"2017-01-01\") & (df[\"date\"] <= \"2018-12-31\")\n",
    "    valid_mask = (df[\"date\"] >= \"2019-01-01\") & (df[\"date\"] <= \"2019-06-30\")\n",
    "    test_mask  = (df[\"date\"] >= \"2019-07-01\") & (df[\"date\"] <= \"2019-12-31\")\n",
    "\n",
    "    # 5. 날짜별 sample weight 계산 함수\n",
    "    def make_X_y_w_dates(sub_df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        subset DataFrame에 대해:\n",
    "          - X: feature matrix\n",
    "          - y: target (value)\n",
    "          - w: sample weight (1 / 날짜별 기사 수)\n",
    "          - d: 날짜 문자열 (JSON/평가 저장용)\n",
    "        \"\"\"\n",
    "        if len(sub_df) == 0:\n",
    "            return None, None, None, None\n",
    "\n",
    "        if \"value\" not in sub_df.columns:\n",
    "            raise ValueError(\"target 컬럼 'value'가 없습니다.\")\n",
    "\n",
    "        # 날짜 기준 기사 개수\n",
    "        # date_index가 있으면 그걸 쓰고, 없으면 date를 사용\n",
    "        if \"date_index\" in sub_df.columns:\n",
    "            key = sub_df[\"date_index\"]\n",
    "        else:\n",
    "            key = sub_df[\"date\"]\n",
    "\n",
    "        counts = key.value_counts()\n",
    "        weights = 1.0 / key.map(counts)\n",
    "\n",
    "        X = sub_df[feature_cols]\n",
    "        y = sub_df[\"value\"].astype(float)\n",
    "        d = sub_df[\"date\"].dt.strftime(\"%Y-%m-%d\")   # JSON 저장용 포맷\n",
    "\n",
    "        return X, y, weights, d\n",
    "\n",
    "    # 실제 split\n",
    "    train_df = df[train_mask].copy()\n",
    "    valid_df = df[valid_mask].copy()\n",
    "    test_df  = df[test_mask].copy()\n",
    "\n",
    "    X_train, y_train, w_train, _       = make_X_y_w_dates(train_df)\n",
    "    X_valid, y_valid, w_valid, _       = make_X_y_w_dates(valid_df)\n",
    "    X_test,  y_test,  w_test, d_test   = make_X_y_w_dates(test_df)\n",
    "\n",
    "    print(f\"   >>> Split sizes | train: {len(train_df)}, valid: {len(valid_df)}, test: {len(test_df)}\")\n",
    "    return (X_train, y_train, w_train), (X_valid, y_valid, w_valid), (X_test, y_test, w_test, d_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c39bd7",
   "metadata": {},
   "source": [
    "7. LightGBM 학습/예측 프로세스 (파일 하나 기준)\n",
    "\n",
    "- 이 함수는 **하나의 parquet 파일(dataset_*.parquet)** 을 입력으로 받아:\n",
    "    1. 데이터 로드\n",
    "    2. 전처리 + split + weight 계산\n",
    "    3. LightGBM 학습\n",
    "    4. 기사 단위 예측 → 날짜별 평균\n",
    "    5. 일별 MSE 계산\n",
    "    6. JSON / metrics 리스트에 결과 기록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc3bdf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lightgbm_for_file(fname: str, metrics: list):\n",
    "    \"\"\"\n",
    "    단일 feature dataset 파일에 대해 LightGBM 학습 및 평가를 수행하고,\n",
    "    결과를 metrics 리스트에 추가한다.\n",
    "    \"\"\"\n",
    "    info = parse_dataset_filename(fname)\n",
    "    Dataset = info[\"Dataset\"]\n",
    "    Method  = info[\"Method\"]\n",
    "    Type    = info[\"Type\"]\n",
    "\n",
    "    print(f\"\\n>>> Processing file: {fname}\")\n",
    "    print(f\"    Dataset={Dataset}, Method={Method}, Type={Type}\")\n",
    "\n",
    "    # 1. 데이터 로드\n",
    "    path = FEATURE_DIR / fname\n",
    "    df = pd.read_parquet(path)\n",
    "\n",
    "    # 2. 전처리 + split + sample weight\n",
    "    train_set, valid_set, test_set = preprocess_and_split(df)\n",
    "    X_train, y_train, w_train = train_set\n",
    "    X_valid, y_valid, w_valid = valid_set\n",
    "    X_test,  y_test,  w_test, d_test = test_set\n",
    "\n",
    "    if X_train is None or X_valid is None or X_test is None:\n",
    "        print(\"   [Warning] Empty split (train/valid/test 중 일부가 비어 있음). 스킵합니다.\")\n",
    "        return\n",
    "\n",
    "    # 3. LightGBM Dataset 생성\n",
    "    dtrain = lgb.Dataset(X_train, label=y_train, weight=w_train)\n",
    "    dvalid = lgb.Dataset(X_valid, label=y_valid, weight=w_valid, reference=dtrain)\n",
    "\n",
    "    # 4. LightGBM 파라미터 (v4.x 호환)\n",
    "    params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"mse\",\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"num_leaves\": 31,\n",
    "        \"feature_fraction\": 0.9,\n",
    "        \"bagging_fraction\": 0.8,\n",
    "        \"bagging_freq\": 1,\n",
    "        \"seed\": RANDOM_SEED,\n",
    "        \"verbosity\": -1,\n",
    "        \"n_jobs\": -1,\n",
    "    }\n",
    "\n",
    "    callbacks = [\n",
    "        lgb.early_stopping(stopping_rounds=50, verbose=False),\n",
    "        lgb.log_evaluation(period=100),\n",
    "    ]\n",
    "\n",
    "    # 5. 학습\n",
    "    print(\"   >>> Training LightGBM ...\")\n",
    "    model = lgb.train(\n",
    "        params=params,\n",
    "        train_set=dtrain,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[dtrain, dvalid],\n",
    "        valid_names=[\"train\", \"valid\"],\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "    # 6. 기사 단위 예측\n",
    "    print(\"   >>> Predicting on test set ...\")\n",
    "    y_pred_raw = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "\n",
    "    # 7. 날짜별 집계\n",
    "    #    - 같은 날짜에 여러 기사가 있을 수 있으므로\n",
    "    #      actual은 first(), pred는 mean()으로 집계\n",
    "    res_df = pd.DataFrame({\n",
    "        \"date\": d_test.values,\n",
    "        \"actual\": y_test.values,\n",
    "        \"pred\": y_pred_raw,\n",
    "    })\n",
    "\n",
    "    daily_actual = res_df.groupby(\"date\")[\"actual\"].first()\n",
    "    daily_pred   = res_df.groupby(\"date\")[\"pred\"].mean()\n",
    "    daily_df     = pd.concat([daily_actual, daily_pred], axis=1)\n",
    "\n",
    "    # 8. 일별 MSE 계산\n",
    "    mse = mean_squared_error(daily_df[\"actual\"], daily_df[\"pred\"])\n",
    "    print(f\"   [Result] Daily MSE: {mse:.6f}\")\n",
    "\n",
    "    # 9. metrics 리스트에 기록 (나중에 CSV로 저장)\n",
    "    metrics.append({\n",
    "        \"Dataset\": Dataset,\n",
    "        \"Method\": Method if Method is not None else \"none\",\n",
    "        \"Type\": Type if Type is not None else \"none\",\n",
    "        \"Model\": \"LightGBM\",\n",
    "        \"MSE\": mse,\n",
    "    })\n",
    "\n",
    "    # 10. 일별 예측 결과 JSON 저장\n",
    "    #     구조: { \"YYYY-MM-DD\": { \"actual\": ..., \"pred\": ... }, ... }\n",
    "    json_obj = {\n",
    "        date: {\n",
    "            \"actual\": float(row[\"actual\"]),\n",
    "            \"pred\": float(row[\"pred\"]),\n",
    "        }\n",
    "        for date, row in daily_df.iterrows()\n",
    "    }\n",
    "\n",
    "    json_name = f\"pred_LGBM_{Dataset}_{Method or 'none'}_{Type or 'none'}.json\"\n",
    "    json_path = OUTPUT_DIR / json_name\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_obj, f, indent=2)\n",
    "    print(f\"   >>> Saved prediction JSON to: {json_path}\")\n",
    "\n",
    "    # 11. 메모리 정리\n",
    "    del df, dtrain, dvalid, model, X_train, X_valid, X_test\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478c37f5",
   "metadata": {},
   "source": [
    "8. 전체 파일에 대해 LightGBM 실행\n",
    "- feature_datasets 폴더 안의 모든 `dataset_*.parquet` 파일에 대해\n",
    "  위에서 정의한 `run_lightgbm_for_file()`를 순차 실행\n",
    "- 실행이 성공한 경우 metrics 리스트에 한 줄씩 기록\n",
    "- 마지막에 metrics를 하나의 CSV로 저장:\n",
    "    - Columns: Dataset / Method / Type / Model / MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0208a671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25 dataset files.\n",
      "\n",
      ">>> Processing file: dataset_A.parquet\n",
      "    Dataset=A, Method=None, Type=None\n",
      "   >>> Selected 5 feature columns.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   >>> Split sizes | train: 502, valid: 124, test: 128\n",
      "   >>> Training LightGBM ...\n",
      "[100]\ttrain's l2: 244.718\tvalid's l2: 479.234\n",
      "[200]\ttrain's l2: 166.844\tvalid's l2: 467.409\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    if not FEATURE_DIR.exists():\n",
    "        print(\"[Error] feature_datasets 폴더를 찾을 수 없습니다.\")\n",
    "        return\n",
    "\n",
    "    # 처리할 파일 목록\n",
    "    file_list = sorted(\n",
    "        [f for f in os.listdir(FEATURE_DIR) if f.startswith(\"dataset_\") and f.endswith(\".parquet\")]\n",
    "    )\n",
    "    print(f\"Found {len(file_list)} dataset files.\")\n",
    "\n",
    "    metrics = []\n",
    "\n",
    "    for fname in file_list:\n",
    "        try:\n",
    "            run_lightgbm_for_file(fname, metrics)\n",
    "        except Exception as e:\n",
    "            print(f\"   [Error] Failed processing {fname}: {e}\")\n",
    "\n",
    "    # 결과를 CSV로 저장\n",
    "    if metrics:\n",
    "        metrics_df = pd.DataFrame(metrics)\n",
    "        metrics_df = metrics_df.sort_values([\"Dataset\", \"Method\", \"Type\"])\n",
    "\n",
    "        csv_path = OUTPUT_DIR / \"lightgbm_evaluation_metrics.csv\"\n",
    "        metrics_df.to_csv(csv_path, index=False)\n",
    "        print(\"\\n[Done] All files processed.\")\n",
    "        print(\"Metrics saved to:\", csv_path)\n",
    "        print(metrics_df)\n",
    "    else:\n",
    "        print(\"No metrics were produced. (모든 파일이 에러로 스킵된 듯 합니다.)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
