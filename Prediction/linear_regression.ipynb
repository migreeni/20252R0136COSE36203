{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHnDibxQeOWz"
      },
      "source": [
        "### **Linear Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94BbcRQ-eJae"
      },
      "source": [
        "**0. Import**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Bm3z6s5ObJin"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOYzkY32bww4"
      },
      "source": [
        "**1. 경로 설정**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dl0U29klbyyG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Source: ../feature_datasets\n",
            "Output Path: results_lr\n"
          ]
        }
      ],
      "source": [
        "# DATA_DIR = Path(\"/content/drive/MyDrive/COSE362/data/feature_engineering\")\n",
        "# OUTPUT_DIR = Path(\"/content/drive/MyDrive/COSE362/data/prediction_output\")\n",
        "DATA_DIR = Path(\"../feature_datasets\")\n",
        "OUTPUT_DIR = Path(\"results_lr\")\n",
        "RESULTS_DIR = OUTPUT_DIR / \"results\"  \n",
        "\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "RESULTS_DIR.mkdir(parents=True, exist_ok=True) \n",
        "\n",
        "\n",
        "\n",
        "print(f\"Data Source: {DATA_DIR}\")\n",
        "print(f\"Output Path: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvByCANab0-K"
      },
      "source": [
        "**2. 데이터 로드**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aUEGo0z2b2Ro"
      },
      "outputs": [],
      "source": [
        "def load_data(file_path):\n",
        "    \"\"\"\n",
        "    Parquet 파일 로드\n",
        "    \"\"\"\n",
        "    print(f\"Loading {file_path.name}...\")\n",
        "    df = pd.read_parquet(file_path)\n",
        "    \n",
        "    # 날짜순 정렬\n",
        "    if 'date_index' in df.columns:\n",
        "        df = df.sort_values('date_index').reset_index(drop=True)\n",
        "    \n",
        "    # ========================================\n",
        "    # pub_date → Date 변환\n",
        "    # ========================================\n",
        "    if 'pub_date' in df.columns:\n",
        "        # pub_date 형식: '2019_12_30' (언더스코어로 구분)\n",
        "        df['Date'] = pd.to_datetime(df['pub_date'], format='%Y_%m_%d')\n",
        "        print(f\"   Using 'pub_date' for Date column\")\n",
        "    else:\n",
        "        # pub_date가 없으면 임시 날짜 생성 (fallback)\n",
        "        print(\"   [Warning] No 'pub_date' found. Using default date range.\")\n",
        "        df['Date'] = pd.date_range(start='2017-01-01', periods=len(df), freq='D')\n",
        "    \n",
        "    print(f\"   Loaded {len(df)} rows, {len(df.columns)} columns\")\n",
        "    print(f\"   ✅ Date range: {df['Date'].min().strftime('%Y-%m-%d')} to {df['Date'].max().strftime('%Y-%m-%d')}\")\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XB5E2PUyb8wH"
      },
      "source": [
        "**3. 전처리 & 가중치 생성 함수**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4Go3h3xlb_n8"
      },
      "outputs": [],
      "source": [
        "def preprocess_and_split(df, target_col='value'):\n",
        "    \"\"\"\n",
        "    전처리 및 Train/Valid/Test 분할\n",
        "    \"\"\"\n",
        "    \n",
        "    # ========================================\n",
        "    # 1. Target 생성\n",
        "    # ========================================\n",
        "    daily_prices = df[['date_index', target_col]].drop_duplicates().sort_values('date_index')\n",
        "    daily_prices['target'] = daily_prices[target_col].shift(-1)\n",
        "    \n",
        "    df = df.drop(columns=['target'], errors='ignore')\n",
        "    df = df.merge(daily_prices[['date_index', 'target']], on='date_index', how='left')\n",
        "    df = df.dropna(subset=['target'])\n",
        "    \n",
        "    print(f\"   After target creation: {len(df)} rows\")\n",
        "    \n",
        "    # ========================================\n",
        "    # 2. Sample Weight 계산\n",
        "    # ========================================\n",
        "    date_counts = df['date_index'].value_counts()\n",
        "    df['sample_weight'] = df['date_index'].map(lambda x: 1.0 / date_counts[x])\n",
        "    \n",
        "    print(f\"   Sample weights: min={df['sample_weight'].min():.4f}, \"\n",
        "          f\"max={df['sample_weight'].max():.4f}, mean={df['sample_weight'].mean():.4f}\")\n",
        "    \n",
        "    # ========================================\n",
        "    # 3. ✅ Date 먼저 추출 (split용)\n",
        "    # ========================================\n",
        "    if 'Date' not in df.columns:\n",
        "        raise ValueError(\"'Date' column not found in dataframe\")\n",
        "    \n",
        "    dates = df['Date'].copy()\n",
        "    \n",
        "    # ✅ 즉시 Date 컬럼 제거\n",
        "    df = df.drop(columns=['Date'])\n",
        "    \n",
        "    # ========================================\n",
        "    # 4. 드롭할 컬럼 정의\n",
        "    # ========================================\n",
        "    cols_to_drop = [\n",
        "        # 메타데이터\n",
        "        'person',\n",
        "        'person_id', \n",
        "        'article_id',\n",
        "        \n",
        "        # 날짜 (이미 제거됨)\n",
        "        'pub_date',\n",
        "        # 'Date',  ← 이미 위에서 제거했으므로 불필요\n",
        "        \n",
        "        # Target 관련\n",
        "        'value',\n",
        "        'target',     # y로 사용 (X에서만 제외)\n",
        "        \n",
        "        # Fear-Greed\n",
        "        'fg_value',\n",
        "        \n",
        "        # Weight\n",
        "        'sample_weight',\n",
        "    ]\n",
        "    \n",
        "    # 실제 존재하는 컬럼만 필터링\n",
        "    actual_drop = [c for c in cols_to_drop if c in df.columns]\n",
        "    print(f\"   Dropping columns: {actual_drop}\")\n",
        "    \n",
        "    # ========================================\n",
        "    # 5. X, y, weights 추출\n",
        "    # ========================================\n",
        "    X = df.drop(columns=actual_drop, errors='ignore')\n",
        "    y = df['target'].copy()\n",
        "    weights = df['sample_weight'].copy()\n",
        "    \n",
        "    # ✅ X에 datetime 컬럼이 없는지 확인\n",
        "    datetime_cols = X.select_dtypes(include=['datetime64']).columns.tolist()\n",
        "    if datetime_cols:\n",
        "        print(f\"   ⚠️ WARNING: Found datetime columns in X: {datetime_cols}\")\n",
        "        X = X.drop(columns=datetime_cols)\n",
        "        print(f\"   Removed datetime columns from X\")\n",
        "    \n",
        "    print(f\"   Feature columns ({len(X.columns)}): {list(X.columns[:10])}...\")\n",
        "    \n",
        "    # ========================================\n",
        "    # 6. Train/Valid/Test Split\n",
        "    # ========================================\n",
        "    train_mask = (dates <= '2018-12-31')\n",
        "    valid_mask = (dates >= '2019-01-01') & (dates <= '2019-06-30')\n",
        "    test_mask  = (dates >= '2019-07-01')\n",
        "    \n",
        "    print(f\"   Train: {train_mask.sum()} rows\")\n",
        "    print(f\"   Valid: {valid_mask.sum()} rows\")\n",
        "    print(f\"   Test:  {test_mask.sum()} rows\")\n",
        "    \n",
        "    return (\n",
        "        (X[train_mask], y[train_mask], weights[train_mask]),\n",
        "        (X[valid_mask], y[valid_mask], weights[valid_mask]),\n",
        "        (X[test_mask], y[test_mask], weights[test_mask], dates[test_mask])\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r6OTeYYcMpV"
      },
      "source": [
        "**4. 모델 튜닝 및 학습 함수**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "A8iAvnx2cOed"
      },
      "outputs": [],
      "source": [
        "def train_linear_models(dataset_name, file_path):\n",
        "    \"\"\"\n",
        "    Linear, Ridge, Lasso Grid Search (alpha = [0.1, 1.0, 10.0])\n",
        "    총 7개 조합: Linear(1) + Ridge(3) + Lasso(3)\n",
        "    \n",
        "    ✅ 이미 결과 파일이 존재하면 스킵\n",
        "    \"\"\"\n",
        "    \n",
        "    # ========================================\n",
        "    # 0. ✅ 결과 파일 존재 여부 확인\n",
        "    # ========================================\n",
        "    output_path = RESULTS_DIR / f\"pred_linear_{dataset_name}.json\"\n",
        "    \n",
        "    if output_path.exists():\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"⏭️  SKIPPING: {dataset_name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"   Result already exists: {output_path.name}\")\n",
        "        \n",
        "        # ✅ 기존 결과 파일에서 MSE 읽기 (optional)\n",
        "        try:\n",
        "            with open(output_path, 'r') as f:\n",
        "                result_data = json.load(f)\n",
        "            \n",
        "            # MSE 계산 (저장된 예측값으로)\n",
        "            actuals = [item['actual'] for item in result_data]\n",
        "            preds = [item['predicted'] for item in result_data]\n",
        "            mse = mean_squared_error(actuals, preds)\n",
        "            \n",
        "            # 어떤 모델이었는지는 모르니 \"Cached\" 표시\n",
        "            print(f\"   ✅ Cached Test MSE: {mse:.4f}\")\n",
        "            \n",
        "            return \"Cached\", mse\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ⚠️ Warning: Could not read cached MSE: {e}\")\n",
        "            print(f\"   Re-running experiment...\")\n",
        "            # 에러 시 계속 진행 (아래 코드 실행)\n",
        "    \n",
        "    # ========================================\n",
        "    # 1. 데이터 로드 및 전처리\n",
        "    # ========================================\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Processing: {dataset_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    df = load_data(file_path)\n",
        "    \n",
        "    (X_train, y_train, w_train), \\\n",
        "    (X_valid, y_valid, w_valid), \\\n",
        "    (X_test, y_test, w_test, dates_test) = preprocess_and_split(df)\n",
        "    \n",
        "    del df\n",
        "    gc.collect()\n",
        "    \n",
        "    # ========================================\n",
        "    # 2. Grid Search 설정\n",
        "    # ========================================\n",
        "    alphas = [0.1]\n",
        "    \n",
        "    models_config = [\n",
        "        ('Linear', LinearRegression(), None),\n",
        "        ('Ridge', Ridge(), alphas),\n",
        "        ('Lasso', Lasso(max_iter=2000), alphas)\n",
        "    ]\n",
        "    \n",
        "    print(f\"   Grid Search Configuration:\")\n",
        "    print(f\"   - Linear: 1 combination\")\n",
        "    print(f\"   - Ridge: {len(alphas)} combinations (alphas={alphas})\")\n",
        "    print(f\"   - Lasso: {len(alphas)} combinations (alphas={alphas})\")\n",
        "    print(f\"   - Total: {1 + len(alphas)*2} combinations\\n\")\n",
        "    \n",
        "    # ========================================\n",
        "    # 3. Grid Search 실행\n",
        "    # ========================================\n",
        "    best_mse = float('inf')\n",
        "    best_model = None\n",
        "    best_info = \"\"\n",
        "    \n",
        "    for model_name, model_base, alpha_list in models_config:\n",
        "        \n",
        "        if alpha_list is None:\n",
        "            print(f\"   Training {model_name}...\")\n",
        "            \n",
        "            pipeline = Pipeline([\n",
        "                ('scaler', StandardScaler()),\n",
        "                ('model', model_base)\n",
        "            ])\n",
        "            \n",
        "            pipeline.fit(X_train, y_train, model__sample_weight=w_train)\n",
        "            val_pred = pipeline.predict(X_valid)\n",
        "            val_mse = mean_squared_error(y_valid, val_pred)\n",
        "            \n",
        "            print(f\"      Valid MSE: {val_mse:.4f}\")\n",
        "            \n",
        "            if val_mse < best_mse:\n",
        "                best_mse = val_mse\n",
        "                best_model = pipeline\n",
        "                best_info = model_name\n",
        "        \n",
        "        else:\n",
        "            for alpha in alpha_list:\n",
        "                print(f\"   Training {model_name}(alpha={alpha})...\")\n",
        "                \n",
        "                if model_name == 'Ridge':\n",
        "                    current_model = Ridge(alpha=alpha)\n",
        "                else:\n",
        "                    current_model = Lasso(alpha=alpha, max_iter=2000)\n",
        "                \n",
        "                pipeline = Pipeline([\n",
        "                    ('scaler', StandardScaler()),\n",
        "                    ('model', current_model)\n",
        "                ])\n",
        "                \n",
        "                pipeline.fit(X_train, y_train, model__sample_weight=w_train)\n",
        "                val_pred = pipeline.predict(X_valid)\n",
        "                val_mse = mean_squared_error(y_valid, val_pred)\n",
        "                \n",
        "                print(f\"      Valid MSE: {val_mse:.4f}\")\n",
        "                \n",
        "                if val_mse < best_mse:\n",
        "                    best_mse = val_mse\n",
        "                    best_model = pipeline\n",
        "                    best_info = f\"{model_name}(alpha={alpha})\"\n",
        "    \n",
        "    # ========================================\n",
        "    # 4. Best Model 평가\n",
        "    # ========================================\n",
        "    print(f\"\\n   ✅ Best Model: {best_info}\")\n",
        "    print(f\"   ✅ Validation MSE: {best_mse:.4f}\")\n",
        "    \n",
        "    test_pred = best_model.predict(X_test)\n",
        "    test_mse = mean_squared_error(y_test, test_pred)\n",
        "    print(f\"   ✅ Test MSE: {test_mse:.4f}\")\n",
        "    \n",
        "    # ========================================\n",
        "    # 5. ✅ 결과 저장 (RESULTS_DIR)\n",
        "    # ========================================\n",
        "    result_data = []\n",
        "    for date, actual, pred in zip(dates_test, y_test, test_pred):\n",
        "        result_data.append({\n",
        "            \"date\": date.strftime('%Y-%m-%d'),\n",
        "            \"actual\": float(actual),\n",
        "            \"predicted\": float(pred)\n",
        "        })\n",
        "    \n",
        "    # ✅ RESULTS_DIR에 저장\n",
        "    with open(output_path, \"w\") as f:\n",
        "        json.dump(result_data, f, indent=4)\n",
        "    \n",
        "    print(f\"   Saved predictions to: results/{output_path.name}\")\n",
        "    \n",
        "    # 메모리 정리\n",
        "    del X_train, y_train, X_valid, y_valid, X_test, y_test\n",
        "    gc.collect()\n",
        "    \n",
        "    return best_info, test_mse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnBbrmCUcicE"
      },
      "source": [
        "**5. Main Execution Loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "45F_IT8WcmcV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "BASELINE: Dataset A\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: A\n",
            "============================================================\n",
            "   Result already exists: pred_linear_A.json\n",
            "   ✅ Cached Test MSE: 1015.6601\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: B_headlines_pca\n",
            "============================================================\n",
            "   Result already exists: pred_linear_B_headlines_pca.json\n",
            "   ✅ Cached Test MSE: 1105.7666\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: B_headlines_orig\n",
            "============================================================\n",
            "   Result already exists: pred_linear_B_headlines_orig.json\n",
            "   ✅ Cached Test MSE: 1098.6073\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: B_chunking_pca\n",
            "============================================================\n",
            "   Result already exists: pred_linear_B_chunking_pca.json\n",
            "   ✅ Cached Test MSE: 1104.6235\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: B_chunking_orig\n",
            "============================================================\n",
            "   Result already exists: pred_linear_B_chunking_orig.json\n",
            "   ✅ Cached Test MSE: 1099.9708\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: B_bodyText_pca\n",
            "============================================================\n",
            "   Result already exists: pred_linear_B_bodyText_pca.json\n",
            "   ✅ Cached Test MSE: 1096.2533\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: B_bodyText_orig\n",
            "============================================================\n",
            "   Result already exists: pred_linear_B_bodyText_orig.json\n",
            "   ✅ Cached Test MSE: 1098.0591\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: B_paragraphs_pca\n",
            "============================================================\n",
            "   Result already exists: pred_linear_B_paragraphs_pca.json\n",
            "   ✅ Cached Test MSE: 1101.5742\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: B_paragraphs_orig\n",
            "============================================================\n",
            "   Result already exists: pred_linear_B_paragraphs_orig.json\n",
            "   ✅ Cached Test MSE: 1102.7649\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: C_headlines_pca\n",
            "============================================================\n",
            "   Result already exists: pred_linear_C_headlines_pca.json\n",
            "   ✅ Cached Test MSE: 1105.7042\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: C_headlines_orig\n",
            "============================================================\n",
            "   Result already exists: pred_linear_C_headlines_orig.json\n",
            "   ✅ Cached Test MSE: 1098.6348\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: C_chunking_pca\n",
            "============================================================\n",
            "   Result already exists: pred_linear_C_chunking_pca.json\n",
            "   ✅ Cached Test MSE: 1104.1838\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: C_chunking_orig\n",
            "============================================================\n",
            "   Result already exists: pred_linear_C_chunking_orig.json\n",
            "   ✅ Cached Test MSE: 1099.9154\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: C_bodyText_pca\n",
            "============================================================\n",
            "   Result already exists: pred_linear_C_bodyText_pca.json\n",
            "   ✅ Cached Test MSE: 1096.2035\n",
            "\n",
            "============================================================\n",
            "Processing: C_bodyText_orig\n",
            "============================================================\n",
            "Loading dataset_C_bodyText_orig.parquet...\n",
            "   Using 'pub_date' for Date column\n",
            "   Loaded 460722 rows, 1137 columns\n",
            "   ✅ Date range: 2017-01-01 to 2019-12-31\n",
            "   After target creation: 460368 rows\n",
            "   Sample weights: min=0.0006, max=0.0040, mean=0.0016\n",
            "   Dropping columns: ['person', 'person_id', 'article_id', 'pub_date', 'value', 'target', 'sample_weight']\n",
            "   ⚠️ WARNING: Found datetime columns in X: ['article_date']\n",
            "   Removed datetime columns from X\n",
            "   Feature columns (1130): ['date_index', 'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'emb_0', 'emb_1', 'emb_2', 'emb_3']...\n",
            "   Train: 312185 rows\n",
            "   Valid: 76136 rows\n",
            "   Test:  72047 rows\n",
            "   Grid Search Configuration:\n",
            "   - Linear: 1 combination\n",
            "   - Ridge: 1 combinations (alphas=[0.1])\n",
            "   - Lasso: 1 combinations (alphas=[0.1])\n",
            "   - Total: 3 combinations\n",
            "\n",
            "   Training Linear...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     51\u001b[39m dname = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlevel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     info, mse = \u001b[43mtrain_linear_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     metrics_list.append({\n\u001b[32m     56\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m: level,\n\u001b[32m     57\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMethod\u001b[39m\u001b[33m\"\u001b[39m: method,\n\u001b[32m   (...)\u001b[39m\u001b[32m     60\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTest_MSE\u001b[39m\u001b[33m\"\u001b[39m: mse\n\u001b[32m     61\u001b[39m     })\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 91\u001b[39m, in \u001b[36mtrain_linear_models\u001b[39m\u001b[34m(dataset_name, file_path)\u001b[39m\n\u001b[32m     85\u001b[39m pipeline = Pipeline([\n\u001b[32m     86\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mscaler\u001b[39m\u001b[33m'\u001b[39m, StandardScaler()),\n\u001b[32m     87\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m, model_base)\n\u001b[32m     88\u001b[39m ])\n\u001b[32m     90\u001b[39m pipeline.fit(X_train, y_train, model__sample_weight=w_train)\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m val_pred = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m val_mse = mean_squared_error(y_valid, val_pred)\n\u001b[32m     94\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m      Valid MSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_mse\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mlproject/lib/python3.11/site-packages/sklearn/pipeline.py:788\u001b[39m, in \u001b[36mPipeline.predict\u001b[39m\u001b[34m(self, X, **params)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled():\n\u001b[32m    787\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iter(with_final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m         Xt = \u001b[43mtransform\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m1\u001b[39m].predict(Xt, **params)\n\u001b[32m    791\u001b[39m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mlproject/lib/python3.11/site-packages/sklearn/utils/_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mlproject/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:1075\u001b[39m, in \u001b[36mStandardScaler.transform\u001b[39m\u001b[34m(self, X, copy)\u001b[39m\n\u001b[32m   1072\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1074\u001b[39m copy = copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy\n\u001b[32m-> \u001b[39m\u001b[32m1075\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1084\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sparse.issparse(X):\n\u001b[32m   1087\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.with_mean:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mlproject/lib/python3.11/site-packages/sklearn/utils/validation.py:2954\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2952\u001b[39m         out = X, y\n\u001b[32m   2953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2954\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2955\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2956\u001b[39m     out = _check_y(y, **check_params)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mlproject/lib/python3.11/site-packages/sklearn/utils/validation.py:1115\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1114\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1115\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmay_share_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marray_orig\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1116\u001b[39m             array = _asarray_with_order(\n\u001b[32m   1117\u001b[39m                 array, dtype=dtype, order=order, copy=\u001b[38;5;28;01mTrue\u001b[39;00m, xp=xp\n\u001b[32m   1118\u001b[39m             )\n\u001b[32m   1119\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1120\u001b[39m         \u001b[38;5;66;03m# always make a copy for non-numpy arrays\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mlproject/lib/python3.11/site-packages/pandas/core/generic.py:2168\u001b[39m, in \u001b[36mNDFrame.__array__\u001b[39m\u001b[34m(self, dtype, copy)\u001b[39m\n\u001b[32m   2155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mgr.is_single_block \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.empty:\n\u001b[32m   2156\u001b[39m     \u001b[38;5;66;03m# check this manually, otherwise ._values will already return a copy\u001b[39;00m\n\u001b[32m   2157\u001b[39m     \u001b[38;5;66;03m# and np.array(values, copy=False) will not raise a warning\u001b[39;00m\n\u001b[32m   2158\u001b[39m     warnings.warn(\n\u001b[32m   2159\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mStarting with NumPy 2.0, the behavior of the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcopy\u001b[39m\u001b[33m'\u001b[39m\u001b[33m keyword has \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2160\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mchanged and passing \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcopy=False\u001b[39m\u001b[33m'\u001b[39m\u001b[33m raises an error when returning \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2166\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m   2167\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2168\u001b[39m values = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_values\u001b[49m\n\u001b[32m   2169\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2170\u001b[39m     \u001b[38;5;66;03m# Note: branch avoids `copy=None` for NumPy 1.x support\u001b[39;00m\n\u001b[32m   2171\u001b[39m     arr = np.asarray(values, dtype=dtype)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mlproject/lib/python3.11/site-packages/pandas/core/frame.py:1131\u001b[39m, in \u001b[36mDataFrame._values\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1129\u001b[39m blocks = mgr.blocks\n\u001b[32m   1130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(blocks) != \u001b[32m1\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1131\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ensure_wrapped_if_datetimelike(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m)\n\u001b[32m   1133\u001b[39m arr = blocks[\u001b[32m0\u001b[39m].values\n\u001b[32m   1134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m arr.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m   1135\u001b[39m     \u001b[38;5;66;03m# non-2D ExtensionArray\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mlproject/lib/python3.11/site-packages/pandas/core/frame.py:12691\u001b[39m, in \u001b[36mDataFrame.values\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m  12617\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m  12618\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> np.ndarray:\n\u001b[32m  12619\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m  12620\u001b[39m \u001b[33;03m    Return a Numpy representation of the DataFrame.\u001b[39;00m\n\u001b[32m  12621\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m  12689\u001b[39m \u001b[33;03m           ['monkey', nan, None]], dtype=object)\u001b[39;00m\n\u001b[32m  12690\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m> \u001b[39m\u001b[32m12691\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mlproject/lib/python3.11/site-packages/pandas/core/internals/managers.py:1713\u001b[39m, in \u001b[36mBlockManager.as_array\u001b[39m\u001b[34m(self, dtype, copy, na_value)\u001b[39m\n\u001b[32m   1711\u001b[39m         arr.flags.writeable = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1712\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1713\u001b[39m     arr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_interleave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1714\u001b[39m     \u001b[38;5;66;03m# The underlying data was copied within _interleave, so no need\u001b[39;00m\n\u001b[32m   1715\u001b[39m     \u001b[38;5;66;03m# to further copy if copy=True or setting na_value\u001b[39;00m\n\u001b[32m   1717\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m lib.no_default:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mlproject/lib/python3.11/site-packages/pandas/core/internals/managers.py:1746\u001b[39m, in \u001b[36mBlockManager._interleave\u001b[39m\u001b[34m(self, dtype, na_value)\u001b[39m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# error: Argument 1 to \"ensure_np_dtype\" has incompatible type\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;66;03m# \"Optional[dtype[Any]]\"; expected \"Union[dtype[Any], ExtensionDtype]\"\u001b[39;00m\n\u001b[32m   1745\u001b[39m dtype = ensure_np_dtype(dtype)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1746\u001b[39m result = np.empty(\u001b[38;5;28mself\u001b[39m.shape, dtype=dtype)\n\u001b[32m   1748\u001b[39m itemmask = np.zeros(\u001b[38;5;28mself\u001b[39m.shape[\u001b[32m0\u001b[39m])\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype == np.dtype(\u001b[33m\"\u001b[39m\u001b[33mobject\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m lib.no_default:\n\u001b[32m   1751\u001b[39m     \u001b[38;5;66;03m# much more performant than using to_numpy below\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# Main Execution Loop\n",
        "# ========================================\n",
        "\n",
        "levels = ['B', 'C', 'D']\n",
        "methods = ['headlines', 'chunking', 'bodyText', 'paragraphs']\n",
        "types = ['pca', 'orig']\n",
        "\n",
        "metrics_list = []\n",
        "\n",
        "# Dataset A (Baseline)\n",
        "path_A = DATA_DIR / \"dataset_A.parquet\"\n",
        "if path_A.exists():\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"BASELINE: Dataset A\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    try:\n",
        "        info, mse = train_linear_models(\"A\", path_A)\n",
        "        metrics_list.append({\n",
        "            \"Dataset\": \"A\",\n",
        "            \"Method\": \"-\",\n",
        "            \"Type\": \"-\",\n",
        "            \"Best_Model\": info,\n",
        "            \"Test_MSE\": mse\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error on Dataset A: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    finally:\n",
        "        gc.collect()\n",
        "else:\n",
        "    print(f\"Warning: {path_A} not found. Skipping Dataset A.\")\n",
        "\n",
        "# Dataset B, C, D\n",
        "total_datasets = len(levels) * len(methods) * len(types)\n",
        "current = 0\n",
        "\n",
        "for level in levels:\n",
        "    for method in methods:\n",
        "        for dtype in types:\n",
        "            current += 1\n",
        "            fname = f\"dataset_{level}_{method}_{dtype}.parquet\"\n",
        "            fpath = DATA_DIR / fname\n",
        "            \n",
        "            if not fpath.exists():\n",
        "                print(f\"\\n[{current}/{total_datasets}] Skipping {fname}: File not found.\")\n",
        "                continue\n",
        "            \n",
        "            dname = f\"{level}_{method}_{dtype}\"\n",
        "            \n",
        "            try:\n",
        "                info, mse = train_linear_models(dname, fpath)\n",
        "                metrics_list.append({\n",
        "                    \"Dataset\": level,\n",
        "                    \"Method\": method,\n",
        "                    \"Type\": dtype,\n",
        "                    \"Best_Model\": info,\n",
        "                    \"Test_MSE\": mse\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"\\n❌ Error on {dname}: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "            finally:\n",
        "                gc.collect()\n",
        "\n",
        "# ========================================\n",
        "# 최종 결과 정리 및 저장\n",
        "# ========================================\n",
        "final_df = pd.DataFrame(metrics_list).sort_values(\"Test_MSE\")\n",
        "\n",
        "# ✅ OUTPUT_DIR (results_lr/)에 CSV 저장\n",
        "csv_path = OUTPUT_DIR / \"linear_evaluation_metrics.csv\"\n",
        "final_df.to_csv(csv_path, index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ALL TASKS COMPLETED\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nResults saved to: {csv_path}\")\n",
        "print(f\"\\nTop 10 Models by Test MSE:\")\n",
        "print(final_df.head(10))\n",
        "\n",
        "# 10개 60분"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mlproject",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
