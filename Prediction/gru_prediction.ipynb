{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# GRU Stock Price Prediction\n",
        "\n",
        "## Configuration\n",
        "- Sequence Length: 5 days\n",
        "- Hidden Dim: 64\n",
        "- Num Layers: 2\n",
        "- Dropout: 0.3\n",
        "- Batch Size: 128\n",
        "- Epochs: 100 (Early Stopping: patience=10)\n",
        "- Sample Weighting: Enabled\n",
        "- Target: Next day stock value\n",
        "- Normalization: MinMaxScaler\n",
        "- Aggregation: Article-level prediction → Daily average"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import"
      },
      "source": [
        "## 0. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "imports"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.9.0+cu128\n",
            "CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import gc\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "path"
      },
      "source": [
        "## 1. Path Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "paths"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Source: ../feature_datasets\n",
            "Output Path: results_gru\n",
            "Results Path: results_gru/results\n"
          ]
        }
      ],
      "source": [
        "# Path configuration\n",
        "# Google Colab:\n",
        "# DATA_DIR = Path(\"/content/drive/MyDrive/COSE362/data/feature_engineering\")\n",
        "# OUTPUT_DIR = Path(\"/content/drive/MyDrive/COSE362/data/prediction_output/results_gru\")\n",
        "\n",
        "# Local:\n",
        "DATA_DIR = Path(\"../feature_datasets\")\n",
        "OUTPUT_DIR = Path(\"results_gru\")\n",
        "\n",
        "RESULTS_DIR = OUTPUT_DIR / \"results\"\n",
        "\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Data Source: {DATA_DIR}\")\n",
        "print(f\"Output Path: {OUTPUT_DIR}\")\n",
        "print(f\"Results Path: {RESULTS_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "device"
      },
      "source": [
        "## 2. Device Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "device_config"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "GPU: NVIDIA GeForce RTX 4090\n",
            "Memory: 25.26 GB\n"
          ]
        }
      ],
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyperparams"
      },
      "source": [
        "## 3. Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "config"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GRU Configuration:\n",
            "  seq_length: 5\n",
            "  hidden_dim: 64\n",
            "  num_layers: 2\n",
            "  dropout: 0.3\n",
            "  batch_size: 128\n",
            "  epochs: 100\n",
            "  early_stopping_patience: 10\n",
            "  learning_rate: 0.001\n"
          ]
        }
      ],
      "source": [
        "# GRU Hyperparameters\n",
        "CONFIG = {\n",
        "    'seq_length': 5,\n",
        "    'hidden_dim': 64,\n",
        "    'num_layers': 2,\n",
        "    'dropout': 0.3,\n",
        "    'batch_size': 128,\n",
        "    'epochs': 100,\n",
        "    'early_stopping_patience': 10,\n",
        "    'learning_rate': 0.001,\n",
        "}\n",
        "\n",
        "print(\"GRU Configuration:\")\n",
        "for key, value in CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_loading"
      },
      "source": [
        "## 4. Data Loading Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "load_data"
      },
      "outputs": [],
      "source": [
        "def load_data(file_path):\n",
        "    \"\"\"\n",
        "    Load parquet file and create Date column\n",
        "    \"\"\"\n",
        "    print(f\"Loading {file_path.name}...\")\n",
        "    df = pd.read_parquet(file_path)\n",
        "    \n",
        "    # Sort by date_index\n",
        "    if 'date_index' in df.columns:\n",
        "        df = df.sort_values('date_index').reset_index(drop=True)\n",
        "    \n",
        "    # Create Date column\n",
        "    if 'pub_date' in df.columns:\n",
        "        df['Date'] = pd.to_datetime(df['pub_date'], format='%Y_%m_%d')\n",
        "        print(f\"   Using 'pub_date' for Date column\")\n",
        "    elif 'date_str' in df.columns:\n",
        "        df['Date'] = pd.to_datetime(df['date_str'], format='%Y_%m_%d')\n",
        "        print(f\"   Using 'date_str' for Date column\")\n",
        "    elif 'date_index' in df.columns:\n",
        "        base_date = pd.to_datetime('2017-01-01')\n",
        "        df['Date'] = base_date + pd.to_timedelta(df['date_index'], unit='D')\n",
        "        print(f\"   Using 'date_index' for Date column (base: 2017-01-01)\")\n",
        "    else:\n",
        "        print(\"   [Warning] No date column found. Using default range.\")\n",
        "        df['Date'] = pd.date_range(start='2017-01-01', periods=len(df), freq='D')\n",
        "    \n",
        "    print(f\"   Loaded {len(df)} rows, {len(df.columns)} columns\")\n",
        "    print(f\"   ✅ Date range: {df['Date'].min().strftime('%Y-%m-%d')} to {df['Date'].max().strftime('%Y-%m-%d')}\")\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preprocessing"
      },
      "source": [
        "## 5. Preprocessing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "preprocess"
      },
      "outputs": [],
      "source": [
        "def preprocess_and_split(df, target_col='value'):\n",
        "    \"\"\"\n",
        "    Preprocess data and split into train/valid/test\n",
        "    \n",
        "    Features to DROP:\n",
        "    - lag_1, lag_2, lag_3, lag_4, lag_5 (GRU learns temporal patterns)\n",
        "    - fg_value (look-ahead bias)\n",
        "    - metadata columns\n",
        "    \n",
        "    Features to KEEP:\n",
        "    - date_index (temporal trend)\n",
        "    - emb_* or pca_* (text embeddings)\n",
        "    - person_* (one-hot vectors)\n",
        "    - fg_lag_* (economic indicators, lagged)\n",
        "    \"\"\"\n",
        "    \n",
        "    # ========================================\n",
        "    # 1. Target Creation (next day value)\n",
        "    # ========================================\n",
        "    daily_prices = df[['date_index', target_col]].drop_duplicates().sort_values('date_index')\n",
        "    daily_prices['target'] = daily_prices[target_col].shift(-1)\n",
        "    \n",
        "    df = df.drop(columns=['target'], errors='ignore')\n",
        "    df = df.merge(daily_prices[['date_index', 'target']], on='date_index', how='left')\n",
        "    df = df.dropna(subset=['target'])\n",
        "    \n",
        "    print(f\"   After target creation: {len(df)} rows\")\n",
        "    \n",
        "    # ========================================\n",
        "    # 2. Sample Weight Calculation\n",
        "    # ========================================\n",
        "    date_counts = df['date_index'].value_counts()\n",
        "    df['sample_weight'] = df['date_index'].map(lambda x: 1.0 / date_counts[x])\n",
        "    \n",
        "    print(f\"   Sample weights: min={df['sample_weight'].min():.4f}, \"\n",
        "          f\"max={df['sample_weight'].max():.4f}, mean={df['sample_weight'].mean():.4f}\")\n",
        "    \n",
        "    # ========================================\n",
        "    # 3. Extract Date for splitting\n",
        "    # ========================================\n",
        "    if 'Date' not in df.columns:\n",
        "        raise ValueError(\"'Date' column not found in dataframe\")\n",
        "    \n",
        "    dates = df['Date'].copy()\n",
        "    date_indices = df['date_index'].copy()\n",
        "    df = df.drop(columns=['Date'])\n",
        "    \n",
        "    # ========================================\n",
        "    # 4. Columns to Drop\n",
        "    # ========================================\n",
        "    cols_to_drop = [\n",
        "        # Metadata\n",
        "        'person', 'person_id', 'article_id',\n",
        "        \n",
        "        # Date columns\n",
        "        'pub_date', 'article_date',\n",
        "        \n",
        "        # Target related\n",
        "        'value',      # Current stock price\n",
        "        'target',     # Target (will be extracted separately)\n",
        "        \n",
        "        # Lag features (GRU learns temporal patterns itself)\n",
        "        'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5',\n",
        "        \n",
        "        # Fear-Greed current value (look-ahead bias)\n",
        "        'fg_value',\n",
        "        \n",
        "        # Weight (will be extracted separately)\n",
        "        'sample_weight',\n",
        "    ]\n",
        "    \n",
        "    actual_drop = [c for c in cols_to_drop if c in df.columns]\n",
        "    print(f\"   Dropping columns: {actual_drop}\")\n",
        "    \n",
        "    # ========================================\n",
        "    # 5. Extract X, y, weights\n",
        "    # ========================================\n",
        "    X = df.drop(columns=actual_drop, errors='ignore')\n",
        "    y = df['target'].copy()\n",
        "    weights = df['sample_weight'].copy()\n",
        "    \n",
        "    # Remove datetime/object columns\n",
        "    datetime_cols = X.select_dtypes(include=['datetime64']).columns.tolist()\n",
        "    if datetime_cols:\n",
        "        print(f\"   ⚠️ Removing datetime columns: {datetime_cols}\")\n",
        "        X = X.drop(columns=datetime_cols)\n",
        "    \n",
        "    object_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
        "    if object_cols:\n",
        "        print(f\"   ⚠️ WARNING: Found non-numeric columns: {object_cols}\")\n",
        "        print(f\"   Removing them...\")\n",
        "        X = X.drop(columns=object_cols)\n",
        "    \n",
        "    print(f\"   ✅ Feature columns ({len(X.columns)}): {list(X.columns[:10])}...\")\n",
        "    print(f\"   ✅ All features are numeric: {X.dtypes.apply(lambda x: x.kind in 'biufc').all()}\")\n",
        "    \n",
        "    # ========================================\n",
        "    # 6. Train/Valid/Test Split\n",
        "    # ========================================\n",
        "    train_mask = (dates <= '2018-12-31')\n",
        "    valid_mask = (dates >= '2019-01-01') & (dates <= '2019-06-30')\n",
        "    test_mask = (dates >= '2019-07-01')\n",
        "    \n",
        "    print(f\"   Train: {train_mask.sum()} rows\")\n",
        "    print(f\"   Valid: {valid_mask.sum()} rows\")\n",
        "    print(f\"   Test:  {test_mask.sum()} rows\")\n",
        "    \n",
        "    if train_mask.sum() == 0 or test_mask.sum() == 0:\n",
        "        raise ValueError(\"Train or Test set is empty!\")\n",
        "    \n",
        "    return (\n",
        "        (X[train_mask], y[train_mask], weights[train_mask], date_indices[train_mask]),\n",
        "        (X[valid_mask], y[valid_mask], weights[valid_mask], date_indices[valid_mask]),\n",
        "        (X[test_mask], y[test_mask], weights[test_mask], date_indices[test_mask], dates[test_mask])\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset"
      },
      "source": [
        "## 6. Weighted Time Series Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dataset_class"
      },
      "outputs": [],
      "source": [
        "class WeightedTimeSeriesDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Time series dataset with sample weighting\n",
        "    \n",
        "    Returns sequences of length seq_length with corresponding weights\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, weights, seq_length):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            X: (n_samples, n_features) numpy array\n",
        "            y: (n_samples,) numpy array\n",
        "            weights: (n_samples,) numpy array\n",
        "            seq_length: int, sequence length\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.weights = weights\n",
        "        self.seq_length = seq_length\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.X) - self.seq_length + 1\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            X_seq: (seq_length, n_features)\n",
        "            y_target: (1,)\n",
        "            weight: (1,)\n",
        "        \"\"\"\n",
        "        # Sequence: idx ~ idx+seq_length-1\n",
        "        X_seq = self.X[idx : idx + self.seq_length]\n",
        "        \n",
        "        # Target: last time step\n",
        "        y_target = self.y[idx + self.seq_length - 1]\n",
        "        \n",
        "        # Weight: last time step\n",
        "        weight = self.weights[idx + self.seq_length - 1]\n",
        "        \n",
        "        return (\n",
        "            torch.FloatTensor(X_seq),\n",
        "            torch.FloatTensor([y_target]),\n",
        "            torch.FloatTensor([weight])\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model"
      },
      "source": [
        "## 7. GRU Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "model_class"
      },
      "outputs": [],
      "source": [
        "class GRUModel(nn.Module):\n",
        "    \"\"\"\n",
        "    GRU-based stock price prediction model\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim=64, num_layers=2, dropout=0.3):\n",
        "        super(GRUModel, self).__init__()\n",
        "        \n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.gru = nn.GRU(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch, seq_length, input_dim)\n",
        "        Returns:\n",
        "            out: (batch, 1)\n",
        "        \"\"\"\n",
        "        # GRU forward\n",
        "        gru_out, _ = self.gru(x)\n",
        "        \n",
        "        # Use last time step output\n",
        "        last_out = gru_out[:, -1, :]  # (batch, hidden_dim)\n",
        "        \n",
        "        # Fully connected layer\n",
        "        out = self.fc(last_out)  # (batch, 1)\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "early_stopping"
      },
      "source": [
        "## 8. Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "early_stopping_class"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"\n",
        "    Early stopping to stop training when validation loss doesn't improve\n",
        "    \"\"\"\n",
        "    def __init__(self, patience=10, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "    \n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss - self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training"
      },
      "source": [
        "## 9. Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "train_function"
      },
      "outputs": [],
      "source": [
        "def train_gru_model(dataset_name, file_path):\n",
        "    \"\"\"\n",
        "    Train GRU model with early stopping and sample weighting\n",
        "    \n",
        "    Returns:\n",
        "        test_mse: float\n",
        "    \"\"\"\n",
        "    \n",
        "    # ========================================\n",
        "    # 0. Check if result already exists\n",
        "    # ========================================\n",
        "    output_path = RESULTS_DIR / f\"pred_gru_{dataset_name}.json\"\n",
        "    \n",
        "    if output_path.exists():\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"⏭️  SKIPPING: {dataset_name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"   Result already exists: {output_path.name}\")\n",
        "        \n",
        "        try:\n",
        "            with open(output_path, 'r') as f:\n",
        "                result_data = json.load(f)\n",
        "            \n",
        "            actuals = [item['actual'] for item in result_data]\n",
        "            preds = [item['predicted'] for item in result_data]\n",
        "            mse = mean_squared_error(actuals, preds)\n",
        "            \n",
        "            print(f\"   ✅ Cached Test MSE: {mse:.4f}\")\n",
        "            return mse\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ⚠️ Warning: Could not read cached MSE: {e}\")\n",
        "            print(f\"   Re-running experiment...\")\n",
        "    \n",
        "    # ========================================\n",
        "    # 1. Load and preprocess data\n",
        "    # ========================================\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Processing: {dataset_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    df = load_data(file_path)\n",
        "    \n",
        "    (X_train, y_train, w_train, date_idx_train), \\\n",
        "    (X_valid, y_valid, w_valid, date_idx_valid), \\\n",
        "    (X_test, y_test, w_test, date_idx_test, dates_test) = preprocess_and_split(df)\n",
        "    \n",
        "    del df\n",
        "    gc.collect()\n",
        "    \n",
        "    # ========================================\n",
        "    # 2. Normalization (MinMaxScaler)\n",
        "    # ========================================\n",
        "    print(f\"\\n   Normalizing features...\")\n",
        "    \n",
        "    scaler_x = MinMaxScaler()\n",
        "    scaler_y = MinMaxScaler()\n",
        "    \n",
        "    X_train_scaled = scaler_x.fit_transform(X_train.values).astype(np.float32)\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).flatten().astype(np.float32)\n",
        "    \n",
        "    X_valid_scaled = scaler_x.transform(X_valid.values).astype(np.float32)\n",
        "    y_valid_scaled = scaler_y.transform(y_valid.values.reshape(-1, 1)).flatten().astype(np.float32)\n",
        "    \n",
        "    X_test_scaled = scaler_x.transform(X_test.values).astype(np.float32)\n",
        "    y_test_raw = y_test.values.astype(np.float32)\n",
        "    \n",
        "    w_train = w_train.values.astype(np.float32)\n",
        "    w_valid = w_valid.values.astype(np.float32)\n",
        "    w_test = w_test.values.astype(np.float32)\n",
        "    \n",
        "    print(f\"   ✅ Features normalized to [0, 1]\")\n",
        "    \n",
        "    # ========================================\n",
        "    # 3. Create datasets and dataloaders\n",
        "    # ========================================\n",
        "    seq_length = CONFIG['seq_length']\n",
        "    batch_size = CONFIG['batch_size']\n",
        "    \n",
        "    train_dataset = WeightedTimeSeriesDataset(X_train_scaled, y_train_scaled, w_train, seq_length)\n",
        "    valid_dataset = WeightedTimeSeriesDataset(X_valid_scaled, y_valid_scaled, w_valid, seq_length)\n",
        "    test_dataset = WeightedTimeSeriesDataset(X_test_scaled, y_test_raw, w_test, seq_length)\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "    \n",
        "    print(f\"\\n   Dataset sizes:\")\n",
        "    print(f\"   - Train: {len(train_dataset)} samples\")\n",
        "    print(f\"   - Valid: {len(valid_dataset)} samples\")\n",
        "    print(f\"   - Test: {len(test_dataset)} samples\")\n",
        "    \n",
        "    # ========================================\n",
        "    # 4. Initialize model\n",
        "    # ========================================\n",
        "    input_dim = X_train_scaled.shape[1]\n",
        "    \n",
        "    model = GRUModel(\n",
        "        input_dim=input_dim,\n",
        "        hidden_dim=CONFIG['hidden_dim'],\n",
        "        num_layers=CONFIG['num_layers'],\n",
        "        dropout=CONFIG['dropout']\n",
        "    ).to(DEVICE)\n",
        "    \n",
        "    criterion = nn.MSELoss(reduction='none')\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])\n",
        "    early_stopping = EarlyStopping(patience=CONFIG['early_stopping_patience'])\n",
        "    \n",
        "    print(f\"\\n   Model initialized:\")\n",
        "    print(f\"   - Input dim: {input_dim}\")\n",
        "    print(f\"   - Hidden dim: {CONFIG['hidden_dim']}\")\n",
        "    print(f\"   - Num layers: {CONFIG['num_layers']}\")\n",
        "    print(f\"   - Dropout: {CONFIG['dropout']}\")\n",
        "    print(f\"   - Parameters: {sum(p.numel() for p in model.parameters())}\")\n",
        "    \n",
        "    # ========================================\n",
        "    # 5. Training loop\n",
        "    # ========================================\n",
        "    print(f\"\\n   Training started...\")\n",
        "    \n",
        "    best_val_loss = float('inf')\n",
        "    \n",
        "    for epoch in range(CONFIG['epochs']):\n",
        "        # Train\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_batches = 0\n",
        "        \n",
        "        for X_batch, y_batch, w_batch in train_loader:\n",
        "            X_batch = X_batch.to(DEVICE)\n",
        "            y_batch = y_batch.to(DEVICE)\n",
        "            w_batch = w_batch.to(DEVICE)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward\n",
        "            predictions = model(X_batch)\n",
        "            \n",
        "            # Weighted MSE loss\n",
        "            losses = criterion(predictions, y_batch)\n",
        "            weighted_loss = (losses * w_batch).mean()\n",
        "            \n",
        "            # Backward\n",
        "            weighted_loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += weighted_loss.item()\n",
        "            train_batches += 1\n",
        "        \n",
        "        train_loss /= train_batches\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_batches = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch, w_batch in valid_loader:\n",
        "                X_batch = X_batch.to(DEVICE)\n",
        "                y_batch = y_batch.to(DEVICE)\n",
        "                w_batch = w_batch.to(DEVICE)\n",
        "                \n",
        "                predictions = model(X_batch)\n",
        "                losses = criterion(predictions, y_batch)\n",
        "                weighted_loss = (losses * w_batch).mean()\n",
        "                \n",
        "                val_loss += weighted_loss.item()\n",
        "                val_batches += 1\n",
        "        \n",
        "        val_loss /= val_batches\n",
        "        \n",
        "        # Logging\n",
        "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "            print(f\"   Epoch [{epoch+1:3d}/{CONFIG['epochs']}] \"\n",
        "                  f\"Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
        "        \n",
        "        # Early stopping\n",
        "        early_stopping(val_loss)\n",
        "        if early_stopping.early_stop:\n",
        "            print(f\"\\n   ⚠️ Early stopping triggered at epoch {epoch+1}\")\n",
        "            break\n",
        "        \n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "    \n",
        "    print(f\"\\n   ✅ Training completed\")\n",
        "    print(f\"   ✅ Best validation loss: {best_val_loss:.6f}\")\n",
        "    \n",
        "    # ========================================\n",
        "    # 6. Test prediction\n",
        "    # ========================================\n",
        "    print(f\"\\n   Predicting on test set...\")\n",
        "    \n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for X_batch, _, _ in test_loader:\n",
        "            X_batch = X_batch.to(DEVICE)\n",
        "            predictions = model(X_batch)\n",
        "            all_preds.append(predictions.cpu().numpy())\n",
        "    \n",
        "    test_preds_scaled = np.vstack(all_preds).flatten()\n",
        "    test_preds = scaler_y.inverse_transform(test_preds_scaled.reshape(-1, 1)).flatten()\n",
        "    \n",
        "    # Align test data (first seq_length-1 samples are lost)\n",
        "    y_test_aligned = y_test_raw[seq_length-1:]\n",
        "    date_idx_test_aligned = date_idx_test.values[seq_length-1:]\n",
        "    dates_test_aligned = dates_test.values[seq_length-1:]\n",
        "    \n",
        "    # ========================================\n",
        "    # 7. Daily aggregation (article-level → daily)\n",
        "    # ========================================\n",
        "    results_df = pd.DataFrame({\n",
        "        'date_index': date_idx_test_aligned,\n",
        "        'date': dates_test_aligned,\n",
        "        'actual': y_test_aligned,\n",
        "        'predicted': test_preds\n",
        "    })\n",
        "    \n",
        "    # Aggregate by date\n",
        "    daily_results = results_df.groupby('date_index').agg({\n",
        "        'date': 'first',\n",
        "        'actual': 'first',  # Same value for all articles on same day\n",
        "        'predicted': 'mean'  # Average predictions\n",
        "    }).reset_index()\n",
        "    \n",
        "    # Calculate MSE\n",
        "    test_mse = mean_squared_error(daily_results['actual'], daily_results['predicted'])\n",
        "    \n",
        "    print(f\"\\n   ✅ Test MSE (Daily): {test_mse:.4f}\")\n",
        "    \n",
        "    # ========================================\n",
        "    # 8. Save results\n",
        "    # ========================================\n",
        "    result_data = []\n",
        "    for _, row in daily_results.iterrows():\n",
        "        result_data.append({\n",
        "            \"date\": pd.to_datetime(row['date']).strftime('%Y-%m-%d'),\n",
        "            \"actual\": float(row['actual']),\n",
        "            \"predicted\": float(row['predicted'])\n",
        "        })\n",
        "    \n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(result_data, f, indent=4)\n",
        "    \n",
        "    print(f\"   Saved predictions to: results/{output_path.name}\")\n",
        "    \n",
        "    # Memory cleanup\n",
        "    del model, optimizer, train_loader, valid_loader, test_loader\n",
        "    del X_train_scaled, X_valid_scaled, X_test_scaled\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    \n",
        "    return test_mse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "main"
      },
      "source": [
        "## 10. Main Execution Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "main_execution"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "BASELINE: Dataset A\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: A\n",
            "============================================================\n",
            "   Result already exists: pred_gru_A.json\n",
            "   ✅ Cached Test MSE: 6204.1334\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: B_headlines_pca\n",
            "============================================================\n",
            "   Result already exists: pred_gru_B_headlines_pca.json\n",
            "   ✅ Cached Test MSE: 467408.1265\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: B_headlines_orig\n",
            "============================================================\n",
            "   Result already exists: pred_gru_B_headlines_orig.json\n",
            "   ✅ Cached Test MSE: 352727.5294\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: B_chunking_pca\n",
            "============================================================\n",
            "   Result already exists: pred_gru_B_chunking_pca.json\n",
            "   ✅ Cached Test MSE: 375878.6068\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: B_chunking_orig\n",
            "============================================================\n",
            "   Result already exists: pred_gru_B_chunking_orig.json\n",
            "   ✅ Cached Test MSE: 370900.7300\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: B_bodyText_pca\n",
            "============================================================\n",
            "   Result already exists: pred_gru_B_bodyText_pca.json\n",
            "   ✅ Cached Test MSE: 470726.3200\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: B_bodyText_orig\n",
            "============================================================\n",
            "   Result already exists: pred_gru_B_bodyText_orig.json\n",
            "   ✅ Cached Test MSE: 359824.3152\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: B_paragraphs_pca\n",
            "============================================================\n",
            "   Result already exists: pred_gru_B_paragraphs_pca.json\n",
            "   ✅ Cached Test MSE: 433227.9489\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: B_paragraphs_orig\n",
            "============================================================\n",
            "   Result already exists: pred_gru_B_paragraphs_orig.json\n",
            "   ✅ Cached Test MSE: 334572.0592\n",
            "\n",
            "============================================================\n",
            "⏭️  SKIPPING: C_headlines_pca\n",
            "============================================================\n",
            "   Result already exists: pred_gru_C_headlines_pca.json\n",
            "   ✅ Cached Test MSE: 426530.4826\n",
            "\n",
            "============================================================\n",
            "Processing: C_headlines_orig\n",
            "============================================================\n",
            "Loading dataset_C_headlines_orig.parquet...\n",
            "   Using 'pub_date' for Date column\n",
            "   Loaded 461270 rows, 1137 columns\n",
            "   ✅ Date range: 2017-01-01 to 2019-12-31\n",
            "   After target creation: 460915 rows\n",
            "   Sample weights: min=0.0006, max=0.0040, mean=0.0016\n",
            "   Dropping columns: ['person', 'person_id', 'article_id', 'pub_date', 'article_date', 'value', 'target', 'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'sample_weight']\n",
            "   ✅ Feature columns (1125): ['date_index', 'emb_0', 'emb_1', 'emb_2', 'emb_3', 'emb_4', 'emb_5', 'emb_6', 'emb_7', 'emb_8']...\n",
            "   ✅ All features are numeric: True\n",
            "   Train: 312535 rows\n",
            "   Valid: 76232 rows\n",
            "   Test:  72148 rows\n",
            "\n",
            "   Normalizing features...\n",
            "   ✅ Features normalized to [0, 1]\n",
            "\n",
            "   Dataset sizes:\n",
            "   - Train: 312531 samples\n",
            "   - Valid: 76228 samples\n",
            "   - Test: 72144 samples\n",
            "\n",
            "   Model initialized:\n",
            "   - Input dim: 1125\n",
            "   - Hidden dim: 64\n",
            "   - Num layers: 2\n",
            "   - Dropout: 0.3\n",
            "   - Parameters: 253697\n",
            "\n",
            "   Training started...\n",
            "   Epoch [  1/100] Train Loss: 0.000034, Val Loss: 0.000368\n",
            "   Epoch [ 10/100] Train Loss: 0.000006, Val Loss: 0.000479\n",
            "\n",
            "   ⚠️ Early stopping triggered at epoch 11\n",
            "\n",
            "   ✅ Training completed\n",
            "   ✅ Best validation loss: 0.000368\n",
            "\n",
            "   Predicting on test set...\n",
            "\n",
            "   ✅ Test MSE (Daily): 347663.7188\n",
            "   Saved predictions to: results/pred_gru_C_headlines_orig.json\n",
            "\n",
            "============================================================\n",
            "Processing: C_chunking_pca\n",
            "============================================================\n",
            "Loading dataset_C_chunking_pca.parquet...\n",
            "   Using 'pub_date' for Date column\n",
            "   Loaded 460722 rows, 357 columns\n",
            "   ✅ Date range: 2017-01-01 to 2019-12-31\n",
            "   After target creation: 460368 rows\n",
            "   Sample weights: min=0.0006, max=0.0040, mean=0.0016\n",
            "   Dropping columns: ['person', 'person_id', 'article_id', 'pub_date', 'article_date', 'value', 'target', 'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'sample_weight']\n",
            "   ✅ Feature columns (345): ['date_index', 'pca_0', 'pca_1', 'pca_2', 'pca_3', 'pca_4', 'pca_5', 'pca_6', 'pca_7', 'pca_8']...\n",
            "   ✅ All features are numeric: True\n",
            "   Train: 312185 rows\n",
            "   Valid: 76136 rows\n",
            "   Test:  72047 rows\n",
            "\n",
            "   Normalizing features...\n",
            "   ✅ Features normalized to [0, 1]\n",
            "\n",
            "   Dataset sizes:\n",
            "   - Train: 312181 samples\n",
            "   - Valid: 76132 samples\n",
            "   - Test: 72043 samples\n",
            "\n",
            "   Model initialized:\n",
            "   - Input dim: 345\n",
            "   - Hidden dim: 64\n",
            "   - Num layers: 2\n",
            "   - Dropout: 0.3\n",
            "   - Parameters: 103937\n",
            "\n",
            "   Training started...\n",
            "   Epoch [  1/100] Train Loss: 0.000021, Val Loss: 0.000483\n",
            "   Epoch [ 10/100] Train Loss: 0.000005, Val Loss: 0.000640\n",
            "\n",
            "   ⚠️ Early stopping triggered at epoch 11\n",
            "\n",
            "   ✅ Training completed\n",
            "   ✅ Best validation loss: 0.000483\n",
            "\n",
            "   Predicting on test set...\n",
            "\n",
            "   ✅ Test MSE (Daily): 428712.9062\n",
            "   Saved predictions to: results/pred_gru_C_chunking_pca.json\n",
            "\n",
            "============================================================\n",
            "Processing: C_chunking_orig\n",
            "============================================================\n",
            "Loading dataset_C_chunking_orig.parquet...\n",
            "   Using 'pub_date' for Date column\n",
            "   Loaded 460722 rows, 1137 columns\n",
            "   ✅ Date range: 2017-01-01 to 2019-12-31\n",
            "   After target creation: 460368 rows\n",
            "   Sample weights: min=0.0006, max=0.0040, mean=0.0016\n",
            "   Dropping columns: ['person', 'person_id', 'article_id', 'pub_date', 'article_date', 'value', 'target', 'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'sample_weight']\n",
            "   ✅ Feature columns (1125): ['date_index', 'emb_0', 'emb_1', 'emb_2', 'emb_3', 'emb_4', 'emb_5', 'emb_6', 'emb_7', 'emb_8']...\n",
            "   ✅ All features are numeric: True\n",
            "   Train: 312185 rows\n",
            "   Valid: 76136 rows\n",
            "   Test:  72047 rows\n",
            "\n",
            "   Normalizing features...\n",
            "   ✅ Features normalized to [0, 1]\n",
            "\n",
            "   Dataset sizes:\n",
            "   - Train: 312181 samples\n",
            "   - Valid: 76132 samples\n",
            "   - Test: 72043 samples\n",
            "\n",
            "   Model initialized:\n",
            "   - Input dim: 1125\n",
            "   - Hidden dim: 64\n",
            "   - Num layers: 2\n",
            "   - Dropout: 0.3\n",
            "   - Parameters: 253697\n",
            "\n",
            "   Training started...\n",
            "   Epoch [  1/100] Train Loss: 0.000034, Val Loss: 0.000168\n",
            "   Epoch [ 10/100] Train Loss: 0.000007, Val Loss: 0.000421\n",
            "\n",
            "   ⚠️ Early stopping triggered at epoch 11\n",
            "\n",
            "   ✅ Training completed\n",
            "   ✅ Best validation loss: 0.000168\n",
            "\n",
            "   Predicting on test set...\n",
            "\n",
            "   ✅ Test MSE (Daily): 369582.6875\n",
            "   Saved predictions to: results/pred_gru_C_chunking_orig.json\n",
            "\n",
            "============================================================\n",
            "Processing: C_bodyText_pca\n",
            "============================================================\n",
            "Loading dataset_C_bodyText_pca.parquet...\n",
            "   Using 'pub_date' for Date column\n",
            "   Loaded 460722 rows, 304 columns\n",
            "   ✅ Date range: 2017-01-01 to 2019-12-31\n",
            "   After target creation: 460368 rows\n",
            "   Sample weights: min=0.0006, max=0.0040, mean=0.0016\n",
            "   Dropping columns: ['person', 'person_id', 'article_id', 'pub_date', 'article_date', 'value', 'target', 'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'sample_weight']\n",
            "   ✅ Feature columns (292): ['date_index', 'pca_0', 'pca_1', 'pca_2', 'pca_3', 'pca_4', 'pca_5', 'pca_6', 'pca_7', 'pca_8']...\n",
            "   ✅ All features are numeric: True\n",
            "   Train: 312185 rows\n",
            "   Valid: 76136 rows\n",
            "   Test:  72047 rows\n",
            "\n",
            "   Normalizing features...\n",
            "   ✅ Features normalized to [0, 1]\n",
            "\n",
            "   Dataset sizes:\n",
            "   - Train: 312181 samples\n",
            "   - Valid: 76132 samples\n",
            "   - Test: 72043 samples\n",
            "\n",
            "   Model initialized:\n",
            "   - Input dim: 292\n",
            "   - Hidden dim: 64\n",
            "   - Num layers: 2\n",
            "   - Dropout: 0.3\n",
            "   - Parameters: 93761\n",
            "\n",
            "   Training started...\n",
            "   Epoch [  1/100] Train Loss: 0.000020, Val Loss: 0.000577\n",
            "   Epoch [ 10/100] Train Loss: 0.000004, Val Loss: 0.000584\n",
            "\n",
            "   ⚠️ Early stopping triggered at epoch 17\n",
            "\n",
            "   ✅ Training completed\n",
            "   ✅ Best validation loss: 0.000541\n",
            "\n",
            "   Predicting on test set...\n",
            "\n",
            "   ✅ Test MSE (Daily): 414691.4688\n",
            "   Saved predictions to: results/pred_gru_C_bodyText_pca.json\n",
            "\n",
            "============================================================\n",
            "Processing: C_bodyText_orig\n",
            "============================================================\n",
            "Loading dataset_C_bodyText_orig.parquet...\n",
            "   Using 'pub_date' for Date column\n",
            "   Loaded 460722 rows, 1137 columns\n",
            "   ✅ Date range: 2017-01-01 to 2019-12-31\n",
            "   After target creation: 460368 rows\n",
            "   Sample weights: min=0.0006, max=0.0040, mean=0.0016\n",
            "   Dropping columns: ['person', 'person_id', 'article_id', 'pub_date', 'article_date', 'value', 'target', 'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'sample_weight']\n",
            "   ✅ Feature columns (1125): ['date_index', 'emb_0', 'emb_1', 'emb_2', 'emb_3', 'emb_4', 'emb_5', 'emb_6', 'emb_7', 'emb_8']...\n",
            "   ✅ All features are numeric: True\n",
            "   Train: 312185 rows\n",
            "   Valid: 76136 rows\n",
            "   Test:  72047 rows\n",
            "\n",
            "   Normalizing features...\n",
            "   ✅ Features normalized to [0, 1]\n",
            "\n",
            "   Dataset sizes:\n",
            "   - Train: 312181 samples\n",
            "   - Valid: 76132 samples\n",
            "   - Test: 72043 samples\n",
            "\n",
            "   Model initialized:\n",
            "   - Input dim: 1125\n",
            "   - Hidden dim: 64\n",
            "   - Num layers: 2\n",
            "   - Dropout: 0.3\n",
            "   - Parameters: 253697\n",
            "\n",
            "   Training started...\n",
            "   Epoch [  1/100] Train Loss: 0.000036, Val Loss: 0.000194\n",
            "   Epoch [ 10/100] Train Loss: 0.000006, Val Loss: 0.000436\n",
            "\n",
            "   ⚠️ Early stopping triggered at epoch 11\n",
            "\n",
            "   ✅ Training completed\n",
            "   ✅ Best validation loss: 0.000194\n",
            "\n",
            "   Predicting on test set...\n",
            "\n",
            "   ✅ Test MSE (Daily): 385692.8438\n",
            "   Saved predictions to: results/pred_gru_C_bodyText_orig.json\n",
            "\n",
            "============================================================\n",
            "Processing: C_paragraphs_pca\n",
            "============================================================\n",
            "Loading dataset_C_paragraphs_pca.parquet...\n",
            "   Using 'pub_date' for Date column\n",
            "   Loaded 460722 rows, 321 columns\n",
            "   ✅ Date range: 2017-01-01 to 2019-12-31\n",
            "   After target creation: 460368 rows\n",
            "   Sample weights: min=0.0006, max=0.0040, mean=0.0016\n",
            "   Dropping columns: ['person', 'person_id', 'article_id', 'pub_date', 'article_date', 'value', 'target', 'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'sample_weight']\n",
            "   ✅ Feature columns (309): ['date_index', 'pca_0', 'pca_1', 'pca_2', 'pca_3', 'pca_4', 'pca_5', 'pca_6', 'pca_7', 'pca_8']...\n",
            "   ✅ All features are numeric: True\n",
            "   Train: 312185 rows\n",
            "   Valid: 76136 rows\n",
            "   Test:  72047 rows\n",
            "\n",
            "   Normalizing features...\n",
            "   ✅ Features normalized to [0, 1]\n",
            "\n",
            "   Dataset sizes:\n",
            "   - Train: 312181 samples\n",
            "   - Valid: 76132 samples\n",
            "   - Test: 72043 samples\n",
            "\n",
            "   Model initialized:\n",
            "   - Input dim: 309\n",
            "   - Hidden dim: 64\n",
            "   - Num layers: 2\n",
            "   - Dropout: 0.3\n",
            "   - Parameters: 97025\n",
            "\n",
            "   Training started...\n",
            "   Epoch [  1/100] Train Loss: 0.000022, Val Loss: 0.000549\n",
            "   Epoch [ 10/100] Train Loss: 0.000005, Val Loss: 0.000537\n",
            "\n",
            "   ⚠️ Early stopping triggered at epoch 17\n",
            "\n",
            "   ✅ Training completed\n",
            "   ✅ Best validation loss: 0.000512\n",
            "\n",
            "   Predicting on test set...\n",
            "\n",
            "   ✅ Test MSE (Daily): 416388.4688\n",
            "   Saved predictions to: results/pred_gru_C_paragraphs_pca.json\n",
            "\n",
            "============================================================\n",
            "Processing: C_paragraphs_orig\n",
            "============================================================\n",
            "Loading dataset_C_paragraphs_orig.parquet...\n",
            "   Using 'pub_date' for Date column\n",
            "   Loaded 460722 rows, 1137 columns\n",
            "   ✅ Date range: 2017-01-01 to 2019-12-31\n",
            "   After target creation: 460368 rows\n",
            "   Sample weights: min=0.0006, max=0.0040, mean=0.0016\n",
            "   Dropping columns: ['person', 'person_id', 'article_id', 'pub_date', 'article_date', 'value', 'target', 'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'sample_weight']\n",
            "   ✅ Feature columns (1125): ['date_index', 'emb_0', 'emb_1', 'emb_2', 'emb_3', 'emb_4', 'emb_5', 'emb_6', 'emb_7', 'emb_8']...\n",
            "   ✅ All features are numeric: True\n",
            "   Train: 312185 rows\n",
            "   Valid: 76136 rows\n",
            "   Test:  72047 rows\n",
            "\n",
            "   Normalizing features...\n",
            "   ✅ Features normalized to [0, 1]\n",
            "\n",
            "   Dataset sizes:\n",
            "   - Train: 312181 samples\n",
            "   - Valid: 76132 samples\n",
            "   - Test: 72043 samples\n",
            "\n",
            "   Model initialized:\n",
            "   - Input dim: 1125\n",
            "   - Hidden dim: 64\n",
            "   - Num layers: 2\n",
            "   - Dropout: 0.3\n",
            "   - Parameters: 253697\n",
            "\n",
            "   Training started...\n",
            "   Epoch [  1/100] Train Loss: 0.000038, Val Loss: 0.000246\n",
            "   Epoch [ 10/100] Train Loss: 0.000006, Val Loss: 0.000477\n",
            "\n",
            "   ⚠️ Early stopping triggered at epoch 11\n",
            "\n",
            "   ✅ Training completed\n",
            "   ✅ Best validation loss: 0.000246\n",
            "\n",
            "   Predicting on test set...\n",
            "\n",
            "   ✅ Test MSE (Daily): 348838.7500\n",
            "   Saved predictions to: results/pred_gru_C_paragraphs_orig.json\n",
            "\n",
            "============================================================\n",
            "Processing: D_headlines_pca\n",
            "============================================================\n",
            "Loading dataset_D_headlines_pca.parquet...\n",
            "   Using 'pub_date' for Date column\n",
            "   Loaded 461270 rows, 423 columns\n",
            "   ✅ Date range: 2017-01-01 to 2019-12-31\n",
            "   After target creation: 460915 rows\n",
            "   Sample weights: min=0.0006, max=0.0040, mean=0.0016\n",
            "   Dropping columns: ['person', 'person_id', 'article_id', 'pub_date', 'article_date', 'value', 'target', 'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'fg_value', 'sample_weight']\n",
            "   ✅ Feature columns (410): ['date_index', 'pca_0', 'pca_1', 'pca_2', 'pca_3', 'pca_4', 'pca_5', 'pca_6', 'pca_7', 'pca_8']...\n",
            "   ✅ All features are numeric: True\n",
            "   Train: 312535 rows\n",
            "   Valid: 76232 rows\n",
            "   Test:  72148 rows\n",
            "\n",
            "   Normalizing features...\n",
            "   ✅ Features normalized to [0, 1]\n",
            "\n",
            "   Dataset sizes:\n",
            "   - Train: 312531 samples\n",
            "   - Valid: 76228 samples\n",
            "   - Test: 72144 samples\n",
            "\n",
            "   Model initialized:\n",
            "   - Input dim: 410\n",
            "   - Hidden dim: 64\n",
            "   - Num layers: 2\n",
            "   - Dropout: 0.3\n",
            "   - Parameters: 116417\n",
            "\n",
            "   Training started...\n",
            "   Epoch [  1/100] Train Loss: 0.000016, Val Loss: 0.000053\n",
            "   Epoch [ 10/100] Train Loss: 0.000002, Val Loss: 0.000042\n",
            "   Epoch [ 20/100] Train Loss: 0.000001, Val Loss: 0.000063\n",
            "\n",
            "   ⚠️ Early stopping triggered at epoch 20\n",
            "\n",
            "   ✅ Training completed\n",
            "   ✅ Best validation loss: 0.000042\n",
            "\n",
            "   Predicting on test set...\n",
            "\n",
            "   ✅ Test MSE (Daily): 21032.6406\n",
            "   Saved predictions to: results/pred_gru_D_headlines_pca.json\n",
            "\n",
            "============================================================\n",
            "Processing: D_headlines_orig\n",
            "============================================================\n",
            "Loading dataset_D_headlines_orig.parquet...\n",
            "   Using 'pub_date' for Date column\n",
            "   Loaded 461270 rows, 1143 columns\n",
            "   ✅ Date range: 2017-01-01 to 2019-12-31\n",
            "   After target creation: 460915 rows\n",
            "   Sample weights: min=0.0006, max=0.0040, mean=0.0016\n",
            "   Dropping columns: ['person', 'person_id', 'article_id', 'pub_date', 'article_date', 'value', 'target', 'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'fg_value', 'sample_weight']\n",
            "   ✅ Feature columns (1130): ['date_index', 'emb_0', 'emb_1', 'emb_2', 'emb_3', 'emb_4', 'emb_5', 'emb_6', 'emb_7', 'emb_8']...\n",
            "   ✅ All features are numeric: True\n",
            "   Train: 312535 rows\n",
            "   Valid: 76232 rows\n",
            "   Test:  72148 rows\n",
            "\n",
            "   Normalizing features...\n",
            "   ✅ Features normalized to [0, 1]\n",
            "\n",
            "   Dataset sizes:\n",
            "   - Train: 312531 samples\n",
            "   - Valid: 76228 samples\n",
            "   - Test: 72144 samples\n",
            "\n",
            "   Model initialized:\n",
            "   - Input dim: 1130\n",
            "   - Hidden dim: 64\n",
            "   - Num layers: 2\n",
            "   - Dropout: 0.3\n",
            "   - Parameters: 254657\n",
            "\n",
            "   Training started...\n",
            "   Epoch [  1/100] Train Loss: 0.000027, Val Loss: 0.000042\n",
            "   Epoch [ 10/100] Train Loss: 0.000003, Val Loss: 0.000062\n",
            "\n",
            "   ⚠️ Early stopping triggered at epoch 11\n",
            "\n",
            "   ✅ Training completed\n",
            "   ✅ Best validation loss: 0.000042\n",
            "\n",
            "   Predicting on test set...\n",
            "\n",
            "   ✅ Test MSE (Daily): 20551.6914\n",
            "   Saved predictions to: results/pred_gru_D_headlines_orig.json\n",
            "\n",
            "============================================================\n",
            "Processing: D_chunking_pca\n",
            "============================================================\n",
            "Loading dataset_D_chunking_pca.parquet...\n",
            "   Using 'pub_date' for Date column\n",
            "   Loaded 460722 rows, 363 columns\n",
            "   ✅ Date range: 2017-01-01 to 2019-12-31\n",
            "   After target creation: 460368 rows\n",
            "   Sample weights: min=0.0006, max=0.0040, mean=0.0016\n",
            "   Dropping columns: ['person', 'person_id', 'article_id', 'pub_date', 'article_date', 'value', 'target', 'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'fg_value', 'sample_weight']\n",
            "   ✅ Feature columns (350): ['date_index', 'pca_0', 'pca_1', 'pca_2', 'pca_3', 'pca_4', 'pca_5', 'pca_6', 'pca_7', 'pca_8']...\n",
            "   ✅ All features are numeric: True\n",
            "   Train: 312185 rows\n",
            "   Valid: 76136 rows\n",
            "   Test:  72047 rows\n",
            "\n",
            "   Normalizing features...\n",
            "   ✅ Features normalized to [0, 1]\n",
            "\n",
            "   Dataset sizes:\n",
            "   - Train: 312181 samples\n",
            "   - Valid: 76132 samples\n",
            "   - Test: 72043 samples\n",
            "\n",
            "   Model initialized:\n",
            "   - Input dim: 350\n",
            "   - Hidden dim: 64\n",
            "   - Num layers: 2\n",
            "   - Dropout: 0.3\n",
            "   - Parameters: 104897\n",
            "\n",
            "   Training started...\n",
            "   Epoch [  1/100] Train Loss: 0.000016, Val Loss: 0.000071\n",
            "   Epoch [ 10/100] Train Loss: 0.000002, Val Loss: 0.000100\n",
            "\n",
            "   ⚠️ Early stopping triggered at epoch 11\n",
            "\n",
            "   ✅ Training completed\n",
            "   ✅ Best validation loss: 0.000071\n",
            "\n",
            "   Predicting on test set...\n",
            "\n",
            "   ✅ Test MSE (Daily): 121019.7891\n",
            "   Saved predictions to: results/pred_gru_D_chunking_pca.json\n",
            "\n",
            "============================================================\n",
            "Processing: D_chunking_orig\n",
            "============================================================\n",
            "Loading dataset_D_chunking_orig.parquet...\n",
            "   Using 'pub_date' for Date column\n",
            "   Loaded 460722 rows, 1143 columns\n",
            "   ✅ Date range: 2017-01-01 to 2019-12-31\n",
            "   After target creation: 460368 rows\n",
            "   Sample weights: min=0.0006, max=0.0040, mean=0.0016\n",
            "   Dropping columns: ['person', 'person_id', 'article_id', 'pub_date', 'article_date', 'value', 'target', 'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'fg_value', 'sample_weight']\n",
            "   ✅ Feature columns (1130): ['date_index', 'emb_0', 'emb_1', 'emb_2', 'emb_3', 'emb_4', 'emb_5', 'emb_6', 'emb_7', 'emb_8']...\n",
            "   ✅ All features are numeric: True\n",
            "   Train: 312185 rows\n",
            "   Valid: 76136 rows\n",
            "   Test:  72047 rows\n",
            "\n",
            "   Normalizing features...\n",
            "   ✅ Features normalized to [0, 1]\n",
            "\n",
            "   Dataset sizes:\n",
            "   - Train: 312181 samples\n",
            "   - Valid: 76132 samples\n",
            "   - Test: 72043 samples\n",
            "\n",
            "   Model initialized:\n",
            "   - Input dim: 1130\n",
            "   - Hidden dim: 64\n",
            "   - Num layers: 2\n",
            "   - Dropout: 0.3\n",
            "   - Parameters: 254657\n",
            "\n",
            "   Training started...\n",
            "   Epoch [  1/100] Train Loss: 0.000026, Val Loss: 0.000046\n",
            "   Epoch [ 10/100] Train Loss: 0.000003, Val Loss: 0.000054\n",
            "   Epoch [ 20/100] Train Loss: 0.000003, Val Loss: 0.000052\n",
            "\n",
            "   ⚠️ Early stopping triggered at epoch 24\n",
            "\n",
            "   ✅ Training completed\n",
            "   ✅ Best validation loss: 0.000039\n",
            "\n",
            "   Predicting on test set...\n",
            "\n",
            "   ✅ Test MSE (Daily): 22366.1211\n",
            "   Saved predictions to: results/pred_gru_D_chunking_orig.json\n",
            "\n",
            "============================================================\n",
            "Processing: D_bodyText_pca\n",
            "============================================================\n",
            "Loading dataset_D_bodyText_pca.parquet...\n",
            "   Using 'pub_date' for Date column\n",
            "   Loaded 460722 rows, 310 columns\n",
            "   ✅ Date range: 2017-01-01 to 2019-12-31\n",
            "   After target creation: 460368 rows\n",
            "   Sample weights: min=0.0006, max=0.0040, mean=0.0016\n",
            "   Dropping columns: ['person', 'person_id', 'article_id', 'pub_date', 'article_date', 'value', 'target', 'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'fg_value', 'sample_weight']\n",
            "   ✅ Feature columns (297): ['date_index', 'pca_0', 'pca_1', 'pca_2', 'pca_3', 'pca_4', 'pca_5', 'pca_6', 'pca_7', 'pca_8']...\n",
            "   ✅ All features are numeric: True\n",
            "   Train: 312185 rows\n",
            "   Valid: 76136 rows\n",
            "   Test:  72047 rows\n",
            "\n",
            "   Normalizing features...\n",
            "   ✅ Features normalized to [0, 1]\n",
            "\n",
            "   Dataset sizes:\n",
            "   - Train: 312181 samples\n",
            "   - Valid: 76132 samples\n",
            "   - Test: 72043 samples\n",
            "\n",
            "   Model initialized:\n",
            "   - Input dim: 297\n",
            "   - Hidden dim: 64\n",
            "   - Num layers: 2\n",
            "   - Dropout: 0.3\n",
            "   - Parameters: 94721\n",
            "\n",
            "   Training started...\n",
            "   Epoch [  1/100] Train Loss: 0.000016, Val Loss: 0.000066\n",
            "   Epoch [ 10/100] Train Loss: 0.000002, Val Loss: 0.000071\n",
            "\n",
            "   ⚠️ Early stopping triggered at epoch 16\n",
            "\n",
            "   ✅ Training completed\n",
            "   ✅ Best validation loss: 0.000054\n",
            "\n",
            "   Predicting on test set...\n",
            "\n",
            "   ✅ Test MSE (Daily): 37474.5430\n",
            "   Saved predictions to: results/pred_gru_D_bodyText_pca.json\n",
            "\n",
            "============================================================\n",
            "Processing: D_bodyText_orig\n",
            "============================================================\n",
            "Loading dataset_D_bodyText_orig.parquet...\n",
            "   Using 'pub_date' for Date column\n",
            "   Loaded 460722 rows, 1143 columns\n",
            "   ✅ Date range: 2017-01-01 to 2019-12-31\n",
            "   After target creation: 460368 rows\n",
            "   Sample weights: min=0.0006, max=0.0040, mean=0.0016\n",
            "   Dropping columns: ['person', 'person_id', 'article_id', 'pub_date', 'article_date', 'value', 'target', 'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'fg_value', 'sample_weight']\n",
            "   ✅ Feature columns (1130): ['date_index', 'emb_0', 'emb_1', 'emb_2', 'emb_3', 'emb_4', 'emb_5', 'emb_6', 'emb_7', 'emb_8']...\n",
            "   ✅ All features are numeric: True\n",
            "   Train: 312185 rows\n",
            "   Valid: 76136 rows\n",
            "   Test:  72047 rows\n",
            "\n",
            "   Normalizing features...\n",
            "   ✅ Features normalized to [0, 1]\n",
            "\n",
            "   Dataset sizes:\n",
            "   - Train: 312181 samples\n",
            "   - Valid: 76132 samples\n",
            "   - Test: 72043 samples\n",
            "\n",
            "   Model initialized:\n",
            "   - Input dim: 1130\n",
            "   - Hidden dim: 64\n",
            "   - Num layers: 2\n",
            "   - Dropout: 0.3\n",
            "   - Parameters: 254657\n",
            "\n",
            "   Training started...\n",
            "   Epoch [  1/100] Train Loss: 0.000027, Val Loss: 0.000046\n",
            "   Epoch [ 10/100] Train Loss: 0.000003, Val Loss: 0.000055\n",
            "\n",
            "   ⚠️ Early stopping triggered at epoch 11\n",
            "\n",
            "   ✅ Training completed\n",
            "   ✅ Best validation loss: 0.000046\n",
            "\n",
            "   Predicting on test set...\n",
            "\n",
            "   ✅ Test MSE (Daily): 20595.7344\n",
            "   Saved predictions to: results/pred_gru_D_bodyText_orig.json\n",
            "\n",
            "============================================================\n",
            "Processing: D_paragraphs_pca\n",
            "============================================================\n",
            "Loading dataset_D_paragraphs_pca.parquet...\n",
            "   Using 'pub_date' for Date column\n",
            "   Loaded 460722 rows, 327 columns\n",
            "   ✅ Date range: 2017-01-01 to 2019-12-31\n",
            "   After target creation: 460368 rows\n",
            "   Sample weights: min=0.0006, max=0.0040, mean=0.0016\n",
            "   Dropping columns: ['person', 'person_id', 'article_id', 'pub_date', 'article_date', 'value', 'target', 'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'fg_value', 'sample_weight']\n",
            "   ✅ Feature columns (314): ['date_index', 'pca_0', 'pca_1', 'pca_2', 'pca_3', 'pca_4', 'pca_5', 'pca_6', 'pca_7', 'pca_8']...\n",
            "   ✅ All features are numeric: True\n",
            "   Train: 312185 rows\n",
            "   Valid: 76136 rows\n",
            "   Test:  72047 rows\n",
            "\n",
            "   Normalizing features...\n",
            "   ✅ Features normalized to [0, 1]\n",
            "\n",
            "   Dataset sizes:\n",
            "   - Train: 312181 samples\n",
            "   - Valid: 76132 samples\n",
            "   - Test: 72043 samples\n",
            "\n",
            "   Model initialized:\n",
            "   - Input dim: 314\n",
            "   - Hidden dim: 64\n",
            "   - Num layers: 2\n",
            "   - Dropout: 0.3\n",
            "   - Parameters: 97985\n",
            "\n",
            "   Training started...\n",
            "   Epoch [  1/100] Train Loss: 0.000015, Val Loss: 0.000091\n",
            "   Epoch [ 10/100] Train Loss: 0.000002, Val Loss: 0.000093\n",
            "\n",
            "   ⚠️ Early stopping triggered at epoch 11\n",
            "\n",
            "   ✅ Training completed\n",
            "   ✅ Best validation loss: 0.000091\n",
            "\n",
            "   Predicting on test set...\n",
            "\n",
            "   ✅ Test MSE (Daily): 118705.8203\n",
            "   Saved predictions to: results/pred_gru_D_paragraphs_pca.json\n",
            "\n",
            "============================================================\n",
            "Processing: D_paragraphs_orig\n",
            "============================================================\n",
            "Loading dataset_D_paragraphs_orig.parquet...\n",
            "   Using 'pub_date' for Date column\n",
            "   Loaded 460722 rows, 1143 columns\n",
            "   ✅ Date range: 2017-01-01 to 2019-12-31\n",
            "   After target creation: 460368 rows\n",
            "   Sample weights: min=0.0006, max=0.0040, mean=0.0016\n",
            "   Dropping columns: ['person', 'person_id', 'article_id', 'pub_date', 'article_date', 'value', 'target', 'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'fg_value', 'sample_weight']\n",
            "   ✅ Feature columns (1130): ['date_index', 'emb_0', 'emb_1', 'emb_2', 'emb_3', 'emb_4', 'emb_5', 'emb_6', 'emb_7', 'emb_8']...\n",
            "   ✅ All features are numeric: True\n",
            "   Train: 312185 rows\n",
            "   Valid: 76136 rows\n",
            "   Test:  72047 rows\n",
            "\n",
            "   Normalizing features...\n",
            "   ✅ Features normalized to [0, 1]\n",
            "\n",
            "   Dataset sizes:\n",
            "   - Train: 312181 samples\n",
            "   - Valid: 76132 samples\n",
            "   - Test: 72043 samples\n",
            "\n",
            "   Model initialized:\n",
            "   - Input dim: 1130\n",
            "   - Hidden dim: 64\n",
            "   - Num layers: 2\n",
            "   - Dropout: 0.3\n",
            "   - Parameters: 254657\n",
            "\n",
            "   Training started...\n",
            "   Epoch [  1/100] Train Loss: 0.000030, Val Loss: 0.000037\n",
            "   Epoch [ 10/100] Train Loss: 0.000003, Val Loss: 0.000058\n",
            "\n",
            "   ⚠️ Early stopping triggered at epoch 11\n",
            "\n",
            "   ✅ Training completed\n",
            "   ✅ Best validation loss: 0.000037\n",
            "\n",
            "   Predicting on test set...\n",
            "\n",
            "   ✅ Test MSE (Daily): 23974.7832\n",
            "   Saved predictions to: results/pred_gru_D_paragraphs_orig.json\n",
            "\n",
            "============================================================\n",
            "ALL TASKS COMPLETED\n",
            "============================================================\n",
            "\n",
            "Results saved to: results_gru/gru_evaluation_metrics.csv\n",
            "\n",
            "Top 10 Models by Test MSE:\n",
            "   Dataset      Method  Type Best_Model       Test_MSE\n",
            "0        A           -     -        GRU    6204.133435\n",
            "18       D   headlines  orig        GRU   20551.691406\n",
            "22       D    bodyText  orig        GRU   20595.734375\n",
            "17       D   headlines   pca        GRU   21032.640625\n",
            "20       D    chunking  orig        GRU   22366.121094\n",
            "24       D  paragraphs  orig        GRU   23974.783203\n",
            "21       D    bodyText   pca        GRU   37474.542969\n",
            "23       D  paragraphs   pca        GRU  118705.820312\n",
            "19       D    chunking   pca        GRU  121019.789062\n",
            "8        B  paragraphs  orig        GRU  334572.059224\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# Main Execution\n",
        "# ========================================\n",
        "\n",
        "levels = ['B', 'C', 'D']\n",
        "methods = ['headlines', 'chunking', 'bodyText', 'paragraphs']\n",
        "types = ['pca', 'orig']\n",
        "\n",
        "metrics_list = []\n",
        "\n",
        "# ========================================\n",
        "# Dataset A (Baseline)\n",
        "# ========================================\n",
        "path_A = DATA_DIR / \"dataset_A.parquet\"\n",
        "if path_A.exists():\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"BASELINE: Dataset A\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    try:\n",
        "        mse = train_gru_model(\"A\", path_A)\n",
        "        metrics_list.append({\n",
        "            \"Dataset\": \"A\",\n",
        "            \"Method\": \"-\",\n",
        "            \"Type\": \"-\",\n",
        "            \"Best_Model\": \"GRU\",\n",
        "            \"Test_MSE\": mse\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error on Dataset A: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "    finally:\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "else:\n",
        "    print(f\"Warning: {path_A} not found. Skipping Dataset A.\")\n",
        "\n",
        "# ========================================\n",
        "# Datasets B, C, D\n",
        "# ========================================\n",
        "total_datasets = len(levels) * len(methods) * len(types)\n",
        "current = 0\n",
        "\n",
        "for level in levels:\n",
        "    for method in methods:\n",
        "        for dtype in types:\n",
        "            current += 1\n",
        "            fname = f\"dataset_{level}_{method}_{dtype}.parquet\"\n",
        "            fpath = DATA_DIR / fname\n",
        "            \n",
        "            if not fpath.exists():\n",
        "                print(f\"\\n[{current}/{total_datasets}] Skipping {fname}: File not found.\")\n",
        "                continue\n",
        "            \n",
        "            dname = f\"{level}_{method}_{dtype}\"\n",
        "            \n",
        "            try:\n",
        "                mse = train_gru_model(dname, fpath)\n",
        "                metrics_list.append({\n",
        "                    \"Dataset\": level,\n",
        "                    \"Method\": method,\n",
        "                    \"Type\": dtype,\n",
        "                    \"Best_Model\": \"GRU\",\n",
        "                    \"Test_MSE\": mse\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"\\n❌ Error on {dname}: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "            finally:\n",
        "                gc.collect()\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "# ========================================\n",
        "# Save final results\n",
        "# ========================================\n",
        "final_df = pd.DataFrame(metrics_list).sort_values(\"Test_MSE\")\n",
        "csv_path = OUTPUT_DIR / \"gru_evaluation_metrics.csv\"\n",
        "final_df.to_csv(csv_path, index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ALL TASKS COMPLETED\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nResults saved to: {csv_path}\")\n",
        "print(f\"\\nTop 10 Models by Test MSE:\")\n",
        "print(final_df.head(10))\n",
        "\n",
        "# 총 51분 정도"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "analysis"
      },
      "source": [
        "## 11. Results Analysis (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "analysis_code"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== GRU Results Summary ===\n",
            "\n",
            "Total experiments: 25\n",
            "\n",
            "Best Model:\n",
            "Dataset                 A\n",
            "Method                  -\n",
            "Type                    -\n",
            "Best_Model            GRU\n",
            "Test_MSE      6204.133435\n",
            "Name: 0, dtype: object\n",
            "\n",
            "MSE Statistics:\n",
            "count        25.000000\n",
            "mean     267811.688758\n",
            "std      176878.872981\n",
            "min        6204.133435\n",
            "25%       37474.542969\n",
            "50%      352727.529401\n",
            "75%      414691.468750\n",
            "max      470726.320014\n",
            "Name: Test_MSE, dtype: float64\n",
            "\n",
            "=== PCA vs Original ===\n",
            "PCA average MSE: 310983.0935\n",
            "Original average MSE: 246440.9137\n",
            "Difference: 64542.1798\n",
            "\n",
            "=== Embedding Method Comparison ===\n",
            "headlines: 272652.3649\n",
            "bodyText: 281500.8708\n",
            "chunking: 281410.1401\n",
            "paragraphs: 279284.6384\n"
          ]
        }
      ],
      "source": [
        "# Load results\n",
        "results_df = pd.read_csv(OUTPUT_DIR / \"gru_evaluation_metrics.csv\")\n",
        "\n",
        "print(\"\\n=== GRU Results Summary ===\")\n",
        "print(f\"\\nTotal experiments: {len(results_df)}\")\n",
        "print(f\"\\nBest Model:\")\n",
        "print(results_df.iloc[0])\n",
        "print(f\"\\nMSE Statistics:\")\n",
        "print(results_df['Test_MSE'].describe())\n",
        "\n",
        "# Compare by Type\n",
        "print(f\"\\n=== PCA vs Original ===\")\n",
        "pca_mse = results_df[results_df['Type'] == 'pca']['Test_MSE'].mean()\n",
        "orig_mse = results_df[results_df['Type'] == 'orig']['Test_MSE'].mean()\n",
        "print(f\"PCA average MSE: {pca_mse:.4f}\")\n",
        "print(f\"Original average MSE: {orig_mse:.4f}\")\n",
        "print(f\"Difference: {abs(pca_mse - orig_mse):.4f}\")\n",
        "\n",
        "# Compare by Method\n",
        "print(f\"\\n=== Embedding Method Comparison ===\")\n",
        "for method in results_df['Method'].unique():\n",
        "    if method != '-':\n",
        "        method_mse = results_df[results_df['Method'] == method]['Test_MSE'].mean()\n",
        "        print(f\"{method}: {method_mse:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mlproject",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
