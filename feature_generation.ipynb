{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "실행 전 Requirements!!\n",
        "\n",
        "이 노트북을 로컬에서 실행하려면 아래 라이브러리들이 필요합니다!!(특히 `pyarrow`나 `fastparquet`가 없으면 파일 저장이 안 됩니다!)\n",
        "\n",
        "터미널(cmd)에서 다음 명령어를 실행해 주세요!\n",
        "\n",
        "```bash\n",
        "pip install pandas numpy scikit-learn pyarrow fastparquet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxUpYouPXYL5"
      },
      "source": [
        "0. 세팅: Drive 마운트 & 경로 설정 (Local 버전)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bT-lcfg9f4o",
        "outputId": "2db23b64-af47-445c-b3f5-3d6b9902159f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment: Local Machine\n",
            "PROJECT ROOT: /Users/migreeni/Library/CloudStorage/OneDrive-개인/25-2/Machine learning/20252R0136COSE36203\n",
            "Embedding data ROOT: /Users/migreeni/Library/CloudStorage/OneDrive-개인/25-2/Machine learning/20252R0136COSE36203/embedding\n",
            "Economic index data ROOT: /Users/migreeni/Library/CloudStorage/OneDrive-개인/25-2/Machine learning/20252R0136COSE36203/Economic_index\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# 1. Colab vs Local\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    is_colab = True\n",
        "except ImportError:\n",
        "    is_colab = False\n",
        "\n",
        "# 2. 경로 설정\n",
        "if is_colab:\n",
        "    print(\"Environment: Google Colab\")\n",
        "    drive.mount('/content/drive')\n",
        "    \n",
        "    # 본인의 드라이브 경로에 맞게 수정.\n",
        "    ROOT = Path(\"/content/drive/MyDrive/ML_team_project_final\") \n",
        "else:\n",
        "    print(\"Environment: Local Machine\")\n",
        "    # 로컬에서는 현재 노트북 파일이 있는 위치를 기준.\n",
        "    ROOT = Path.cwd()\n",
        "    EMB_DIR = ROOT / \"embedding\"\n",
        "    EIDX_DIR = ROOT / \"Economic_index\"\n",
        "\n",
        "print(\"PROJECT ROOT:\", ROOT)\n",
        "print(\"Embedding data ROOT:\", EMB_DIR)\n",
        "print(\"Economic index data ROOT:\", EIDX_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9-3uMifXlH_"
      },
      "source": [
        "1. 입력 파일 경로 정의 (Local 버전)\n",
        "- sp500.csv\n",
        "- fear_greed.csv\n",
        "- 4가지 임베딩 + 메타데이터 파일"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwjYFX2E9k8m",
        "outputId": "03c763a8-8ddb-454d-9627-6f1ad41a6f8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== INPUT FILE CHECK ===\n",
            "sp500.csv exists?       True\n",
            "fear_greed.csv exists?  True\n",
            "headlines emb exists?   True\n",
            "headlines meta exists?  True\n",
            "chunking emb exists?   True\n",
            "chunking meta exists?  True\n",
            "bodyText emb exists?   True\n",
            "bodyText meta exists?  True\n",
            "paragraphs emb exists?   True\n",
            "paragraphs meta exists?  True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "SP500_CSV_PATH      = f\"{EIDX_DIR}/sp500.csv\"\n",
        "FEARGREED_CSV_PATH  = f\"{EIDX_DIR}/fear_greed.csv\"\n",
        "\n",
        "EMB_CONFIGS = {\n",
        "    \"headlines\": {\n",
        "        \"emb_path\": EMB_DIR / \"vector_headlines\" / \"embeddings.npy\",\n",
        "        \"meta_path\": EMB_DIR / \"vector_headlines\" / \"metadata.jsonl\",\n",
        "    },\n",
        "    \"chunking\": {\n",
        "        \"emb_path\": EMB_DIR / \"vector_chunking\" / \"embeddings.npy\",\n",
        "        \"meta_path\": EMB_DIR / \"vector_chunking\" / \"metadata.jsonl\",\n",
        "    },\n",
        "    \"bodyText\": {\n",
        "        \"emb_path\": EMB_DIR / \"vector_bodyText\" / \"embeddings.npy\",\n",
        "        \"meta_path\": EMB_DIR / \"vector_bodyText\" / \"metadata.jsonl\",\n",
        "    },\n",
        "    \"paragraphs\": {\n",
        "        \"emb_path\": EMB_DIR / \"vector_paragraphs\" / \"embeddings.npy\",\n",
        "        \"meta_path\": EMB_DIR / \"vector_paragraphs\" / \"metadata.jsonl\",\n",
        "    },\n",
        "}\n",
        "\n",
        "# lag 길이 L\n",
        "LAG_L = 5\n",
        "NUM_PERSONS = 100\n",
        "\n",
        "print(\"=== INPUT FILE CHECK ===\")\n",
        "print(\"sp500.csv exists?      \", os.path.exists(SP500_CSV_PATH))\n",
        "print(\"fear_greed.csv exists? \", os.path.exists(FEARGREED_CSV_PATH))\n",
        "for name, cfg in EMB_CONFIGS.items():\n",
        "    print(f\"{name} emb exists?  \", os.path.exists(cfg['emb_path']))\n",
        "    print(f\"{name} meta exists? \", os.path.exists(cfg['meta_path']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc9q7AtMX4pp"
      },
      "source": [
        "2. 기본 import & 날짜 유틸 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "v8eBmraabSrX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "def to_datetime_any(dt_str):\n",
        "    \"\"\"문자열을 pandas.Timestamp로 변환.\"\"\"\n",
        "    return pd.to_datetime(dt_str).normalize()\n",
        "\n",
        "def pubdate_to_timestamp(pub_date_str: str):\n",
        "    \"\"\"\n",
        "    metadata의 pub_date: 'YYYY_MM_DD' -> Timestamp로 변환.\n",
        "    예: '2017_12_31' -> 2017-12-31\n",
        "    \"\"\"\n",
        "    return pd.to_datetime(pub_date_str, format=\"%Y_%m_%d\").normalize()\n",
        "\n",
        "def format_date_underscore(ts: pd.Timestamp):\n",
        "    \"\"\"Timestamp -> 'YYYY_MM_DD' 문자열.\"\"\"\n",
        "    return ts.strftime(\"%Y_%m_%d\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eT4_QFTEbgMP"
      },
      "source": [
        "3. S&P 500 로드 + date_index 생성\n",
        "- Date, Open 사용\n",
        "- value = Open\n",
        "- 날짜 오름차순 정렬 후 date_index = 0,1,2,... (거래일 순서)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHNTauu8bf_I",
        "outputId": "6c68bb1c-71ef-4140-90d4-2407a5008d79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== STEP 3: Load S&P 500 ===\n",
            "sp500_df.shape = (754, 4)\n",
            "        Date    value  date_index    date_str\n",
            "0 2017-01-03  2251.57           0  2017_01_03\n",
            "1 2017-01-04  2261.60           1  2017_01_04\n",
            "2 2017-01-05  2268.18           2  2017_01_05\n",
            "3 2017-01-06  2271.14           3  2017_01_06\n"
          ]
        }
      ],
      "source": [
        "def load_sp500(path: str) -> pd.DataFrame:\n",
        "    df = pd.read_csv(path)\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "\n",
        "    # 필요한 컬럼만 사용\n",
        "    if not {\"Date\", \"Open\"}.issubset(df.columns):\n",
        "        raise ValueError(\"sp500.csv에는 'Date', 'Open' 컬럼이 있어야 합니다.\")\n",
        "\n",
        "    df = df[[\"Date\", \"Open\"]].copy()\n",
        "    df[\"Date\"] = df[\"Date\"].apply(to_datetime_any)\n",
        "    df = df.sort_values(\"Date\").reset_index(drop=True)\n",
        "\n",
        "    df = df.rename(columns={\"Open\": \"value\"})\n",
        "    df[\"date_index\"] = np.arange(len(df))  # 0부터 시작\n",
        "    df[\"date_str\"] = df[\"Date\"].apply(format_date_underscore)\n",
        "    return df\n",
        "\n",
        "print(\"=== STEP 3: Load S&P 500 ===\")\n",
        "sp500_df = load_sp500(SP500_CSV_PATH)\n",
        "print(\"sp500_df.shape =\", sp500_df.shape)\n",
        "print(sp500_df.head(4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFf8i6mwbsyq"
      },
      "source": [
        "4. Fear-Greed Index 로드\n",
        "- Date, fear_greed_index 사용\n",
        "- value = fear_greed_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_LwXzrEbf8R",
        "outputId": "b3b74932-4725-4e64-94a4-afb22a350a03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== STEP 4: Load Fear-Greed ===\n",
            "fear_greed_raw.shape = (754, 2)\n",
            "   value       Date\n",
            "0     70 2017-01-03\n",
            "1     70 2017-01-04\n",
            "2     70 2017-01-05\n",
            "3     68 2017-01-06\n"
          ]
        }
      ],
      "source": [
        "def load_fear_greed(path: str) -> pd.DataFrame:\n",
        "    df = pd.read_csv(path)\n",
        "    df.columns = [c.strip().lower() for c in df.columns]\n",
        "\n",
        "    if not {\"date\", \"fear_greed_index\"}.issubset(df.columns):\n",
        "        raise ValueError(\"fear_greed.csv에는 'date', 'fear_greed_index' 컬럼이 있어야 합니다.\")\n",
        "\n",
        "    df = df[[\"date\", \"fear_greed_index\"]].copy()\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"date\"]).dt.normalize()\n",
        "    df = df.rename(columns={\"fear_greed_index\": \"value\"})\n",
        "    df = df.drop(columns=[\"date\"])\n",
        "    df = df.sort_values(\"Date\").reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "print(\"=== STEP 4: Load Fear-Greed ===\")\n",
        "fear_greed_raw = load_fear_greed(FEARGREED_CSV_PATH)\n",
        "print(\"fear_greed_raw.shape =\", fear_greed_raw.shape)\n",
        "print(fear_greed_raw.head(4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV8-0sCjb2bC"
      },
      "source": [
        "5. 모든 metadata 로드 + person/date 범위 확인\n",
        "\n",
        "metadata 형식 (예시): {\"person\": \"alex_morgan\", \"article_id\": \"...\", \"pub_date\": \"2017_12_31\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__82A7w2bz2_",
        "outputId": "8ac7c47e-cb4e-44c7-c3f8-3f94e4c16573"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== STEP 5: Load metadata for all methods ===\n",
            "\n",
            "[metadata] method = headlines\n",
            "  meta_df.shape = (461270, 4)\n",
            "        person                                         article_id    pub_date  \\\n",
            "0  alex_morgan  uk-news/2017/dec/31/new-years-eve-celebrations...  2017_12_31   \n",
            "1  alex_morgan  sport/2017/dec/31/alastair-cook-david-warner-t...  2017_12_31   \n",
            "2  alex_morgan  world/2017/dec/31/eight-big-ideas-for-2018-pol...  2017_12_31   \n",
            "3  alex_morgan  world/2017/dec/31/look-to-the-future-what-does...  2017_12_31   \n",
            "\n",
            "  article_date  \n",
            "0   2017-12-31  \n",
            "1   2017-12-31  \n",
            "2   2017-12-31  \n",
            "3   2017-12-31  \n",
            "\n",
            "[metadata] method = chunking\n",
            "  meta_df.shape = (460722, 4)\n",
            "        person                                         article_id    pub_date  \\\n",
            "0  alex_morgan  uk-news/2017/dec/31/new-years-eve-celebrations...  2017_12_31   \n",
            "1  alex_morgan  sport/2017/dec/31/alastair-cook-david-warner-t...  2017_12_31   \n",
            "2  alex_morgan  world/2017/dec/31/eight-big-ideas-for-2018-pol...  2017_12_31   \n",
            "3  alex_morgan  world/2017/dec/31/look-to-the-future-what-does...  2017_12_31   \n",
            "\n",
            "  article_date  \n",
            "0   2017-12-31  \n",
            "1   2017-12-31  \n",
            "2   2017-12-31  \n",
            "3   2017-12-31  \n",
            "\n",
            "[metadata] method = bodyText\n",
            "  meta_df.shape = (460722, 4)\n",
            "        person                                         article_id    pub_date  \\\n",
            "0  alex_morgan  uk-news/2017/dec/31/new-years-eve-celebrations...  2017_12_31   \n",
            "1  alex_morgan  sport/2017/dec/31/alastair-cook-david-warner-t...  2017_12_31   \n",
            "2  alex_morgan  world/2017/dec/31/eight-big-ideas-for-2018-pol...  2017_12_31   \n",
            "3  alex_morgan  world/2017/dec/31/look-to-the-future-what-does...  2017_12_31   \n",
            "\n",
            "  article_date  \n",
            "0   2017-12-31  \n",
            "1   2017-12-31  \n",
            "2   2017-12-31  \n",
            "3   2017-12-31  \n",
            "\n",
            "[metadata] method = paragraphs\n",
            "  meta_df.shape = (460722, 4)\n",
            "        person                                         article_id    pub_date  \\\n",
            "0  alex_morgan  uk-news/2017/dec/31/new-years-eve-celebrations...  2017_12_31   \n",
            "1  alex_morgan  sport/2017/dec/31/alastair-cook-david-warner-t...  2017_12_31   \n",
            "2  alex_morgan  world/2017/dec/31/eight-big-ideas-for-2018-pol...  2017_12_31   \n",
            "3  alex_morgan  world/2017/dec/31/look-to-the-future-what-does...  2017_12_31   \n",
            "\n",
            "  article_date  \n",
            "0   2017-12-31  \n",
            "1   2017-12-31  \n",
            "2   2017-12-31  \n",
            "3   2017-12-31  \n",
            "\n",
            "전체 person 수: 100\n",
            "person 예시 10개: ['alex_morgan', 'alicia_keys', 'andres_manuel_lopez', 'ann_mckee', 'ashley_graham', 'barbara_lynch', 'barbara_rae_venter', 'barry_jenkins', 'benjamin_netanyahu', 'bernard_tyson']\n"
          ]
        }
      ],
      "source": [
        "def load_metadata_jsonl(path: str) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            rows.append(json.loads(line.strip()))\n",
        "    df = pd.DataFrame(rows)\n",
        "\n",
        "    # 필수 컬럼 체크\n",
        "    for col in [\"person\", \"article_id\", \"pub_date\"]:\n",
        "        if col not in df.columns:\n",
        "            raise ValueError(f\"{path} 에 '{col}' 컬럼이 없습니다.\")\n",
        "\n",
        "    # 날짜 처리\n",
        "    df[\"article_date\"] = df[\"pub_date\"].apply(pubdate_to_timestamp)\n",
        "    df[\"pub_date\"] = df[\"pub_date\"].astype(str)\n",
        "    return df\n",
        "\n",
        "print(\"=== STEP 5: Load metadata for all methods ===\")\n",
        "meta_raw_dict = {}\n",
        "all_persons = set()\n",
        "all_meta_dates = []\n",
        "\n",
        "for method_name, cfg in EMB_CONFIGS.items():\n",
        "    print(f\"\\n[metadata] method = {method_name}\")\n",
        "    meta_df = load_metadata_jsonl(cfg[\"meta_path\"])\n",
        "\n",
        "    if method_name == \"headlines\" and \"headline\" in meta_df.columns:\n",
        "        meta_df = meta_df.drop(columns=[\"headline\"])\n",
        "\n",
        "    meta_raw_dict[method_name] = meta_df\n",
        "\n",
        "    all_persons.update(meta_df[\"person\"].unique())\n",
        "    all_meta_dates.append(meta_df[\"article_date\"].min())\n",
        "    all_meta_dates.append(meta_df[\"article_date\"].max())\n",
        "\n",
        "    print(\"  meta_df.shape =\", meta_df.shape)\n",
        "    print(meta_df.head(4))\n",
        "\n",
        "print(\"\\n전체 person 수:\", len(all_persons))\n",
        "print(\"person 예시 10개:\", sorted(all_persons)[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzI-3MQ2cExR"
      },
      "source": [
        "6. person → person_id (1..NUM_PERSONS) 매핑\n",
        "\n",
        "문자열 person을 고정된 정수 ID로 변환해서\n",
        "나중에 one-hot vector를 만들 수 있게 한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srRBJmmxbf5J",
        "outputId": "0f67e376-66f0-4cd2-c5d8-76e04c182d24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== STEP 6: Build person_to_id mapping ===\n",
            "=== person_to_id 예시 (앞 10개) ===\n",
            "  alex_morgan -> 1\n",
            "  alicia_keys -> 2\n",
            "  andres_manuel_lopez -> 3\n",
            "  ann_mckee -> 4\n",
            "  ashley_graham -> 5\n",
            "  barbara_lynch -> 6\n",
            "  barbara_rae_venter -> 7\n",
            "  barry_jenkins -> 8\n",
            "  benjamin_netanyahu -> 9\n",
            "  bernard_tyson -> 10\n",
            "총 person 수 = 100\n"
          ]
        }
      ],
      "source": [
        "def build_person_mapping(person_names, num_persons_expected=None):\n",
        "    sorted_persons = sorted(person_names)\n",
        "    person_to_id = {p: i+1 for i, p in enumerate(sorted_persons)}  # 1-based\n",
        "\n",
        "    print(\"=== person_to_id 예시 (앞 10개) ===\")\n",
        "    for p in sorted_persons[:10]:\n",
        "        print(f\"  {p} -> {person_to_id[p]}\")\n",
        "    print(f\"총 person 수 = {len(sorted_persons)}\")\n",
        "\n",
        "    if num_persons_expected is not None and len(sorted_persons) != num_persons_expected:\n",
        "        print(f\"경고: 기대 인원 수 {num_persons_expected}명과 실제 {len(sorted_persons)}명이 다릅니다.\")\n",
        "    return person_to_id, sorted_persons\n",
        "\n",
        "print(\"=== STEP 6: Build person_to_id mapping ===\")\n",
        "person_to_id, sorted_persons = build_person_mapping(all_persons, num_persons_expected=NUM_PERSONS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs03t9tgcSMF"
      },
      "source": [
        "7. Global date range & date_index_df\n",
        "- sp500, fear_greed, 모든 metadata 날짜 범위를 합쳐서 전체 캘린더 날짜(Date)에 대해 date_index를 부여한다.\n",
        "- 규칙: 가장 가까운 이전 S&P 거래일의 index 사용 (이전이 없으면 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b43UuVfbf2S",
        "outputId": "f84c7aae-66cd-48ea-d881-9a9c2049f88d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== STEP 7-1: Compute global date range ===\n",
            "global_start = 2017-01-01 00:00:00\n",
            "global_end   = 2019-12-31 00:00:00\n",
            "\n",
            "=== STEP 7-2: Build date_index_df ===\n",
            "date_index_df.shape = (1095, 3)\n",
            "date_index_df.head(8):\n",
            "        Date  date_index    date_str\n",
            "0 2017-01-01           0  2017_01_01\n",
            "1 2017-01-02           0  2017_01_02\n",
            "2 2017-01-03           0  2017_01_03\n",
            "3 2017-01-04           1  2017_01_04\n",
            "4 2017-01-05           2  2017_01_05\n",
            "5 2017-01-06           3  2017_01_06\n",
            "6 2017-01-07           3  2017_01_07\n",
            "7 2017-01-08           3  2017_01_08\n",
            "date_index_df.tail(8):\n",
            "           Date  date_index    date_str\n",
            "1087 2019-12-24         749  2019_12_24\n",
            "1088 2019-12-25         749  2019_12_25\n",
            "1089 2019-12-26         750  2019_12_26\n",
            "1090 2019-12-27         751  2019_12_27\n",
            "1091 2019-12-28         751  2019_12_28\n",
            "1092 2019-12-29         751  2019_12_29\n",
            "1093 2019-12-30         752  2019_12_30\n",
            "1094 2019-12-31         753  2019_12_31\n"
          ]
        }
      ],
      "source": [
        "def compute_global_date_range(sp500_df, fear_greed_raw, meta_raw_dict):\n",
        "    dates = [\n",
        "        sp500_df[\"Date\"].min(),\n",
        "        sp500_df[\"Date\"].max(),\n",
        "        fear_greed_raw[\"Date\"].min(),\n",
        "        fear_greed_raw[\"Date\"].max(),\n",
        "    ]\n",
        "    for meta_df in meta_raw_dict.values():\n",
        "        dates.append(meta_df[\"article_date\"].min())\n",
        "        dates.append(meta_df[\"article_date\"].max())\n",
        "    start_date = min(dates)\n",
        "    end_date = max(dates)\n",
        "    return start_date, end_date\n",
        "\n",
        "print(\"=== STEP 7-1: Compute global date range ===\")\n",
        "global_start, global_end = compute_global_date_range(sp500_df, fear_greed_raw, meta_raw_dict)\n",
        "print(\"global_start =\", global_start)\n",
        "print(\"global_end   =\", global_end)\n",
        "\n",
        "def build_date_index_df(sp500_df, start_date, end_date):\n",
        "    calendar = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n",
        "    df = pd.DataFrame({\"Date\": calendar})\n",
        "\n",
        "    # S&P 거래일의 date_index를 merge (NaN은 이후 ffill)\n",
        "    df = df.merge(sp500_df[[\"Date\", \"date_index\"]], on=\"Date\", how=\"left\")\n",
        "\n",
        "    # 가장 가까운 이전 거래일의 index로 forward-fill\n",
        "    df[\"date_index\"] = df[\"date_index\"].ffill()\n",
        "\n",
        "    # 첫 거래일 이전 날짜: NaN → 0\n",
        "    df[\"date_index\"] = df[\"date_index\"].fillna(0).astype(int)\n",
        "    df[\"date_str\"] = df[\"Date\"].apply(format_date_underscore)\n",
        "    return df\n",
        "\n",
        "print(\"\\n=== STEP 7-2: Build date_index_df ===\")\n",
        "date_index_df = build_date_index_df(sp500_df, global_start, global_end)\n",
        "print(\"date_index_df.shape =\", date_index_df.shape)\n",
        "print(\"date_index_df.head(8):\")\n",
        "print(date_index_df.head(8))\n",
        "print(\"date_index_df.tail(8):\")\n",
        "print(date_index_df.tail(8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0VpIQgRcoyo"
      },
      "source": [
        "8. Feature A: S&P 500 lag features (dataset_A)\n",
        "- 입력: sp500_df (거래일 기준)\n",
        "- 출력: dataset_A (date_index, date_str, value, lag_1..lag_L)\n",
        "- 없는 과거 값: 첫 value 반복"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hZPcr7Tbfys",
        "outputId": "511b2325-fd43-498d-ccb6-af44f757ab6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== STEP 8: Build dataset_A ===\n",
            "dataset_A.shape = (754, 8)\n",
            "   date_index    pub_date    value    lag_1    lag_2    lag_3    lag_4  \\\n",
            "0           0  2017_01_03  2251.57  2251.57  2251.57  2251.57  2251.57   \n",
            "1           1  2017_01_04  2261.60  2251.57  2251.57  2251.57  2251.57   \n",
            "2           2  2017_01_05  2268.18  2261.60  2251.57  2251.57  2251.57   \n",
            "3           3  2017_01_06  2271.14  2268.18  2261.60  2251.57  2251.57   \n",
            "\n",
            "     lag_5  \n",
            "0  2251.57  \n",
            "1  2251.57  \n",
            "2  2251.57  \n",
            "3  2251.57  \n"
          ]
        }
      ],
      "source": [
        "def build_lag_features_sp500(sp500_df: pd.DataFrame, L: int = 5) -> pd.DataFrame:\n",
        "    sp = sp500_df.sort_values(\"date_index\").reset_index(drop=True)\n",
        "    values = sp[\"value\"].values\n",
        "    T = len(values)\n",
        "\n",
        "    data = {\n",
        "        \"date_index\": sp[\"date_index\"],\n",
        "        \"pub_date\": sp[\"date_str\"],\n",
        "        \"value\": sp[\"value\"],\n",
        "    }\n",
        "    for k in range(1, L+1):\n",
        "        lag_list = []\n",
        "        for j in range(T):\n",
        "            prev_idx = j - k\n",
        "            if prev_idx < 0:\n",
        "                lag_list.append(values[0])\n",
        "            else:\n",
        "                lag_list.append(values[prev_idx])\n",
        "        data[f\"lag_{k}\"] = lag_list\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "print(\"=== STEP 8: Build dataset_A ===\")\n",
        "dataset_A = build_lag_features_sp500(sp500_df, L=LAG_L)\n",
        "print(\"dataset_A.shape =\", dataset_A.shape)\n",
        "print(dataset_A.head(4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APsBPjj5c64r"
      },
      "source": [
        "9. Fear-Greed: S&P 거래일 기준 align + lag\n",
        "- fg_on_sp: 각 S&P date_index에 대응하는 Fear-Greed 값\n",
        "- fg_lag_df: date_index, fg_value, fg_lag_1..fg_lag_L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFo4PbQIc4qx",
        "outputId": "74556d62-a7e7-4235-9104-51d788305aa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== STEP 9-1: Align Fear-Greed to S&P ===\n",
            "fg_on_sp.shape = (754, 3)\n",
            "        Date  date_index  fg_value\n",
            "0 2017-01-03           0      70.0\n",
            "1 2017-01-04           1      70.0\n",
            "2 2017-01-05           2      70.0\n",
            "3 2017-01-06           3      68.0\n",
            "\n",
            "=== STEP 9-2: Build fg_lag_df ===\n",
            "fg_lag_df.shape = (754, 7)\n",
            "   date_index  fg_value  fg_lag_1  fg_lag_2  fg_lag_3  fg_lag_4  fg_lag_5\n",
            "0           0      70.0      70.0      70.0      70.0      70.0      70.0\n",
            "1           1      70.0      70.0      70.0      70.0      70.0      70.0\n",
            "2           2      70.0      70.0      70.0      70.0      70.0      70.0\n",
            "3           3      68.0      70.0      70.0      70.0      70.0      70.0\n"
          ]
        }
      ],
      "source": [
        "def align_fear_greed_to_sp500(sp500_df, fg_raw, date_index_df):\n",
        "    # global calendar에 대해 FG 시계열 만들기\n",
        "    fg_full = fg_raw.set_index(\"Date\").reindex(date_index_df[\"Date\"])\n",
        "    fg_full[\"value\"] = fg_full[\"value\"].ffill().bfill()\n",
        "    fg_full = fg_full.reset_index().rename(columns={\"value\": \"fg_value\"})\n",
        "\n",
        "    # S&P 거래일에 대해 Fear& Greed 가져오기\n",
        "    sp = sp500_df[[\"Date\", \"date_index\"]].merge(\n",
        "        fg_full[[\"Date\", \"fg_value\"]],\n",
        "        on=\"Date\",\n",
        "        how=\"left\"\n",
        "    )\n",
        "    return sp\n",
        "\n",
        "print(\"=== STEP 9-1: Align Fear-Greed to S&P ===\")\n",
        "fg_on_sp = align_fear_greed_to_sp500(sp500_df, fear_greed_raw, date_index_df)\n",
        "print(\"fg_on_sp.shape =\", fg_on_sp.shape)\n",
        "print(fg_on_sp.head(4))\n",
        "\n",
        "def build_lag_features_fear_greed(fg_on_sp: pd.DataFrame, L: int = 5) -> pd.DataFrame:\n",
        "    fg = fg_on_sp.sort_values(\"date_index\").reset_index(drop=True)\n",
        "    values = fg[\"fg_value\"].values\n",
        "    T = len(values)\n",
        "\n",
        "    data = {\n",
        "        \"date_index\": fg[\"date_index\"],\n",
        "        \"fg_value\": fg[\"fg_value\"],\n",
        "    }\n",
        "    for k in range(1, L+1):\n",
        "        lag_list = []\n",
        "        for j in range(T):\n",
        "            prev_idx = j - k\n",
        "            if prev_idx < 0:\n",
        "                lag_list.append(values[0])\n",
        "            else:\n",
        "                lag_list.append(values[prev_idx])\n",
        "        data[f\"fg_lag_{k}\"] = lag_list\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "print(\"\\n=== STEP 9-2: Build fg_lag_df ===\")\n",
        "fg_lag_df = build_lag_features_fear_greed(fg_on_sp, L=LAG_L)\n",
        "print(\"fg_lag_df.shape =\", fg_lag_df.shape)\n",
        "print(fg_lag_df.head(4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRR9KaB_dRIH"
      },
      "source": [
        "10. metadata에 date_index & person_id 붙이기\n",
        "- article_date를 date_index_df와 merge해서 date_index 얻기\n",
        "- person 문자열을 person_id(1..NUM_PERSONS)로 매핑"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7HE3onwdP1Q",
        "outputId": "43b9b3c8-776d-4b77-8751-7c32076fb9dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== STEP 10: Attach date_index & person_id to metadata ===\n",
            "\n",
            "[method: headlines]\n",
            "meta_with.shape = (461270, 6)\n",
            "        person                                         article_id    pub_date  \\\n",
            "0  alex_morgan  uk-news/2017/dec/31/new-years-eve-celebrations...  2017_12_31   \n",
            "1  alex_morgan  sport/2017/dec/31/alastair-cook-david-warner-t...  2017_12_31   \n",
            "2  alex_morgan  world/2017/dec/31/eight-big-ideas-for-2018-pol...  2017_12_31   \n",
            "3  alex_morgan  world/2017/dec/31/look-to-the-future-what-does...  2017_12_31   \n",
            "\n",
            "  article_date  date_index  person_id  \n",
            "0   2017-12-31         250          1  \n",
            "1   2017-12-31         250          1  \n",
            "2   2017-12-31         250          1  \n",
            "3   2017-12-31         250          1  \n",
            "\n",
            "[method: chunking]\n",
            "meta_with.shape = (460722, 6)\n",
            "        person                                         article_id    pub_date  \\\n",
            "0  alex_morgan  uk-news/2017/dec/31/new-years-eve-celebrations...  2017_12_31   \n",
            "1  alex_morgan  sport/2017/dec/31/alastair-cook-david-warner-t...  2017_12_31   \n",
            "2  alex_morgan  world/2017/dec/31/eight-big-ideas-for-2018-pol...  2017_12_31   \n",
            "3  alex_morgan  world/2017/dec/31/look-to-the-future-what-does...  2017_12_31   \n",
            "\n",
            "  article_date  date_index  person_id  \n",
            "0   2017-12-31         250          1  \n",
            "1   2017-12-31         250          1  \n",
            "2   2017-12-31         250          1  \n",
            "3   2017-12-31         250          1  \n",
            "\n",
            "[method: bodyText]\n",
            "meta_with.shape = (460722, 6)\n",
            "        person                                         article_id    pub_date  \\\n",
            "0  alex_morgan  uk-news/2017/dec/31/new-years-eve-celebrations...  2017_12_31   \n",
            "1  alex_morgan  sport/2017/dec/31/alastair-cook-david-warner-t...  2017_12_31   \n",
            "2  alex_morgan  world/2017/dec/31/eight-big-ideas-for-2018-pol...  2017_12_31   \n",
            "3  alex_morgan  world/2017/dec/31/look-to-the-future-what-does...  2017_12_31   \n",
            "\n",
            "  article_date  date_index  person_id  \n",
            "0   2017-12-31         250          1  \n",
            "1   2017-12-31         250          1  \n",
            "2   2017-12-31         250          1  \n",
            "3   2017-12-31         250          1  \n",
            "\n",
            "[method: paragraphs]\n",
            "meta_with.shape = (460722, 6)\n",
            "        person                                         article_id    pub_date  \\\n",
            "0  alex_morgan  uk-news/2017/dec/31/new-years-eve-celebrations...  2017_12_31   \n",
            "1  alex_morgan  sport/2017/dec/31/alastair-cook-david-warner-t...  2017_12_31   \n",
            "2  alex_morgan  world/2017/dec/31/eight-big-ideas-for-2018-pol...  2017_12_31   \n",
            "3  alex_morgan  world/2017/dec/31/look-to-the-future-what-does...  2017_12_31   \n",
            "\n",
            "  article_date  date_index  person_id  \n",
            "0   2017-12-31         250          1  \n",
            "1   2017-12-31         250          1  \n",
            "2   2017-12-31         250          1  \n",
            "3   2017-12-31         250          1  \n"
          ]
        }
      ],
      "source": [
        "def attach_date_index_and_person_id(meta_df, date_index_df, person_to_id, method_name=\"\"):\n",
        "    # pub_date(str, \"YYYY_MM_DD\") -> Timestamp로 변환\n",
        "    df = meta_df.copy()\n",
        "    df[\"pub_ts\"] = df[\"pub_date\"].apply(pubdate_to_timestamp)\n",
        "\n",
        "    # pub_ts(Timestamp) 기준으로 date_index_df와 매칭\n",
        "    df = df.merge(\n",
        "        date_index_df[[\"Date\", \"date_index\"]],\n",
        "        left_on=\"pub_ts\",\n",
        "        right_on=\"Date\",\n",
        "        how=\"left\"\n",
        "    )\n",
        "\n",
        "    # 더 이상 필요 없는 컬럼 제거\n",
        "    df = df.drop(columns=[\"Date\", \"pub_ts\"])\n",
        "\n",
        "    # date_index NaN이면 마지막 index로 채운다 (이론상 거의 없을 것)\n",
        "    last_idx = sp500_df[\"date_index\"].max()\n",
        "    df[\"date_index\"] = df[\"date_index\"].fillna(last_idx).astype(int)\n",
        "\n",
        "    # person_id 부여\n",
        "    df[\"person_id\"] = df[\"person\"].map(person_to_id)\n",
        "    if df[\"person_id\"].isna().any():\n",
        "        unknown = df[df[\"person_id\"].isna()][\"person\"].unique()\n",
        "        raise ValueError(f\"[{method_name}] person_to_id 매핑에 없는 person이 있습니다: {unknown}\")\n",
        "    return df\n",
        "\n",
        "print(\"=== STEP 10: Attach date_index & person_id to metadata ===\")\n",
        "meta_with_idx_dict = {}\n",
        "for method_name, meta_df in meta_raw_dict.items():\n",
        "    print(f\"\\n[method: {method_name}]\")\n",
        "    meta_with = attach_date_index_and_person_id(meta_df.copy(), date_index_df, person_to_id, method_name)\n",
        "    meta_with_idx_dict[method_name] = meta_with\n",
        "    print(\"meta_with.shape =\", meta_with.shape)\n",
        "    print(meta_with.head(4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1OUcBjQdcFF"
      },
      "source": [
        "11. [함수 정의] embeddings 로드 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUrjiH_snQkf",
        "outputId": "30f6e01b-a1ee-4208-dc2b-424bf07f196e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== STEP 11: Function 'load_embedding_with_metadata' defined. ===\n"
          ]
        }
      ],
      "source": [
        "def load_embedding_with_metadata(config: dict, meta_df: pd.DataFrame, method_name: str):\n",
        "    emb = np.load(config[\"emb_path\"])\n",
        "    N, d = emb.shape\n",
        "    if len(meta_df) != N:\n",
        "        raise ValueError(\n",
        "            f\"[{method_name}] embeddings(N={N})과 metadata(len={len(meta_df)}) 길이가 다릅니다.\"\n",
        "        )\n",
        "    df = meta_df.copy().reset_index(drop=True)\n",
        "    df[\"embedding\"] = list(emb)\n",
        "\n",
        "    print(f\"[{method_name}] emb_df.shape = {df.shape}, embedding_dim = {d}\")\n",
        "    print(df.head(3))\n",
        "    return df, d\n",
        "print(\"=== STEP 11: Function 'load_embedding_with_metadata' defined. ===\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpQVp-t_djQL"
      },
      "source": [
        "12. [함수 정의] Dataset B 생성 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tz1oOggtrrb6",
        "outputId": "4a7c8f29-e25a-444f-fad7-a9d3b31f30cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== STEP 12: Function 'build_dataset_B_for_method_light' defined.(MEMORY SAFE) ===\n"
          ]
        }
      ],
      "source": [
        "# ============================\n",
        "# STEP 12 — Build dataset_B (RAM-SAFE & Logic Fixed)\n",
        "# ============================\n",
        "\n",
        "def build_dataset_B_for_method_light(method_name: str, emb_df: pd.DataFrame, dataset_A: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    메모리 절약 버전: embedding을 1024개 컬럼으로 풀지 않고 merge만 수행.\n",
        "    수정사항: merge 키에서 'date_str' 제외 (주말 기사 매칭 문제 해결)\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Building dataset_B for {method_name} ---\")\n",
        "\n",
        "    # 병합할 때 SP500 데이터프레임에서 불필요한 중복 컬럼(date_str) 제외\n",
        "    # date_index, value, lag_1... 만 가져옴\n",
        "    cols_to_use = [\"date_index\", \"value\"] + [f\"lag_{k}\" for k in range(1, LAG_L+1)]\n",
        "    base_for_merge = dataset_A[cols_to_use]\n",
        "\n",
        "    print(\" base_for_merge shape:\", base_for_merge.shape)\n",
        "\n",
        "    # date_index 기준으로 병합\n",
        "    dataset_B = emb_df.merge(\n",
        "        base_for_merge,\n",
        "        on=\"date_index\",\n",
        "        how=\"left\"\n",
        "    )\n",
        "\n",
        "    # print(f\"[{method_name}] dataset_B.shape =\", dataset_B.shape)\n",
        "    # 확인: value 컬럼에 NaN이 없는지 체크 (앞부분 패딩 구간 제외하고)\n",
        "    nan_count = dataset_B['value'].isna().sum()\n",
        "    # print(f\"[{method_name}] Rows with NaN value (should be 0 or very small): {nan_count}\")\n",
        "\n",
        "    return dataset_B\n",
        "\n",
        "\n",
        "print(\"=== STEP 12: Function 'build_dataset_B_for_method_light' defined.(MEMORY SAFE) ===\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wur0tCb2dtyL"
      },
      "source": [
        "13. Feature C: B + person one-hot\n",
        "- person_id (1..NUM_PERSONS)를 one-hot(100차원)으로 붙인다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "W9Ga3Ky8rw4W"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# STEP 13 — Build dataset_C (one-hot)\n",
        "# ============================\n",
        "\n",
        "def add_person_one_hot(dataset_B: pd.DataFrame, num_persons: int = NUM_PERSONS):\n",
        "    print(\"\\n--- Building dataset_C (one-hot) ---\")\n",
        "    print(\" input dataset_B.shape:\", dataset_B.shape)\n",
        "    print(dataset_B.head(3))\n",
        "\n",
        "    oh = pd.get_dummies(dataset_B[\"person_id\"], prefix=\"person\")\n",
        "\n",
        "    expected_cols = [f\"person_{i}\" for i in range(1, num_persons + 1)]\n",
        "    for col in expected_cols:\n",
        "        if col not in oh.columns:\n",
        "            oh[col] = 0\n",
        "    oh = oh[expected_cols]\n",
        "\n",
        "    dataset_C = pd.concat([dataset_B, oh], axis=1)\n",
        "\n",
        "    print(\"dataset_C.shape =\", dataset_C.shape)\n",
        "    print(dataset_C.head(3))\n",
        "    return dataset_C, expected_cols"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1buG4KQd4Mg"
      },
      "source": [
        "14. Feature D: C + Fear-Greed lag\n",
        "- dataset_C에 fg_lag_df를 date_index 기준으로 merge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "RY1alI7Yr0CK"
      },
      "outputs": [],
      "source": [
        "# ============================\n",
        "# STEP 14 — Build dataset_D (FG lags)\n",
        "# ============================\n",
        "\n",
        "def add_fear_greed_lags_to_dataset_C(dataset_C: pd.DataFrame, fg_lag_df: pd.DataFrame, L: int = 5):\n",
        "    print(\"\\n--- Building dataset_D (FG lags) ---\")\n",
        "    print(\" input dataset_C.shape:\", dataset_C.shape)\n",
        "    print(dataset_C.head(3))\n",
        "\n",
        "    merged = dataset_C.merge(\n",
        "        fg_lag_df,\n",
        "        on=\"date_index\",\n",
        "        how=\"left\"\n",
        "    )\n",
        "\n",
        "    print(\"merged (dataset_D).shape =\", merged.shape)\n",
        "    print(merged.head(3))\n",
        "\n",
        "    fg_cols = [f\"fg_lag_{k}\" for k in range(1, L + 1)]\n",
        "    return merged, fg_cols"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmoibNpm68t0"
      },
      "source": [
        "15. Dimensionality reduction - [함수 정의] PCA 학습 및 변환 함수\n",
        "- 누적 설명 분산 비율(Explained Variance Ratio)이 90%가 되도록 차원 수를 자동으로 결정."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DD-lS5Dtn72Z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from pathlib import Path\n",
        "import gc\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 15: PCA Functions (Sampling + Batch Processing)\n",
        "# ==============================================================================\n",
        "# 메모리 절약을 위해 정의하는 함수들입니다.\n",
        "\n",
        "def fit_pca_on_sample(dataset_B, variance_ratio=0.90, sample_size=100000, random_state=42):\n",
        "    \"\"\"일부 샘플만 사용하여 PCA 학습\"\"\"\n",
        "    print(f\"   [PCA Fit] Sampling {sample_size} rows...\")\n",
        "    n_total = len(dataset_B)\n",
        "\n",
        "    # 샘플링 인덱스 추출\n",
        "    if n_total <= sample_size:\n",
        "        indices = np.arange(n_total)\n",
        "    else:\n",
        "        np.random.seed(random_state)\n",
        "        indices = np.random.choice(n_total, sample_size, replace=False)\n",
        "\n",
        "    # 임베딩 추출 (Stacking)\n",
        "    X_sample = np.stack(dataset_B.iloc[indices]['embedding'].values)\n",
        "\n",
        "    # 학습\n",
        "    pca = PCA(n_components=variance_ratio, random_state=random_state)\n",
        "    pca.fit(X_sample)\n",
        "\n",
        "    print(f\"   [PCA Fit] Components selected: {pca.n_components_}\")\n",
        "    return pca\n",
        "\n",
        "def transform_in_batches(dataset_B, pca_model, batch_size=10000):\n",
        "    \"\"\"전체 데이터를 배치 단위로 나누어 PCA 변환\"\"\"\n",
        "    print(f\"   [PCA Transform] transforming in batches...\")\n",
        "    n_total = len(dataset_B)\n",
        "    pca_results = []\n",
        "\n",
        "    for start_idx in range(0, n_total, batch_size):\n",
        "        end_idx = min(start_idx + batch_size, n_total)\n",
        "\n",
        "        # 배치 추출 및 스택\n",
        "        batch_emb = np.stack(dataset_B.iloc[start_idx:end_idx]['embedding'].values)\n",
        "\n",
        "        # 변환\n",
        "        batch_pca = pca_model.transform(batch_emb)\n",
        "        pca_results.append(batch_pca)\n",
        "\n",
        "        # 메모리 해제\n",
        "        del batch_emb\n",
        "\n",
        "    # 전체 합치기\n",
        "    X_pca_full = np.vstack(pca_results)\n",
        "\n",
        "    # DataFrame 생성\n",
        "    pca_cols = [f\"pca_{i}\" for i in range(X_pca_full.shape[1])]\n",
        "    df_pca = pd.DataFrame(X_pca_full, columns=pca_cols, index=dataset_B.index)\n",
        "\n",
        "    # 원본 컬럼과 병합 ('embedding' 제외)\n",
        "    cols_to_keep = [c for c in dataset_B.columns if c != 'embedding']\n",
        "    dataset_B_pca = pd.concat([dataset_B[cols_to_keep], df_pca], axis=1)\n",
        "\n",
        "    return dataset_B_pca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================\n",
        "# STEP 15.5 — Embedding flatten + column reordering helpers\n",
        "# ============================\n",
        "\n",
        "def expand_embedding_for_save(df: pd.DataFrame, emb_col: str = \"embedding\", prefix: str = \"emb_\"):\n",
        "    if emb_col not in df.columns:\n",
        "        return df.copy(), []\n",
        "\n",
        "    emb_array = np.stack(df[emb_col].values)  # (N, d)\n",
        "    d = emb_array.shape[1]\n",
        "\n",
        "    emb_cols = [f\"{prefix}{i}\" for i in range(d)]\n",
        "    emb_df = pd.DataFrame(emb_array, columns=emb_cols, index=df.index)\n",
        "\n",
        "    out = pd.concat([df.drop(columns=[emb_col]), emb_df], axis=1)\n",
        "    return out, emb_cols\n",
        "\n",
        "\n",
        "def reorder_columns_global(\n",
        "    df: pd.DataFrame,\n",
        "    emb_cols: list,\n",
        "    lag_L: int = LAG_L,\n",
        "    num_persons: int = NUM_PERSONS,\n",
        "):\n",
        "    \"\"\"\n",
        "    순서:\n",
        "    [ pub_date, date_index, person, person_id,\n",
        "      value, lag_1..lag_L,\n",
        "      emb_0..emb_(d-1),\n",
        "      person_1..person_num,\n",
        "      fg_value, fg_lag_1..fg_lag_L,\n",
        "      (그 외 컬럼들) ]\n",
        "    \"\"\"\n",
        "    base_cols = [\"pub_date\", \"date_index\", \"person\", \"person_id\", \"value\"]\n",
        "    lag_cols = [f\"lag_{k}\" for k in range(1, lag_L + 1)]\n",
        "    person_cols = [f\"person_{i}\" for i in range(1, num_persons + 1)]\n",
        "    fg_cols = [\"fg_value\"] + [f\"fg_lag_{k}\" for k in range(1, lag_L + 1)]\n",
        "\n",
        "    desired = base_cols + lag_cols + emb_cols + person_cols + fg_cols\n",
        "\n",
        "    # 실제로 존재하는 컬럼만 유지\n",
        "    ordered_existing = [c for c in desired if c in df.columns]\n",
        "    remaining = [c for c in df.columns if c not in ordered_existing]\n",
        "\n",
        "    return df[ordered_existing + remaining]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usZoD8WJUo_9"
      },
      "source": [
        "16. 결과 저장\n",
        "\n",
        "- Dataset C와 D에 대해 별도로 PCA를 돌리지 않고, PCA가 적용된 Dataset B(df_B_pca)를 이용하여 df_C_pca, df_D_pca를 생성.\n",
        "\n",
        "- 파일명 구분: _orig(원본 임베딩)와 _pca(차원 축소됨)로 구분하여 저장."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H36KRcB9LbwI",
        "outputId": "6c04c38d-569e-4cac-e7e9-4c6ae97be58b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== STEP 16: Sequential Pipeline Started (with Skip functionality) ===\n",
            "Output Directory: /Users/migreeni/Library/CloudStorage/OneDrive-개인/25-2/Machine learning/20252R0136COSE36203/feature_datasets\n",
            "\n",
            "============================================================\n",
            "Processing: Dataset A\n",
            "============================================================\n",
            "✅ SKIPPING: dataset_A.parquet already exists\n",
            "\n",
            "============================================================\n",
            "Processing Method: HEADLINES\n",
            "============================================================\n",
            "✅ SKIPPING: All files for headlines already exist\n",
            "   - dataset_B_headlines_orig.parquet\n",
            "   - dataset_C_headlines_orig.parquet\n",
            "   - dataset_D_headlines_orig.parquet\n",
            "   - dataset_B_headlines_pca.parquet\n",
            "   - dataset_C_headlines_pca.parquet\n",
            "   - dataset_D_headlines_pca.parquet\n",
            "\n",
            "============================================================\n",
            "Processing Method: CHUNKING\n",
            "============================================================\n",
            "✅ SKIPPING: All files for chunking already exist\n",
            "   - dataset_B_chunking_orig.parquet\n",
            "   - dataset_C_chunking_orig.parquet\n",
            "   - dataset_D_chunking_orig.parquet\n",
            "   - dataset_B_chunking_pca.parquet\n",
            "   - dataset_C_chunking_pca.parquet\n",
            "   - dataset_D_chunking_pca.parquet\n",
            "\n",
            "============================================================\n",
            "Processing Method: BODYTEXT\n",
            "============================================================\n",
            "✅ SKIPPING: All files for bodyText already exist\n",
            "   - dataset_B_bodyText_orig.parquet\n",
            "   - dataset_C_bodyText_orig.parquet\n",
            "   - dataset_D_bodyText_orig.parquet\n",
            "   - dataset_B_bodyText_pca.parquet\n",
            "   - dataset_C_bodyText_pca.parquet\n",
            "   - dataset_D_bodyText_pca.parquet\n",
            "\n",
            "============================================================\n",
            "Processing Method: PARAGRAPHS\n",
            "============================================================\n",
            "✅ SKIPPING: All files for paragraphs already exist\n",
            "   - dataset_B_paragraphs_orig.parquet\n",
            "   - dataset_C_paragraphs_orig.parquet\n",
            "   - dataset_D_paragraphs_orig.parquet\n",
            "   - dataset_B_paragraphs_pca.parquet\n",
            "   - dataset_C_paragraphs_pca.parquet\n",
            "   - dataset_D_paragraphs_pca.parquet\n",
            "\n",
            "============================================================\n",
            "All datasets processed successfully!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "from pathlib import Path\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 16: Generate All Datasets & Save (Sequential Processing with Skip)\n",
        "# ==============================================================================\n",
        "\n",
        "OUTPUT_ROOT = Path(ROOT) / \"feature_datasets\"\n",
        "OUTPUT_ROOT.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "print(\"=== STEP 16: Sequential Pipeline Started (with Skip functionality) ===\")\n",
        "print(f\"Output Directory: {OUTPUT_ROOT}\")\n",
        "\n",
        "# ========================================\n",
        "# Helper function: Check if all files exist\n",
        "# ========================================\n",
        "def check_files_exist(output_root, method=None):\n",
        "    \"\"\"\n",
        "    Check if dataset files already exist\n",
        "    \n",
        "    Args:\n",
        "        output_root: Path to output directory\n",
        "        method: Method name (e.g., 'headlines'). If None, checks Dataset A only.\n",
        "    \n",
        "    Returns:\n",
        "        dict: {\"exists\": bool, \"missing\": list of missing files}\n",
        "    \"\"\"\n",
        "    if method is None:\n",
        "        # Dataset A만 체크\n",
        "        file_path = output_root / \"dataset_A.parquet\"\n",
        "        if file_path.exists():\n",
        "            return {\"exists\": True, \"missing\": []}\n",
        "        else:\n",
        "            return {\"exists\": False, \"missing\": [\"dataset_A.parquet\"]}\n",
        "    \n",
        "    # Method 관련 파일 체크\n",
        "    files_to_check = [\n",
        "        f\"dataset_B_{method}_orig.parquet\",\n",
        "        f\"dataset_C_{method}_orig.parquet\",\n",
        "        f\"dataset_D_{method}_orig.parquet\",\n",
        "        f\"dataset_B_{method}_pca.parquet\",\n",
        "        f\"dataset_C_{method}_pca.parquet\",\n",
        "        f\"dataset_D_{method}_pca.parquet\",\n",
        "    ]\n",
        "    \n",
        "    missing = []\n",
        "    for fname in files_to_check:\n",
        "        if not (output_root / fname).exists():\n",
        "            missing.append(fname)\n",
        "    \n",
        "    if not missing:\n",
        "        return {\"exists\": True, \"missing\": []}\n",
        "    else:\n",
        "        return {\"exists\": False, \"missing\": missing}\n",
        "\n",
        "# ========================================\n",
        "# 1. Dataset A 저장\n",
        "# ========================================\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Processing: Dataset A\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "check_result = check_files_exist(OUTPUT_ROOT, method=None)\n",
        "\n",
        "if check_result[\"exists\"]:\n",
        "    print(\"✅ SKIPPING: dataset_A.parquet already exists\")\n",
        "else:\n",
        "    print(\"[Saving] Dataset A...\")\n",
        "    dataset_A.to_parquet(OUTPUT_ROOT / \"dataset_A.parquet\", index=False)\n",
        "    print(\"✅ Saved: dataset_A.parquet\")\n",
        "\n",
        "# ========================================\n",
        "# 2. Method 별 순차 처리\n",
        "# ========================================\n",
        "methods = [\"headlines\", \"chunking\", \"bodyText\", \"paragraphs\"]\n",
        "\n",
        "for method in methods:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Processing Method: {method.upper()}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # 메타데이터 확인\n",
        "    if method not in meta_with_idx_dict:\n",
        "        print(f\"⚠️  SKIPPING: {method} (Metadata not found or excluded in Config)\")\n",
        "        continue\n",
        "    \n",
        "    # ========================================\n",
        "    # 파일 존재 여부 체크\n",
        "    # ========================================\n",
        "    check_result = check_files_exist(OUTPUT_ROOT, method=method)\n",
        "    \n",
        "    if check_result[\"exists\"]:\n",
        "        print(f\"✅ SKIPPING: All files for {method} already exist\")\n",
        "        print(f\"   - dataset_B_{method}_orig.parquet\")\n",
        "        print(f\"   - dataset_C_{method}_orig.parquet\")\n",
        "        print(f\"   - dataset_D_{method}_orig.parquet\")\n",
        "        print(f\"   - dataset_B_{method}_pca.parquet\")\n",
        "        print(f\"   - dataset_C_{method}_pca.parquet\")\n",
        "        print(f\"   - dataset_D_{method}_pca.parquet\")\n",
        "        continue\n",
        "    else:\n",
        "        print(f\"⚠️  Missing files detected: {len(check_result['missing'])} files\")\n",
        "        print(f\"   Will generate all datasets for {method}\")\n",
        "        for missing_file in check_result['missing']:\n",
        "            print(f\"     - {missing_file}\")\n",
        "    \n",
        "    # ========================================\n",
        "    # A. 데이터 로드 (Embeddings)\n",
        "    # ========================================\n",
        "    print(f\"\\n   [Loading] Embeddings for {method}...\")\n",
        "    cfg = EMB_CONFIGS[method]\n",
        "    emb_df, _ = load_embedding_with_metadata(cfg, meta_with_idx_dict[method], method)\n",
        "\n",
        "    # ========================================\n",
        "    # B. 데이터셋 구축 (Original)\n",
        "    # ========================================\n",
        "    print(f\"   [Building] Original datasets for {method}...\")\n",
        "    df_B_orig = build_dataset_B_for_method_light(method, emb_df, dataset_A)\n",
        "\n",
        "    # 임베딩 데이터프레임은 병합되었으니 즉시 삭제\n",
        "    del emb_df\n",
        "    gc.collect()\n",
        "\n",
        "    df_C_orig, _ = add_person_one_hot(df_B_orig, NUM_PERSONS)\n",
        "    df_D_orig, _ = add_fear_greed_lags_to_dataset_C(df_C_orig, fg_lag_df, LAG_L)\n",
        "\n",
        "    # ========================================\n",
        "    # C. 저장 (Original)\n",
        "    # ========================================\n",
        "    print(f\"   [Saving] Original datasets for {method}...\")\n",
        "\n",
        "    # embedding 펼치기 및 컬럼 정렬\n",
        "    df_B_save, emb_cols_B = expand_embedding_for_save(df_B_orig, emb_col=\"embedding\", prefix=\"emb_\")\n",
        "    df_C_save, emb_cols_C = expand_embedding_for_save(df_C_orig, emb_col=\"embedding\", prefix=\"emb_\")\n",
        "    df_D_save, emb_cols_D = expand_embedding_for_save(df_D_orig, emb_col=\"embedding\", prefix=\"emb_\")\n",
        "\n",
        "    df_B_save = reorder_columns_global(df_B_save, emb_cols=emb_cols_B, lag_L=LAG_L, num_persons=NUM_PERSONS)\n",
        "    df_C_save = reorder_columns_global(df_C_save, emb_cols=emb_cols_C, lag_L=LAG_L, num_persons=NUM_PERSONS)\n",
        "    df_D_save = reorder_columns_global(df_D_save, emb_cols=emb_cols_D, lag_L=LAG_L, num_persons=NUM_PERSONS)\n",
        "\n",
        "    # 저장\n",
        "    df_B_save.to_parquet(OUTPUT_ROOT / f\"dataset_B_{method}_orig.parquet\", index=False)\n",
        "    df_C_save.to_parquet(OUTPUT_ROOT / f\"dataset_C_{method}_orig.parquet\", index=False)\n",
        "    df_D_save.to_parquet(OUTPUT_ROOT / f\"dataset_D_{method}_orig.parquet\", index=False)\n",
        "    print(f\"     ✅ Saved: dataset_B_{method}_orig.parquet\")\n",
        "    print(f\"     ✅ Saved: dataset_C_{method}_orig.parquet\")\n",
        "    print(f\"     ✅ Saved: dataset_D_{method}_orig.parquet\")\n",
        "\n",
        "    # ========================================\n",
        "    # D. PCA 학습 및 변환\n",
        "    # ========================================\n",
        "    print(f\"   [Processing] PCA for {method}...\")\n",
        "    pca_model = fit_pca_on_sample(df_B_orig, variance_ratio=0.90)\n",
        "    df_B_pca = transform_in_batches(df_B_orig, pca_model)\n",
        "\n",
        "    # 파생 데이터셋 (PCA 버전)\n",
        "    df_C_pca, _ = add_person_one_hot(df_B_pca, NUM_PERSONS)\n",
        "    df_D_pca, _ = add_fear_greed_lags_to_dataset_C(df_C_pca, fg_lag_df, LAG_L)\n",
        "\n",
        "    # ========================================\n",
        "    # E. 저장 (PCA)\n",
        "    # ========================================\n",
        "    print(f\"   [Saving] PCA datasets for {method}...\")\n",
        "    df_B_pca.to_parquet(OUTPUT_ROOT / f\"dataset_B_{method}_pca.parquet\", index=False)\n",
        "    df_C_pca.to_parquet(OUTPUT_ROOT / f\"dataset_C_{method}_pca.parquet\", index=False)\n",
        "    df_D_pca.to_parquet(OUTPUT_ROOT / f\"dataset_D_{method}_pca.parquet\", index=False)\n",
        "    print(f\"     ✅ Saved: dataset_B_{method}_pca.parquet\")\n",
        "    print(f\"     ✅ Saved: dataset_C_{method}_pca.parquet\")\n",
        "    print(f\"     ✅ Saved: dataset_D_{method}_pca.parquet\")\n",
        "\n",
        "    # ========================================\n",
        "    # F. 메모리 정리 (Cleanup)\n",
        "    # ========================================\n",
        "    print(f\"   [Cleanup] Clearing memory for {method}...\")\n",
        "    del df_B_orig, df_C_orig, df_D_orig\n",
        "    del df_B_pca, df_C_pca, df_D_pca, pca_model\n",
        "    del df_B_save, df_C_save, df_D_save\n",
        "    gc.collect()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"All datasets processed successfully!\")\n",
        "print(\"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mlproject",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
