{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3776fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147f3e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wonjun/.conda/envs/mlproject/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: library imports\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333e8000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Sentence-BERT model...\n",
      "Model loaded! Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: ê²½ë¡œ ì„¤ì •\n",
    "# ê²½ë¡œ ì„¤ì •\n",
    "DATA_DIR = Path(\"guardian_scrapping\")  # JSONL íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬\n",
    "OUTPUT_DIR = Path(\"./miniLM_headline_outputs\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ì¶œë ¥ íŒŒì¼ ê²½ë¡œ\n",
    "EMBEDDINGS_FILE = OUTPUT_DIR / \"embeddings.npy\"\n",
    "METADATA_FILE = OUTPUT_DIR / \"metadata.jsonl\" \n",
    "CHECKPOINT_FILE = OUTPUT_DIR / \"checkpoint.txt\"\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ (all_miniLM)\n",
    "print(\"Loading Sentence-BERT model...\")\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
    "print(f\"Model loaded! Embedding dimension: {model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1426055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 people:\n",
      "['adam_neumann', 'adam_rippon', 'alex_wind', 'ann_mckee', 'bhavish_aggarwal', 'cameron_kasky', 'cardi_b', 'carl_june', 'carmen_yulin_cruz', 'chadwick_boseman'] ...\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: JSONL íŒŒì¼ ì°¾ê¸° ë° people_list ìƒì„±\n",
    "# ëª¨ë“  JSONL íŒŒì¼ ì°¾ê¸° (scrapping_csv í´ë” ì œì™¸)\n",
    "jsonl_files = [f for f in DATA_DIR.glob(\"*.jsonl\")]\n",
    "\n",
    "# people_list ìƒì„± (íŒŒì¼ëª…ì—ì„œ .jsonl ì œê±°)\n",
    "people_list = sorted([f.stem for f in jsonl_files])\n",
    "\n",
    "print(f\"Found {len(people_list)} people:\")\n",
    "print(people_list[:10], \"...\" if len(people_list) > 10 else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0df3c0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ†• Starting fresh\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: ì´ì–´ë‹¬ë¦¬ê¸° ê¸°ëŠ¥ - ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ í™•ì¸\n",
    "\"\"\"\n",
    "ì´ì–´ë‹¬ë¦¬ê¸° ë¡œì§ ì„¤ëª…:\n",
    "1. checkpoint.txt íŒŒì¼ì— ë§ˆì§€ë§‰ìœ¼ë¡œ ì²˜ë¦¬í•œ ì‚¬ëŒì˜ ì´ë¦„ì„ ì €ì¥\n",
    "2. í”„ë¡œê·¸ë¨ ì¬ì‹œì‘ ì‹œ checkpoint.txtë¥¼ ì½ì–´ì„œ ì–´ë””ê¹Œì§€ ì²˜ë¦¬í–ˆëŠ”ì§€ í™•ì¸\n",
    "3. ì´ë¯¸ ì²˜ë¦¬ëœ ì‚¬ëŒì€ ê±´ë„ˆë›°ê³ , ê·¸ ë‹¤ìŒ ì‚¬ëŒë¶€í„° ì²˜ë¦¬\n",
    "4. embeddings.npyì™€ metadata.jsonlì€ append ëª¨ë“œë¡œ ì €ì¥\n",
    "\"\"\"\n",
    "\n",
    "def get_last_processed():\n",
    "    \"\"\"ë§ˆì§€ë§‰ìœ¼ë¡œ ì²˜ë¦¬í•œ ì‚¬ëŒ í™•ì¸\"\"\"\n",
    "    if CHECKPOINT_FILE.exists():\n",
    "        with open(CHECKPOINT_FILE, 'r') as f:\n",
    "            return f.read().strip()\n",
    "    return None\n",
    "\n",
    "def save_checkpoint(person_name):\n",
    "    \"\"\"ì²´í¬í¬ì¸íŠ¸ ì €ì¥\"\"\"\n",
    "    with open(CHECKPOINT_FILE, 'w') as f:\n",
    "        f.write(person_name)\n",
    "\n",
    "def load_existing_data():\n",
    "    \"\"\"ê¸°ì¡´ì— ì €ì¥ëœ ë°ì´í„° ë¡œë“œ\"\"\"\n",
    "    if EMBEDDINGS_FILE.exists() and METADATA_FILE.exists():\n",
    "        embeddings = np.load(EMBEDDINGS_FILE)\n",
    "        \n",
    "        # JSONL íŒŒì¼ ì½ê¸°\n",
    "        metadata = []\n",
    "        with open(METADATA_FILE, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                metadata.append(json.loads(line))\n",
    "        \n",
    "        return embeddings, metadata\n",
    "    return None, None\n",
    "\n",
    "# ê¸°ì¡´ ë°ì´í„° ë¡œë“œ\n",
    "existing_embeddings, existing_metadata = load_existing_data()\n",
    "last_processed = get_last_processed()\n",
    "\n",
    "if last_processed:\n",
    "    print(f\"âœ… Resuming from: {last_processed}\")\n",
    "    print(f\"âœ… Already processed {len(existing_metadata) if existing_metadata else 0} articles\")\n",
    "else:\n",
    "    print(\"ğŸ†• Starting fresh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0d6d60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: ë‚ ì§œ ë³€í™˜ í•¨ìˆ˜\n",
    "def format_date(date_string):\n",
    "    \"\"\"2017-12-21T08:00:01Z -> 2017_12_21\"\"\"\n",
    "    if not date_string:\n",
    "        return None\n",
    "    try:\n",
    "        return date_string.split('T')[0].replace('-', '_')\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6d6ccf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def process_person(person_name, jsonl_path, start_index):\n",
    "    \"\"\"í•œ ì‚¬ëŒì˜ JSONL íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ ì„ë² ë”© ìƒì„±\"\"\"\n",
    "    articles_data = []\n",
    "    \n",
    "    # JSONL íŒŒì¼ ì½ê¸°\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                article = json.loads(line)\n",
    "                \n",
    "                # í•„ìˆ˜ í•„ë“œ í™•ì¸ (ê²°ì¸¡ì¹˜ ì œê±°)\n",
    "                if not article.get('headline') or not article.get('id'):\n",
    "                    continue\n",
    "                \n",
    "                # ë°ì´í„° ì¶”ì¶œ\n",
    "                articles_data.append({\n",
    "                    'person': person_name,\n",
    "                    'article_id': article['id'],\n",
    "                    'pub_date': format_date(article.get('webPublicationDate')),\n",
    "                    'headline': article['headline']\n",
    "                })\n",
    "            except:\n",
    "                continue  # íŒŒì‹± ì—ëŸ¬ ë°œìƒ ì‹œ ê±´ë„ˆë›°ê¸°\n",
    "    \n",
    "    if not articles_data:\n",
    "        return None, None\n",
    "    \n",
    "    # ì„ë² ë”© ìƒì„±\n",
    "    headlines = [item['headline'] for item in articles_data]\n",
    "    embeddings = model.encode(headlines, show_progress_bar=False, convert_to_numpy=True)\n",
    "    \n",
    "    # index ì¶”ê°€\n",
    "    for i, item in enumerate(articles_data):\n",
    "        item['index'] = start_index + i\n",
    "    \n",
    "    return embeddings, articles_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "084c7eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Processing 100 people (from 0/100)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  10%|â–ˆ         | 10/100 [00:06<01:03,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ’¾ Checkpoint saved: 9422 total embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  20%|â–ˆâ–ˆ        | 20/100 [00:33<05:04,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ’¾ Checkpoint saved: 59124 total embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [00:43<01:18,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ’¾ Checkpoint saved: 74550 total embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [01:01<02:32,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ’¾ Checkpoint saved: 102627 total embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [01:21<02:12,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ’¾ Checkpoint saved: 133523 total embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [01:30<00:52,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ’¾ Checkpoint saved: 145224 total embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [01:48<00:46,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ’¾ Checkpoint saved: 171189 total embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [02:03<00:45,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ’¾ Checkpoint saved: 190393 total embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [02:21<00:22,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ’¾ Checkpoint saved: 215186 total embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [02:36<00:00,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ’¾ Checkpoint saved: 232583 total embeddings\n",
      "\n",
      "======================================================================\n",
      "ğŸ‰ Processing complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: ì „ì²´ ì²˜ë¦¬ ì‹¤í–‰\n",
    "\"\"\"\n",
    "ì „ì²´ í”„ë¡œì„¸ìŠ¤:\n",
    "1. people_listë¥¼ ìˆœíšŒí•˜ë©´ì„œ ê° ì‚¬ëŒì˜ JSONL íŒŒì¼ ì²˜ë¦¬\n",
    "2. ì´ì–´ë‹¬ë¦¬ê¸°: last_processed ì´í›„ì˜ ì‚¬ëŒë“¤ë§Œ ì²˜ë¦¬\n",
    "3. ê° ì‚¬ëŒë§ˆë‹¤ ì„ë² ë”©ê³¼ ë©”íƒ€ë°ì´í„° ìƒì„±\n",
    "4. ëˆ„ì í•˜ì—¬ ì €ì¥í•˜ê³  ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸\n",
    "\"\"\"\n",
    "\n",
    "# ê¸°ì¡´ ë°ì´í„°ë¡œ ì‹œì‘\n",
    "all_embeddings = [] if existing_embeddings is None else [existing_embeddings]\n",
    "all_metadata = [] if existing_metadata is None else existing_metadata\n",
    "\n",
    "# í˜„ì¬ ì¸ë±ìŠ¤ (ì´ì–´ë‹¬ë¦¬ê¸°)\n",
    "current_index = len(all_metadata)\n",
    "\n",
    "# ì‹œì‘ ì¸ë±ìŠ¤ ê²°ì • (ì´ì–´ë‹¬ë¦¬ê¸°)\n",
    "start_idx = 0\n",
    "if last_processed and last_processed in people_list:\n",
    "    start_idx = people_list.index(last_processed) + 1\n",
    "\n",
    "# ì²˜ë¦¬í•  ì‚¬ëŒë“¤\n",
    "people_to_process = people_list[start_idx:]\n",
    "\n",
    "print(f\"\\nğŸš€ Processing {len(people_to_process)} people (from {start_idx}/{len(people_list)})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ê° ì‚¬ëŒë³„ë¡œ ì²˜ë¦¬\n",
    "for person_name in tqdm(people_to_process, desc=\"Overall Progress\"):\n",
    "    jsonl_path = DATA_DIR / f\"{person_name}.jsonl\"\n",
    "    \n",
    "    if not jsonl_path.exists():\n",
    "        print(f\"âš ï¸  File not found: {jsonl_path}\")\n",
    "        continue\n",
    "    \n",
    "    #print(f\"\\nğŸ“„ Processing: {person_name}\")\n",
    "    \n",
    "    # ì„ë² ë”© ìƒì„±\n",
    "    embeddings, metadata = process_person(person_name, jsonl_path, current_index)\n",
    "    \n",
    "    if embeddings is None:\n",
    "        print(f\"   âš ï¸  No valid articles found for {person_name}\")\n",
    "        continue\n",
    "    \n",
    "    #print(f\"   âœ… Generated {len(embeddings)} embeddings\")\n",
    "    \n",
    "    # ë°ì´í„° ëˆ„ì \n",
    "    all_embeddings.append(embeddings)\n",
    "    all_metadata.extend(metadata)\n",
    "    current_index += len(embeddings)\n",
    "    \n",
    "    # ì¤‘ê°„ ì €ì¥ (ë©”ëª¨ë¦¬ íš¨ìœ¨)\n",
    "    if len(all_embeddings) % 10 == 0:  # 10ëª…ë§ˆë‹¤ ì €ì¥\n",
    "        # ì„ë² ë”© ì €ì¥\n",
    "        combined_embeddings = np.vstack(all_embeddings)\n",
    "        np.save(EMBEDDINGS_FILE, combined_embeddings)\n",
    "        \n",
    "        # JSONL ì €ì¥\n",
    "        with open(METADATA_FILE, 'w', encoding='utf-8') as f:\n",
    "            for item in all_metadata:\n",
    "                f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        print(f\"   ğŸ’¾ Checkpoint saved: {len(combined_embeddings)} total embeddings\")\n",
    "    \n",
    "    # ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸\n",
    "    save_checkpoint(person_name)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ‰ Processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92076a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Final Results:\n",
      "   ğŸ“Š Total embeddings: (232583, 384)\n",
      "   ğŸ“Š Total articles: 232583\n",
      "   ğŸ’¾ Embeddings saved to: miniLM_headline_outputs/embeddings.npy\n",
      "   ğŸ’¾ Metadata saved to: miniLM_headline_outputs/metadata.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: ìµœì¢… ì €ì¥\n",
    "# ëª¨ë“  ì„ë² ë”© ê²°í•©\n",
    "final_embeddings = np.vstack(all_embeddings)\n",
    "np.save(EMBEDDINGS_FILE, final_embeddings)\n",
    "\n",
    "# JSONL ìµœì¢… ì €ì¥\n",
    "with open(METADATA_FILE, 'w', encoding='utf-8') as f:\n",
    "    for item in all_metadata:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"\\nâœ… Final Results:\")\n",
    "print(f\"   ğŸ“Š Total embeddings: {final_embeddings.shape}\")\n",
    "print(f\"   ğŸ“Š Total articles: {len(all_metadata)}\")\n",
    "print(f\"   ğŸ’¾ Embeddings saved to: {EMBEDDINGS_FILE}\")\n",
    "print(f\"   ğŸ’¾ Metadata saved to: {METADATA_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e77e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (232583, 384)\n",
      "First embedding (first 10 dims): [-0.04351391 -0.02885366 -0.01260174 -0.01757218 -0.02314188  0.04706983\n",
      "  0.02907552  0.01892885  0.00625776 -0.06741666]\n",
      "\n",
      "Metadata count: 232583\n",
      "\n",
      "First 3 items:\n",
      "{\n",
      "  \"person\": \"adam_neumann\",\n",
      "  \"article_id\": \"lifeandstyle/2017/dec/27/adam-gopnik-miracle-before-christmas\",\n",
      "  \"pub_date\": \"2017_12_27\",\n",
      "  \"headline\": \"Adam Gopnik: child star of avant-garde theatre on how a snowstorm saved Christmas\",\n",
      "  \"index\": 0\n",
      "}\n",
      "{\n",
      "  \"person\": \"adam_neumann\",\n",
      "  \"article_id\": \"us-news/2017/dec/16/republicans-trying-to-kill-off-trump-russia-investigation-says-adam-schiff\",\n",
      "  \"pub_date\": \"2017_12_16\",\n",
      "  \"headline\": \"Trump-Russia: Republicans trying to kill off investigation, says Adam Schiff\",\n",
      "  \"index\": 1\n",
      "}\n",
      "{\n",
      "  \"person\": \"adam_neumann\",\n",
      "  \"article_id\": \"books/2017/dec/10/adam-kay-this-is-going-to-hurt-interview-junior-doctor\",\n",
      "  \"pub_date\": \"2017_12_10\",\n",
      "  \"headline\": \"Adam Kay: â€˜If I had kids I would put them off studying medicineâ€™\",\n",
      "  \"index\": 2\n",
      "}\n",
      "\n",
      "Top 10 people:\n",
      "  donald_trump: 26543 articles\n",
      "  jeff_sessions: 8688 articles\n",
      "  ryan_coogler: 7442 articles\n",
      "  prince_harry: 6213 articles\n",
      "  justin_james_watt: 6207 articles\n",
      "  kim_jong_un: 6121 articles\n",
      "  christopher_wylie: 6043 articles\n",
      "  chloe_kim: 5801 articles\n",
      "  mohammed_bin_salman: 5596 articles\n",
      "  elizabeth_diller: 5454 articles\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: ê²°ê³¼ í™•ì¸\n",
    "# ì„ë² ë”© í™•ì¸\n",
    "embeddings = np.load(EMBEDDINGS_FILE)\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"First embedding (first 10 dims): {embeddings[0][:10]}\")\n",
    "\n",
    "# JSONL ë©”íƒ€ë°ì´í„° í™•ì¸\n",
    "metadata = []\n",
    "with open(METADATA_FILE, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        metadata.append(json.loads(line))\n",
    "\n",
    "print(f\"\\nMetadata count: {len(metadata)}\")\n",
    "print(f\"\\nFirst 3 items:\")\n",
    "for item in metadata[:3]:\n",
    "    print(json.dumps(item, indent=2, ensure_ascii=False))\n",
    "\n",
    "# ì‚¬ëŒë³„ ë¶„í¬\n",
    "from collections import Counter\n",
    "person_counts = Counter(item['person'] for item in metadata)\n",
    "print(f\"\\nTop 10 people:\")\n",
    "for person, count in person_counts.most_common(10):\n",
    "    print(f\"  {person}: {count} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4776a819",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
